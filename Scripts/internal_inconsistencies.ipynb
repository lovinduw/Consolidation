{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internal Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import import_ipynb\n",
    "import data_preperation as dp\n",
    "import copy\n",
    "import pyomo.environ as pyo\n",
    "from pyomo.opt import SolverFactory\n",
    "import openpyxl\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries={ 'Austria': 'AT', 'Belgium': 'BE',  'Bulgaria': 'BG', 'Switzerland': 'CH', 'Czech Republic': 'CZ', 'Germany': 'DE', 'Denmark': 'DK', 'Estonia': 'EE', 'Spain': 'ES', 'Finland': 'FI', 'France': 'FR',  'Greece': 'GR', 'Hungary': 'HU', 'Ireland': 'IE', 'Italy': 'IT', 'Lithuania': 'LT', 'Latvia': 'LV', 'Montenegro': 'ME','Netherlands': 'NL', 'Norway': 'NO', 'Poland': 'PL', 'Portugal': 'PT', 'Serbia': 'RS', 'Sweden': 'SE', 'Slovenia': 'SI', 'Slovakia': 'SK', 'United Kingdom': 'UK'}\n",
    "\n",
    "abbr_list=list(countries.values())\n",
    "\n",
    "# load_data = dp.load(countries)\n",
    "# generation_data = dp.generation(countries)\n",
    "# cross_border_data = dp.cross_border(abbr_list)[1]\n",
    "# import_export_using_crossborder_data = dp.import_export_using_crossborder(dp.cross_border(abbr_list)[0],abbr_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Internal inconsistencies based on unedited ENTSO-E data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Calculating missing values and mismatch analysis in data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def omit_dst(df):\n",
    "\n",
    "    # Due to day light saving, all the datasets have null values on 25th March from 02:00 - 03:00. \n",
    "    # Considering the time intervals each country update the data, a total number of rows of 4,2 or 1 are dropped from the 31st March, 02:00 - 03:00 time interval.\n",
    "    # Also this returns a integer ('divider') based on the file length to get the energy values in a later step. In 15 min interval files this is 4, in 30 min interval files this is 2 and in 1 hour interval files this is 1.\n",
    "    \n",
    "    length = len(df)\n",
    "    if length == 35044:\n",
    "        df = df.drop(range(7976, 7980)).reset_index(drop=True)\n",
    "        divider = 4\n",
    "    elif length == 17522:\n",
    "        df = df.drop(range(3988, 3990)).reset_index(drop=True)\n",
    "        divider = 2\n",
    "    else:\n",
    "        df = df.drop(range(1994, 1995)).reset_index(drop=True)\n",
    "        divider = 1\n",
    "    \n",
    "    return(df,divider)\n",
    "\n",
    "def mismatch_raw_data(countries):\n",
    "\n",
    "    load_missing_data = []\n",
    "    generation_missing_data = []\n",
    "    transmission_missing_data =[]\n",
    "    transmission_data = pd.DataFrame()\n",
    "    transmission_data_temp = pd.DataFrame()\n",
    "    temp1 = pd.DataFrame()\n",
    "    \n",
    "    mismatch_data = {}\n",
    "    temp = pd.DataFrame()\n",
    "    csvs = glob.glob(\"../Data Sources/ENTSO-E/2018/Transmission/*.csv\")\n",
    "\n",
    "    # In the following command we read each csv file and drop the rows related to day light saving using 'omit_dst' function\n",
    "    # We create a new dataframe 'transmission_data_temp' with all the columns in all the csvs attached together.\n",
    "    # Then one by one we copy each csv to 'temp' dataframe and make all the 'n/e' values of the 'temp' 0.\n",
    "    # Then we cretae two new column in the 'transmission_data' dataframe and name it as the two country codes the power transmission occurs.\n",
    "    # We use string editing to get the two country codes from the file path.\n",
    "    # Then we save the total number of null values in load data of each country in the load_missing_data list.\n",
    "    # Then we save the total number of null values in generation data of each country in the generation_missing_data list.\n",
    "    # Then we create a list of column names with all the columns of 'transmission_data_temp' if {country_abbreviations} is in the column name.\n",
    "    # Then we save the total number of null values in each 'transmission_data_temp' dataframe of each country in the transmission_missing_data list using the filtered columns in the previous step.\n",
    "    # Then we make a list of column heads of imports and exports associated with a given country_abbreviation\n",
    "    # We calculate the annual data mismatch of each country as (generation + imports - load - exports) of the country.\n",
    "    # Countries provide the data in 15 min, 30 min and 1 hour intervals.\n",
    "    # Therefore to ge the energy values from the power values, we divide the total power data by 4,2 or 1 using the 'divider' variable as required.\n",
    "    \n",
    "    for csv in csvs:\n",
    "\n",
    "        temp, divider_transmission = omit_dst(pd.read_csv(csv))\n",
    "        transmission_data_temp = pd.concat([transmission_data_temp,temp.iloc[:,1:]],axis=1)\n",
    "\n",
    "        temp=temp.replace(['n/e', np.nan], 0)\n",
    "        transmission_data[f'{csv[42:44]} - > {csv[45:47]}'] = pd.to_numeric(temp.iloc[:, 2])/divider_transmission\n",
    "        transmission_data[f'{csv[45:47]} - > {csv[42:44]}'] = pd.to_numeric(temp.iloc[:, 1])/divider_transmission\n",
    "    \n",
    "    for country,abbr in countries.items():\n",
    "        load_data,divider_load = omit_dst(pd.read_csv(f'../Data Sources/ENTSO-E/2018/Load/{country}.csv'))\n",
    "        generation_data, divider_gen = omit_dst(pd.read_csv(f'../Data Sources/ENTSO-E/2018/Generation/{country}.csv', low_memory=False))\n",
    "        \n",
    "        load_missing_data.append(load_data.isnull().sum().sum())\n",
    "        generation_missing_data.append(generation_data.isnull().sum().sum())\n",
    "\n",
    "        transmission_links = [x for x in transmission_data_temp.columns.values if abbr in x]\n",
    "        transmission_missing_data.append(transmission_data_temp[transmission_links].isnull().sum().sum())\n",
    "\n",
    "        imports=[x for x in transmission_data.columns.values if abbr in x[-2:]]\n",
    "        exports=[x for x in transmission_data.columns.values if abbr in x[:2]]\n",
    "\n",
    "        generation_data=generation_data.drop(['Hydro Pumped Storage  - Actual Consumption [MW]'],axis=1)\n",
    "\n",
    "        mismatch_data.update({f'{abbr}': [generation_data.iloc[:, 2:].sum(axis=1).sum()/divider_gen, transmission_data[imports].sum(\n",
    "            axis=1).sum(), load_data.iloc[:, 2].sum()/divider_load, transmission_data[exports].sum(axis=1).sum(), round(abs(generation_data.iloc[:, 2:].sum(axis=1).sum()/divider_gen + transmission_data[imports].sum(\n",
    "                axis=1).sum() - load_data.iloc[:, 2].sum()/divider_load - transmission_data[exports].sum(axis=1).sum())/1000000, 2)]})\n",
    "\n",
    "    temp1['Country'] = list(countries.values())\n",
    "    temp1['No. of missing data in Load data'] = load_missing_data\n",
    "    temp1['No. of missing data in Generation data'] = generation_missing_data\n",
    "    temp1['No. of missing data in Transmission data'] = transmission_missing_data\n",
    "  \n",
    "    mismatch_data_copy = mismatch_data.copy()\n",
    "    mismatch_data_copy = sorted(mismatch_data_copy.items(), key= lambda item: item[1][-1], reverse=True)\n",
    "    display(temp1)   \n",
    "\n",
    "    width = 0.35\n",
    "    labels = [x[0] for x in mismatch_data_copy]\n",
    "    X = np.arange(len(labels))\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.bar([x for x in X], [x[1][-1] for x in mismatch_data_copy], width, color='aqua',edgecolor='black')\n",
    "    plt.xlabel('Countries')\n",
    "    plt.ylabel('Mismatch [TWh]')\n",
    "    plt.title('Mismatch analysis based on (generation + imports - load - exports) in raw ENTSO-E data')\n",
    "    plt.grid()\n",
    "    plt.xticks(X,labels)\n",
    "    plt.show()\n",
    "\n",
    "    return mismatch_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Consolidation based on internally gap filled ENTSO-E data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Nearest Neighbours Mean value based internal approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Polynomial linear regression based internal approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following section we take each generation,load and transmission csvs and send them to the 'function' function.\n",
    "# This function send each dataframe to 'omit_dst' function and removes the empty observations on March 28th and get the 'divider' value which is associated with the length of the dataframe.\n",
    "# Then we replace 'n/e' values with 0\n",
    "\n",
    "def function(df,location):\n",
    "    edited_df = pd.DataFrame()\n",
    "    df, divider = omit_dst(df)\n",
    "    df = df.replace('n/e', 0)\n",
    "\n",
    "    # Then we check the dataframe column by column.\n",
    "    # First we filter the columns in which the whole column is not 0 in the dataframe.\n",
    "    # Then we add the index and value of each value in the column to a list called 'column_data'\n",
    "    # Then we check value by value in the column untill a null value is found.\n",
    "    # If the value is a null value, we get all the values in that column which are the values associated with the same time step of the null value and the absolute difference of the indexes of that value and the null value to the 'temp_list'.\n",
    "    # For example if the null value occurus at 10:00 - 10:15 time step of some day, all the values associated with 10:00 - 10:15 time step throughout the year and and the absolute difference of the indexes of those value and the null value go to the 'temp_list'\n",
    "    # Then the 'temp_list' is sorted and the first 60 values are taken to the 'nearest_neighbours' list\n",
    "    # Here in the 'nearest_neighbours' list we get the values of the nearest 60 days of the same time step in which the null value has occured.\n",
    "    # Then we replace any null values in the 'nearest_neighbours' list with 0\n",
    "    # Then we get the mean value of the 'nearest_neighbours' list and assign that value to the original null value in the dataframe\n",
    "\n",
    "    if location == 'Nearest Neighbours Mean':\n",
    "        for column in df.columns.values:\n",
    "            if not (df[column] == 0).all():\n",
    "                column_data = [[index, value] for index, value in enumerate(df.loc[:, column])]\n",
    "                for index, value in column_data:\n",
    "                    if pd.isnull(value):\n",
    "                        temp_list = [[abs(index-item[0]), item[1]] for item in column_data if (index-item[0]) % (24*divider) == 0]\n",
    "                        temp_list.sort()\n",
    "                        nearest_neighbours = [x[1] for x in temp_list[1:61]]\n",
    "                        nearest_neighbours = [0 if pd.isna(x) else x for x in nearest_neighbours]\n",
    "                        df.loc[index, column] = np.mean(np.array(nearest_neighbours))\n",
    "                edited_df = pd.concat([edited_df, df.loc[:, column]], axis=1)\n",
    "        return edited_df\n",
    "\n",
    "    # In the following section we take each generation,load and transmission csvs and send them to the 'neareset_neighbours_mean' function.\n",
    "    # This function send each dataframe to 'omit_dst' function and removes the empty observations on March 28th and get the 'divider' value which is associated with the length of the dataframe.\n",
    "    # Then we replace 'n/e' values with 0\n",
    "    # Then we check the dataframe column by column.\n",
    "    # First we filter the columns in which the whole column is not 0 in the dataframe.\n",
    "    # Then we add the index and value of each value in the column to a list called 'column_data'\n",
    "    # Then we check value by value in the column untill a null value is found.\n",
    "    # If the value is a null value, we get all the values in that column which are the values associated with the same time step of the null value and the absolute difference of the indexes of that value and the null value to the 'temp_list'.\n",
    "    # For example if the null value occurus at 10:00 - 10:15 time step of some day, all the values associated with 10:00 - 10:15 time step throughout the year and and the absolute difference of the indexes of those value and the null value go to the 'temp_list'\n",
    "    # Then the 'temp_list' is sorted and we get all the indexes in the 'temp_list' except the index of the null value to a 2D numpy array 'X'.\n",
    "    # We get all the values in the 'temp_list' except the value of the null value to a 1D numpy array 'y'\n",
    "    # Then we divide 'X' and 'y' values in the ratio of 30% test and 70% train data.\n",
    "    # We create an array of degree values from 1 to 10.\n",
    "    # Then we iterate the 'degrees' one by one and create polynomial values of 'x_train' data called 'x_poly_train' based on the value of the degree\n",
    "    # Then we fit the polynomial linear regression function using 'x_poly_train' data and 'y_train' data.\n",
    "    # Then based on the polynomial function, using the 'x_poly_test' data we predict the values of the 'y_test' data\n",
    "    # Then based on the predicted values and 'y_test\" data we calculate the Root Mean Square Error.\n",
    "    # Applying the last 4 steps for each degree value, we select the degree value which gives the Lowest Root Mean Square Error.\n",
    "    # Then we fit the polynomial linear regression function again using that degree which gives the Lowest Root Mean Square Error.\n",
    "    # Based on the polynimial function we get the predicted value of the null value.\n",
    "    \n",
    "    elif location == 'Polynomial Linear Regression':\n",
    "        for column in df.columns.values:\n",
    "            if not (df[column] == 0).all():\n",
    "                column_data = [[index, value] for index, value in enumerate(df.loc[:, column])]\n",
    "                for index, value in column_data:\n",
    "                    if pd.isnull(value):\n",
    "                        temp_list = [item for item in column_data if (index-item[0]) % (24*divider) == 0]\n",
    "                        temp_list.sort()\n",
    "                        X = np.array([i[0] for i in temp_list[1:]]).reshape(len(temp_list)-1, 1)\n",
    "                        y = [i[1] for i in temp_list[1:]]\n",
    "                        y = [0 if pd.isna(x) else x for x in y]\n",
    "\n",
    "                        x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "                        degrees = np.arange(1, 11)\n",
    "                        min_rmse, min_deg = 1e10, 0\n",
    "\n",
    "                        for degree in degrees:\n",
    "\n",
    "                            # Preparing polynomial Train features based on x_train\n",
    "                            poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "                            x_poly_train = poly_features.fit_transform(x_train)\n",
    "\n",
    "                            # Polynomial linear regression based on train data\n",
    "                            poly_reg = LinearRegression()\n",
    "                            poly_reg.fit(x_poly_train, y_train)\n",
    "\n",
    "                            # Predicting y values and getting root mean squared error based on predicted y values and y_test values\n",
    "                            x_poly_test = poly_features.fit_transform(x_test)\n",
    "                            poly_predict = poly_reg.predict(x_poly_test)\n",
    "                            poly_mse = mean_squared_error(y_test, poly_predict)\n",
    "                            poly_rmse = np.sqrt(poly_mse)\n",
    "\n",
    "                            # Selecting the best degree of the polynimial function based on lowest root mean squared error\n",
    "                            if min_rmse > poly_rmse:\n",
    "                                min_rmse = poly_rmse\n",
    "                                min_deg = degree\n",
    "\n",
    "                        # Fitting the regression function again based on the selected best degree above\n",
    "                        poly_features = PolynomialFeatures(degree=min_deg, include_bias=False)\n",
    "                        x_poly_train = poly_features.fit_transform(x_train)\n",
    "                        poly_reg = LinearRegression()\n",
    "                        poly_reg.fit(x_poly_train, y_train)\n",
    "\n",
    "                        df.loc[index,column] = poly_reg.predict(poly_features.fit_transform([[0]]))[0]\n",
    "                edited_df = pd.concat([edited_df, df.loc[:, column]], axis=1) \n",
    "        edited_df[edited_df<0]=0\n",
    "        return edited_df\n",
    "    \n",
    "    else:\n",
    "        print('WRONG LOCATION SENT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidation(countries, location):\n",
    "    for country, abbr in countries.items():\n",
    "        df_load = pd.read_csv(f'../Data Sources/ENTSO-E/2018/Load/{country}.csv').iloc[:, 2:]\n",
    "        df_load = df_load.rename(columns={df_load.columns[0]: 'demand'})\n",
    "        load_consolidated = function(df_load,location)\n",
    "        load_consolidated.to_csv(f'../Data Sources/output/{location}/Load/{abbr}.csv')\n",
    "\n",
    "        df_gen = pd.read_csv(f'../Data Sources/ENTSO-E/2018/Generation/{country}.csv').iloc[:, 2:]\n",
    "        df_gen = df_gen.drop(['Hydro Pumped Storage  - Actual Consumption [MW]'], axis=1)\n",
    "        gen_consolidated = function(df_gen, location)\n",
    "        gen_consolidated.to_csv(f'../Data Sources/output/{location}/Generation/{abbr}.csv')\n",
    "    print('LOAD & GENERATION DATA CONSOLIDATED')\n",
    "\n",
    "    transmission_consolidated = pd.DataFrame()\n",
    "    csvs = glob.glob(\"../Data Sources/ENTSO-E/2018/Transmission/*.csv\")\n",
    "    for csv in csvs:\n",
    "        df = pd.read_csv(csv).iloc[:, 1:]\n",
    "        df = df.rename(columns={df.columns[0]: f'{csv[45:47]} - > {csv[42:44]}', df.columns[1]: f'{csv[42:44]} - > {csv[45:47]}'})\n",
    "        transmission_consolidated = pd.concat([transmission_consolidated, function(df,location)], axis=1)\n",
    "    transmission_consolidated.to_csv(f'../Data Sources/output/{location}/Transmission/all_transmissions.csv')\n",
    "    print('TRANSMISSION DATA CONSOLIDATED')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidation(countries, 'Nearest Neighbours Mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Optimization based internal approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def internal_sigma(load_dic, generation_dic, transmission_data, countries):\n",
    "\n",
    "    abbr_list = list(countries.values())\n",
    "    load_dic_copy = copy.deepcopy(load_dic)\n",
    "    generation_dic_copy = copy.deepcopy(generation_dic)\n",
    "    transmission_data_copy = copy.deepcopy(transmission_data)\n",
    "\n",
    "    # In the following command, we calculate the sigma value by sending it to 'data_preperation.ipynb'\n",
    "\n",
    "    sigma = dp.sigma(load_dic_copy, generation_dic_copy, transmission_data_copy, abbr_list)\n",
    "    print('SIGMA CALCULATED')\n",
    "\n",
    "    # In the follwoing commands we create a 'country_index' with the names abbreviations of the 27 countries we consider.\n",
    "    # Then we create a 'time_index' which is a list of integers from 0 to 8760 which indicates the timesteps \n",
    "    # Then we create a 'generation_index' which is a dictionary with country_abbreviations as keys and generation_sources os each country as the values of each key.\n",
    "    # Then we create a 'generation_fuels' which is a list of all the generation_sources we consider.\n",
    "    # Then we create 2 dictionaries 'imports_index' and 'exports_index' with country_abbreviations as the keys.\n",
    "    # In the column heads of 'transmission_data' dataframe, if the name of the key is in the first two characters of the column head, that column head goes as the value in 'export_index' dictionary\n",
    "    # In the column heads of 'transmission_data' dataframe, if the name of the key is in the last two characters of the column head, that column head goes as the value in 'import_index' dictionary\n",
    "    # Then we create a 'transmission_index' with all the column heads of 'transmission_data' dataframe.\n",
    "\n",
    "    country_index = list(countries.values())\n",
    "    time_index = np.arange(8760, dtype=int)\n",
    "\n",
    "    generation_index = {}\n",
    "\n",
    "    for abbr, df in generation_dic.items():\n",
    "        generation_index[abbr] = [x for x in df.columns.values]\n",
    "\n",
    "    generation_fuels = np.array(list(set([item for sublist in generation_index.values() for item in sublist])))\n",
    "    generation_fuels.sort()\n",
    "\n",
    "    imports_index = {}\n",
    "    exports_index = {}\n",
    "\n",
    "    for abbr in abbr_list:\n",
    "        imports_index[abbr] = [x for x in transmission_data.columns.values if abbr in x[-2:]]\n",
    "        exports_index[abbr] = [x for x in transmission_data.columns.values if abbr in x[:2]]\n",
    "\n",
    "    transmission_index = np.array(transmission_data.columns.values)\n",
    "    transmission_index.sort()\n",
    "\n",
    "    # In the following commands, we initiate the pyomo optimization with 'Gurobi' solver\n",
    "    # We declare the three variables 'delta_generation', 'delta_load' and 'delta_transmission'\n",
    "    # 'delta_generation' consists of 2,207,520 values which vary with country, generation_source and timestep.\n",
    "    # 'delta_load' consists of 236,520 values which vary with country and timestep.\n",
    "    # 'delta_transmission' consists of 858,480 values which vary with timestep and with country indirectly.\n",
    "    # We declare the model constraint as the sum of (delta_generation + generation + delta_transmission(imports) + imports) is equal to the sum of (delta_load + load + delta_transmission(exports) + exports) in all time steps.\n",
    "    # We declare objective function as to MINIMIZE the sum of (delta_generation^2 * sigma(generaion)) + sum of (delta_load^2 * sigma(load)) + sum of (delta_transmission^2 * sigma(transmission)) in all timesteps.\n",
    "    # Then solve the model.\n",
    "\n",
    "    model = pyo.ConcreteModel()\n",
    "\n",
    "    model.country_index = pyo.Set(initialize=country_index)\n",
    "    model.time_index = pyo.Set(initialize=time_index)\n",
    "    model.generation_fuels = pyo.Set(initialize=generation_fuels)\n",
    "    model.transmission_index = pyo.Set(initialize=transmission_index)\n",
    "\n",
    "    model.delta_generation = pyo.Var(model.country_index, model.time_index, model.generation_fuels, bounds=(0.0, None))\n",
    "    model.delta_load = pyo.Var(model.country_index, model.time_index, bounds=(0.0, None))\n",
    "    model.delta_transmission = pyo.Var(model.time_index, model.transmission_index, bounds=(0.0, None))\n",
    "\n",
    "    print('VARIABLES DECLARED')\n",
    "\n",
    "    def balance_rule(model, country, time):\n",
    "        return sum(model.delta_generation[country, time, generation] + generation_dic[country][generation][time] for generation in generation_index[country]) + sum(model.delta_transmission[time, link] + transmission_data[link][time] for link in imports_index[country]) ==  \\\n",
    "            model.delta_load[country, time] + load_dic[country][\"demand\"][time] + \\\n",
    "            sum(model.delta_transmission[time, link] + transmission_data[link][time] for link in exports_index[country])\n",
    "    model.balance_rule = pyo.Constraint(model.country_index, model.time_index, rule=balance_rule)\n",
    "\n",
    "    def ObjRule(model):\n",
    "        return sum(model.delta_generation[country, time, generation] ** 2 * float(sigma[country][generation].iloc[time]) for country in model.country_index for time in model.time_index for generation in generation_index[country]) \\\n",
    "            + sum(model.delta_transmission[time, link] ** 2 * float(sigma[\"transmission_data\"][link].iloc[time]) for time in model.time_index for link in model.transmission_index)\\\n",
    "            + sum(model.delta_load[country, time] ** 2 * float(sigma[country][\"demand\"].iloc[time]) for country in model.country_index for time in model.time_index)\n",
    "\n",
    "    model.obj = pyo.Objective(rule=ObjRule, sense=pyo.minimize)\n",
    "    opt = SolverFactory(\"gurobi\", solver_io=\"python\")\n",
    "    opt.solve(model)\n",
    "    print('OPTIMIZATION COMPLETED')\n",
    "\n",
    "    # In the following commands we copy the pyomo results into different intermediary variables.\n",
    "    # We create 'intermediary_var' dictionary and inside it create two other dictionaries as keys called 'generation' and 'load' and create a dataframe as another key called 'transmission'.\n",
    "    # We create 'unit_var' dictionary and inside it create two other dictionaries as keys called 'generation' and 'load' and create a dataframe as another key called 'transmission'.\n",
    "    # We fill the 'intermediary_var[\"generation\"]' from the 'delta_generation' values and 'intermediary_var[\"load\"]' with 'delta_load' values and 'intermediary_var[\"transmission\"]' from 'delta_transmission' values.\n",
    "    # We fill 'unit_var[\"generation\"]', 'unit_var[\"load\"]' and 'unit_var[\"transmission\"]' which have the same size as of 'intermediary_var[\"generation\"]', 'intermediary_var[\"load\"]' and ''intermediary_var[\"transmission\"]' with integer 1.\n",
    "\n",
    "    generation_index_copy = copy.deepcopy(generation_index)\n",
    "\n",
    "    intermediary_var = {}\n",
    "    intermediary_var[\"generation\"] = {}\n",
    "    intermediary_var[\"load\"] = {}\n",
    "    unit_var = {}\n",
    "    unit_var[\"generation\"] = {}\n",
    "    unit_var[\"load\"] = {}\n",
    "\n",
    "    for country in country_index:\n",
    "        table_gen = []\n",
    "        row_load = []\n",
    "        for time in time_index:\n",
    "            row_gen = []\n",
    "            for generation in generation_index_copy[country]:\n",
    "                row_gen.append(model.delta_generation[country, time, generation].value)\n",
    "            table_gen.append(row_gen)\n",
    "            row_load.append(model.delta_load[country, time].value)\n",
    "\n",
    "        intermediary_var[\"generation\"][country] = pd.DataFrame.from_records(table_gen)\n",
    "        intermediary_var[\"load\"][country] = pd.DataFrame(row_load)\n",
    "        intermediary_var[\"generation\"][country].columns = generation_index_copy[country]\n",
    "        intermediary_var[\"load\"][country].columns = ['demand']\n",
    "        intermediary_var[\"generation\"][country].to_csv(\"../Data Sources/output/Sigma/Debugging/\" + \"internal_sigma_gen_\" + country + \".csv\")\n",
    "        intermediary_var[\"load\"][country].to_csv(\"../Data Sources/output/Sigma/Debugging/\" + \"internal_sigma_load_\" + country + \".csv\")\n",
    "        unit_var[\"generation\"][country] = pd.DataFrame(1, index=np.arange(df.shape[0]), columns=generation_index_copy[country])\n",
    "        unit_var[\"load\"][country] = pd.DataFrame(1, index=np.arange(df.shape[0]), columns=['demand'])\n",
    "\n",
    "    table_transmission = []\n",
    "\n",
    "    for time in time_index:\n",
    "        row = []\n",
    "        for link in transmission_index:\n",
    "            row.append(model.delta_transmission[time, link].value)\n",
    "        table_transmission.append(row)\n",
    "    intermediary_var[\"transmission\"] = pd.DataFrame.from_records(table_transmission)\n",
    "    intermediary_var[\"transmission\"].columns = transmission_index\n",
    "    intermediary_var[\"transmission\"].to_csv(\"../Data Sources/output/Sigma/Debugging/internal_sigma_transmission.csv\")\n",
    "    unit_var[\"transmission\"] = pd.DataFrame(1, index=np.arange(transmission_data.shape[0]), columns=transmission_data.columns)\n",
    "\n",
    "    generation_dic_copy = copy.deepcopy(generation_dic)\n",
    "    transmission_data_copy = copy.deepcopy(transmission_data)\n",
    "\n",
    "    # We send the intermediary_var and unit_var to 'data_preperation.ipynb' to get the consolidated generation,load and transmission values.\n",
    "\n",
    "    consolidated_gen_data, consolidated_load_data, consolidated_transmission_data = dp.data_consolidation(generation_dic_copy, load_dic, transmission_data_copy, intermediary_var, unit_var)\n",
    "\n",
    "    return(consolidated_gen_data, consolidated_load_data, consolidated_transmission_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Mismatch analysis in the consolidated ENTSO-E data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to 'omit_dst' function above, we use 'hourly_data' function to get the divider value based on dataframe length\n",
    "\n",
    "def hourly_data(df):\n",
    "    length = len(df.index)\n",
    "    if length == 35040:\n",
    "        divider = 4\n",
    "    elif length == 17520:\n",
    "        divider = 2\n",
    "    else:\n",
    "        divider = 1\n",
    "    return (df, divider)\n",
    "\n",
    "# In the following function we get the ENTSO-E statistical factsheet of 2018 to a dataframe called 'stat_data'\n",
    "# Then we selct only the country_name, generation,load,transmission_imports and transmission_exports columns and convert import,exports values to TWh which are stated in GWh\n",
    "# We create a list with the names of the three folders where the consolidated csvs are saved in each internal consolidation method.\n",
    "# We create another list with the names of the two folders where the consolidated csvs of 'Generation' and 'Load' files are saved.\n",
    "# Then we iterate through the rows of 'stat_data' dataframe.\n",
    "# We check if the country_abbreviation in 'stat_data''s country column is inside the 'selected_countries' list.\n",
    "# Then we get generation and load data of the raw unedited data from 'mismatch_data' dictionary and generation and load data of Nearest Neighbour Mean, Polynomial Linear Regression and Optimization based methods from their csv files in the respective folders.\n",
    "# We get the total sum value in each csv and get the annual energy value using the 'divider' variable and save the value in 'stat_data' dataframe.\n",
    "# Similarly we get the transmission imports and exports from the respective csvs by filtering the country_abbreviations in the column headers.\n",
    "\n",
    "def mismatch_analysis(mismatch_data,selected_countries):\n",
    "    stat_data = pd.read_excel('../Data Sources/ENTSO-E/entsoe_sfs2018_web.xlsx').iloc[1:28, ].reset_index(drop=True)\n",
    "    stat_data = stat_data[['Country', 'Total net generation','Consumption', 'Sum of imports', 'Sum of exports']]\n",
    "    stat_data.iloc[:, 3:5] = stat_data.iloc[:, 3:5]/1000\n",
    "\n",
    "    methods = ['Nearest Neighbours Mean','Polynomial Linear Regression', 'Sigma']\n",
    "    types = ['Generation', 'Load']\n",
    "\n",
    "    for index, value in enumerate(stat_data.loc[:, 'Country']):\n",
    "\n",
    "        if value in selected_countries:\n",
    "\n",
    "            stat_data.loc[index, 'Raw_gen'] = mismatch_data[value][0]/1000000\n",
    "            stat_data.loc[index, 'Raw_load'] = mismatch_data[value][2]/1000000\n",
    "            stat_data.loc[index, 'Raw_imports'] = mismatch_data[value][1]/1000000\n",
    "            stat_data.loc[index, 'Raw_exports'] = mismatch_data[value][3]/1000000\n",
    "\n",
    "            for method in methods:\n",
    "                for type in types:\n",
    "\n",
    "                    df, divider = hourly_data(pd.read_csv(f'../Data Sources/output/{method}/{type}/{value}.csv'))\n",
    "                    stat_data.loc[index, f'{type}_{method}'] = df.iloc[:, 1:].sum(axis=1).sum()/(divider*1000000)\n",
    "\n",
    "                df = pd.read_csv(f'../Data Sources/output/{method}/Transmission/all_transmissions.csv').iloc[:, 1:]\n",
    "\n",
    "                imports = [x for x in df.columns.values if value in x[-2:]]\n",
    "                exports = [x for x in df.columns.values if value in x[:2]]\n",
    "\n",
    "                stat_data.loc[index, f'Imports_{method}'] = df[imports].sum(axis=1).sum()/1000000\n",
    "                stat_data.loc[index, f'Exports_{method}'] = df[exports].sum(axis=1).sum()/1000000\n",
    "        else:\n",
    "            stat_data = stat_data.drop(labels=index, axis=0)\n",
    "    stat_data = stat_data.reset_index(drop=True)\n",
    "\n",
    "    # We create new variable list xi where i belongs to [1,2,3,4]\n",
    "    # Each xi list contains 4 values\n",
    "    # After the iteration is completed x1 has the sum values of generation,load,imports and exports of raw unedited data of all the countries with the percentage difference of each of the 4 values compared to the respective generation,load,imports and exports sum values of ENTSO-E statistical data.\n",
    "    # Similarly x2 has the percentage dofference of the 4 values of generation,load,imports and exports of Nearest Neighbour Mean method.\n",
    "    # In the same way x3 is the Polynmial Linear regression method and x4 is the optimization  based method. \n",
    "\n",
    "    for i in range(1, 5):\n",
    "        globals()[f'x{i}'] = []\n",
    "        for j in range(1, 5):\n",
    "            globals()[f'x{i}'].append(abs(stat_data.iloc[:, j].sum()-stat_data.iloc[:, i*4+j].sum())*100/stat_data.iloc[:, j].sum())\n",
    "\n",
    "    width = 0.18\n",
    "    labels = ['Actual generation per type', 'Actual load','Physical flow imports', 'Physical flow exports']\n",
    "    X = np.arange(4)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar([x-3*width/2 for x in X], x1, width, color='magenta', edgecolor='black', label='Raw data')\n",
    "    plt.bar([x-width/2 for x in X], x2, width, color='aqua', edgecolor='black', label='Nearest Neighbours Mean consolidated data')\n",
    "    plt.bar([x+width/2 for x in X], x3, width, color='gold', edgecolor='black', label='Polynomial Linear Regression consolidated data')\n",
    "    plt.bar([x+3*width/2 for x in X], x4, width, color='lightcoral', edgecolor='black', label='Sigma consolidated data')\n",
    "    plt.xlabel('Data Items')\n",
    "    plt.ylabel('Percentage difference')\n",
    "    plt.title('Percentage difference with respect to ENTSO-E statistical data - 2018')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xticks(X, labels)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internal Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from data_preperation.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import import_ipynb\n",
    "import data_preperation as dp\n",
    "import copy\n",
    "import pyomo.environ as pyo\n",
    "from pyomo.opt import SolverFactory\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries={ 'Austria': 'AT', 'Belgium': 'BE',  'Bulgaria': 'BG', 'Switzerland': 'CH', 'Czech Republic': 'CZ', 'Germany': 'DE', 'Denmark': 'DK', 'Estonia': 'EE', 'Spain': 'ES', 'Finland': 'FI', 'France': 'FR',  'Greece': 'GR', 'Hungary': 'HU', 'Ireland': 'IE', 'Italy': 'IT', 'Lithuania': 'LT', 'Latvia': 'LV', 'Montenegro': 'ME','Netherlands': 'NL', 'Norway': 'NO', 'Poland': 'PL', 'Portugal': 'PT', 'Serbia': 'RS', 'Sweden': 'SE', 'Slovenia': 'SI', 'Slovakia': 'SK', 'United Kingdom': 'GB'}\n",
    "abbr_list=list(countries.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Internal inconsistencies based on unedited ENTSO-E data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Calculating missing values and mismatch analysis in data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_of_missing_observations(df, length, calc):\n",
    "\n",
    "    # The following function checks the length of the missing observations in the entire dataframe.\n",
    "    # 'size' variable calculates the total number of observations in the dataframe including the null values.\n",
    "    # Then update the number of missing observations in each time length in the 'calc' dictionary. 'calc' dictionary is explained in below 'mismatch_raw_data' function\n",
    "    # After calculating the whole dataframe, the obtained values are divided by 'size' to get the percentage value with respect to the total observations in the dataset.\n",
    "    # Then returns the 'calc' values\n",
    "    \n",
    "    size = df.shape[0]*df.shape[1]\n",
    "    for column in df.columns.values:\n",
    "        i = 0\n",
    "        counter = 0\n",
    "        while i < len(df[column]):\n",
    "            if pd.isnull(df.loc[i, column]) and i == len(df[column])-1:\n",
    "                counter += 1\n",
    "            if pd.isnull(df.loc[i, column]) and i != len(df[column])-1:\n",
    "                counter += 1\n",
    "            elif counter == 1:\n",
    "                calc['Single observations'] += counter\n",
    "                counter = 0\n",
    "            elif counter > 1 and counter <= 24*length:\n",
    "                calc['Consecutive Multiple hours in 1 day'] += counter\n",
    "                counter = 0\n",
    "            elif counter > 24*length and counter <= 24*30*length:\n",
    "                calc['More than 1 day less than 1 month'] += counter\n",
    "                counter = 0\n",
    "            elif counter > 24*30*length and counter < len(df[column]):\n",
    "                calc['More than 1 month less than whole year'] += counter\n",
    "                counter = 0\n",
    "            if counter == len(df[column]):\n",
    "                calc['Whole year'] += counter\n",
    "            i += 1\n",
    "    for key, value in calc.items():\n",
    "        calc[key] = round(value*100/size, 3)\n",
    "    return calc\n",
    "\n",
    "def mismatch_raw_data(countries):\n",
    "\n",
    "    types = ['Load', 'Generation', 'Transmission']\n",
    "    columns = ['Country']\n",
    "\n",
    "    # 'calc' dictionary saves the number of missing observations in each column in each length category. These length categoroes are 'The number of single missing observations','Number of missing observations in consecutive multiple hours in 1a single day','Number of missing observations in a time period of more than 1 month' and 'Number of missing observations when the data of the whole column is missing'\n",
    "\n",
    "    calc = {'Single observations': 0, 'Consecutive Multiple hours in 1 day': 0,\n",
    "        'More than 1 day less than 1 month': 0, 'More than 1 month less than whole year': 0, 'Whole year': 0}\n",
    "    for type in types:\n",
    "        for item in calc.keys():\n",
    "            columns.append(f'{type}_{item}')\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    transmission_data = pd.DataFrame()\n",
    "    rows = []\n",
    "\n",
    "    mismatch_data = {}\n",
    "    temp = pd.DataFrame()\n",
    "    csvs = glob.glob(\"../Data Sources/ENTSO-E/2018/Transmission/*.csv\")\n",
    "\n",
    "    # In the following command we read each csv file and drop the rows related to day light saving using 'omit_dst' function\n",
    "    # We create a new dataframe 'transmission_data_temp' with all the columns in all the csvs attached together.\n",
    "    # We apend the 'rows' list with string edited column names of the 'temp' dataframe and number of N/A values in that column.\n",
    "    # Then we cretae two new column in the 'transmission_data' dataframe and name it as the two country codes the power transmission occurs.\n",
    "    # We use string editing to get the two country codes from the file path.\n",
    "    # Then we convert the 'rows' to a dataframe called 'transmission_missing_data' \n",
    "    # We read Load and Generation csvs and send them to 'length_of_missing_observations' function to get the missing observations information.\n",
    "    # We update the 'row' Array with the information of missing observations obtained from the 'length_of_missing_observations' function.\n",
    "    # Then we make a list with all the column names of 'transmission_data' if the country abbreviation is in that column head.\n",
    "    # Then we make a list of column heads of imports and exports associated with a given country_abbreviation\n",
    "    # We calculate the annual data mismatch of each country as (generation + imports - load - exports) of the country and save that in 'mismatch_data' dictionary with the country abbreviation as the key.\n",
    "    # Countries provide the data in 15 min, 30 min and 1 hour intervals.\n",
    "    # Therefore in 'mismatch_data' variable, to ge the energy values from the power values, we divide the total power data by 4,2 or 1 using the 'divider' variable as required.\n",
    "    # After sorting the 'mismatch_data' by values, graph is drawn.\n",
    "\n",
    "    for csv in csvs:\n",
    "\n",
    "        temp, divider_transmission = dp.omit_dst(pd.read_csv(csv))\n",
    "        rows.append([f'{csv[42:44]} - > {csv[45:47]}',temp.iloc[:, 2].isnull().sum()])\n",
    "        rows.append([f'{csv[45:47]} - > {csv[42:44]}',temp.iloc[:, 1].isnull().sum()])\n",
    "\n",
    "        transmission_data[f'{csv[42:44]} - > {csv[45:47]}'] = pd.to_numeric(temp.iloc[:, 2],errors='coerce')/divider_transmission\n",
    "        transmission_data[f'{csv[45:47]} - > {csv[42:44]}'] = pd.to_numeric(temp.iloc[:, 1],errors='coerce')/divider_transmission\n",
    "  \n",
    "    for country,abbr in countries.items():\n",
    "\n",
    "        row = []\n",
    "\n",
    "        load_data,divider_load = dp.omit_dst(pd.read_csv(f'../Data Sources/ENTSO-E/2018/Load/{country}.csv').iloc[:,2:])\n",
    "        row.append((length_of_missing_observations(load_data,divider_load,calc.copy())).values())\n",
    "\n",
    "        generation_data,divider_gen = dp.omit_dst(pd.read_csv(f'../Data Sources/ENTSO-E/2018/Generation/{country}.csv').iloc[:,2:])\n",
    "        generation_data = generation_data.drop(['Hydro Pumped Storage  - Actual Consumption [MW]'], axis=1)\n",
    "        row.append((length_of_missing_observations(generation_data,divider_gen,calc.copy())).values())\n",
    "\n",
    "        transmission=[x for x in transmission_data.columns.values if abbr in x]\n",
    "        imports = [x for x in transmission_data.columns.values if abbr in x[-2:]]\n",
    "        exports = [x for x in transmission_data.columns.values if abbr in x[:2]]\n",
    "        row.append((length_of_missing_observations(transmission_data[transmission],divider_transmission,calc.copy())).values())\n",
    "        row = [x for item in row for x in item]\n",
    "        row.insert(0, abbr)\n",
    "        row = ['-' if x==0 else x for x in row]\n",
    "        row_series = pd.Series(row, index=df.columns)\n",
    "        df = df.append(row_series, ignore_index=True)\n",
    "\n",
    "        mismatch_data[abbr] = round(abs(generation_data.iloc[:,:].sum(axis=1).sum()/divider_gen + transmission_data[imports].sum(\n",
    "                axis=1).sum() - load_data.iloc[:,0].sum()/divider_load - transmission_data[exports].sum(axis=1).sum()), 2)/1000000\n",
    "\n",
    "    mismatch_data = sorted(mismatch_data.items(), key= lambda item: item[1], reverse=True)\n",
    "\n",
    "    width = 0.35\n",
    "    labels = [item[0] for item in mismatch_data]\n",
    "    X = np.arange(len(labels))\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.bar([x for x in X], [item[1] for item in mismatch_data], width, color='aqua',edgecolor='black')\n",
    "    plt.xlabel('Countries')\n",
    "    plt.ylabel('Mismatch [TWh]')\n",
    "    plt.title('Mismatch analysis based on (generation + imports - load - exports) in raw ENTSO-E data')\n",
    "    plt.grid()\n",
    "    plt.axhline(y=0, color='black', linestyle='-')\n",
    "    plt.xticks(X,labels)\n",
    "    plt.show()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Identifying inconsistencies in Generation data of ENTSO-E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inconcistencies_generation_data():    \n",
    "    csvs = glob.glob(\"../Data Sources/ENTSO-E/2018/Generation/Generation per unit/*.csv\")\n",
    "    data = []\n",
    "\n",
    "    # The data obtained of the data item 'Actual Generation Output per Generation' in cvs format are saved in '../Data Sources/ENTSO-E/2018/Generation/Generation per unit' folder\n",
    "    # The data obtained of the data item 'Aggregated Generation per Type' in cvs format are saved in '../Data Sources/ENTSO-E/2018/Generation' folder\n",
    "    # We get a list of paths of csv files in the first folder using glob function.\n",
    "    # Using the path name, respective 'Aggregated Generation per Type' csv of the 'Actual Generation Output per Generation' file is opened with variable name 'df_type' and sent it to 'omit_dst' function.\n",
    "    # Then in every column is renamed by removing the ' Actual Aggregated [MW]' part of the column name.\n",
    "    # Then we open the relevant'Aggregated Generation per Type' csv with variable name 'df_gen'.\n",
    "    # In the 'df_gen' file, if 'Actual Consumption' columns are available, such columns are dropped only keeping the 'Actual Aggregated' columns.\n",
    "    # If 'df_gen' has 'Actual Consumption' columns, then we remove the first row and reset the index of the dataframe.\n",
    "    # Then we replace 'n/e' and null values with 0 and transpose the dataframe.\n",
    "    # In the transposed dataframe we get the set of generation type names from the 1st column to the 'generation_types' variable.\n",
    "    # Then we group the 'df_gen' using the 1st column.\n",
    "    # By iterating through each generation type we get a subgroup of the 'df_gen' filtered by the generation type.\n",
    "    # This subgroup contains only the data related to the power plants of that particular generation type.\n",
    "    # We get the sum of all the columns (Sum of all the power values of the power plants of that particular generation type)\n",
    "    # Then we append the 'data' list with country_name, generation_type, sum value of previous step and sum of the column of that particular generation type in 'df_type' dataframe.\n",
    "    # Finally we convert this list to a dataframe.\n",
    "\n",
    "    for csv in csvs:\n",
    "\n",
    "        df_type,length = dp.omit_dst(pd.read_csv(f'../Data Sources/ENTSO-E/2018/Generation/{csv[60:-4]}.csv', low_memory=False).iloc[:,2:])\n",
    "        df_type = df_type.replace('n/e', 0)\n",
    "        columns = [column[:-26] for column in df_type.columns.values]\n",
    "        df_type.columns = columns\n",
    "\n",
    "        df_gen = pd.read_csv(csv,low_memory=False).iloc[:, 1:]\n",
    "\n",
    "        inc = False\n",
    "        for column in df_gen.columns.values:\n",
    "            if df_gen.loc[1, column] == 'Actual Consumption':\n",
    "                df_gen = df_gen.drop(column, axis=1)\n",
    "                inc = True\n",
    "        if inc:\n",
    "            df_gen = df_gen.drop(index=1).reset_index(drop=True)\n",
    "\n",
    "        df_gen = df_gen.replace(['n/e', np.nan], 0)\n",
    "        df_gen = df_gen.T\n",
    "\n",
    "        generation_types = list(set(df_gen.iloc[:, 0].values))\n",
    "        df_gen = df_gen.groupby(df_gen.iloc[:, 0])\n",
    "        for item in generation_types:\n",
    "            selected_data = df_gen.get_group(item).iloc[:, 1:]\n",
    "            tot = 0\n",
    "            for column in selected_data.columns.values:\n",
    "                tot += pd.to_numeric(selected_data.loc[:, column]).sum()\n",
    "            # print(csv[60:-4], item, tot, df_type.loc[:, item].sum()/length)\n",
    "            data.append([csv[60:-4], item, tot, df_type.loc[:, item].sum()/length])\n",
    "\n",
    "    df = pd.DataFrame(data,columns=['Country','Generation type','Based on per generator','Based on per type'])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Consolidation based on internally gap filled ENTSO-E data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Optimization based internal approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def internal(load_dic, generation_dic, transmission_data, countries):\n",
    "\n",
    "    abbr_list = list(countries.values())\n",
    "    load_dic_copy = copy.deepcopy(load_dic)\n",
    "    generation_dic_copy = copy.deepcopy(generation_dic)\n",
    "\n",
    "    # We filter the transmission links between given two countries in which if both countries associated with the power transmission are included in our country_abbreviation list.\n",
    "    # For example power import(export) occurs from(to) a country other than the countries in the abbreviation_list (for ex: 'Cyprus','Turkey' etc.) are omitted.\n",
    "\n",
    "    transmission_lines = list([x for x in transmission_data.columns.values if x[:2] in abbr_list and x[-2:] in abbr_list])\n",
    "    transmission_data = transmission_data[transmission_lines]\n",
    "    transmission_data_copy = copy.deepcopy(transmission_data)\n",
    "\n",
    "    # In the following command, we calculate the sigma value by sending it to 'data_preperation.ipynb'\n",
    "\n",
    "    sigma = dp.calculate_sigma(load_dic_copy, generation_dic_copy, transmission_data_copy, abbr_list)\n",
    "    print('SIGMA CALCULATED')\n",
    "\n",
    "    # In the follwoing commands we create a 'country_index' with the names abbreviations of the 27 countries we consider.\n",
    "    # Then we create a 'time_index' which is a list of integers from 0 to 8760 which indicates the timesteps \n",
    "    # Then we create a 'generation_index' which is a dictionary with country_abbreviations as keys and generation_sources os each country as the values of each key.\n",
    "    # Then we create a 'generation_fuels' which is a list of all the generation_sources we consider.\n",
    "    # Then we create 2 dictionaries 'imports_index' and 'exports_index' with country_abbreviations as the keys.\n",
    "    # In the column heads of 'transmission_data' dataframe, if the name of the key is in the first two characters of the column head, that column head goes as the value in 'export_index' dictionary\n",
    "    # In the column heads of 'transmission_data' dataframe, if the name of the key is in the last two characters of the column head, that column head goes as the value in 'import_index' dictionary\n",
    "    # Then we create a 'transmission_index' with all the column heads of 'transmission_data' dataframe.\n",
    "\n",
    "    country_index = list(countries.values())\n",
    "    time_index = np.arange(8760, dtype=int)\n",
    "\n",
    "    generation_index = {}\n",
    "\n",
    "    for abbr, df in generation_dic.items():\n",
    "        generation_index[abbr] = [x for x in df.columns.values]\n",
    "\n",
    "    generation_fuels = np.array(list(set([item for sublist in generation_index.values() for item in sublist])))\n",
    "    generation_fuels.sort()\n",
    "\n",
    "    imports_index = {}\n",
    "    exports_index = {}\n",
    "\n",
    "    for abbr in abbr_list:\n",
    "        imports_index[abbr] = [x for x in transmission_data.columns.values if abbr in x[-2:]]\n",
    "        exports_index[abbr] = [x for x in transmission_data.columns.values if abbr in x[:2]]\n",
    "\n",
    "    transmission_index = np.array(transmission_data.columns.values)\n",
    "    transmission_index.sort()\n",
    "\n",
    "    # In the following commands, we initiate the pyomo optimization with 'Gurobi' solver\n",
    "    # We declare the three variables 'delta_generation', 'delta_load' and 'delta_transmission'\n",
    "    # 'delta_generation' consists of 2,207,520 values which vary with country, generation_source and timestep.\n",
    "    # 'delta_load' consists of 236,520 values which vary with country and timestep.\n",
    "    # 'delta_transmission' consists of 858,480 values which vary with timestep and with country indirectly.\n",
    "    # We declare the model constraint as the sum of (delta_generation + generation + delta_transmission(imports) + imports) is equal to the sum of (delta_load + load + delta_transmission(exports) + exports) in all time steps.\n",
    "    # We declare objective function as to MINIMIZE the sum of (delta_generation^2 * sigma(generaion)) + sum of (delta_load^2 * sigma(load)) + sum of (delta_transmission^2 * sigma(transmission)) in all timesteps.\n",
    "    # Then solve the model.\n",
    "\n",
    "    model = pyo.ConcreteModel()\n",
    "\n",
    "    model.country_index = pyo.Set(initialize=country_index)\n",
    "    model.time_index = pyo.Set(initialize=time_index)\n",
    "    model.generation_fuels = pyo.Set(initialize=generation_fuels)\n",
    "    model.transmission_index = pyo.Set(initialize=transmission_index)\n",
    "\n",
    "    model.delta_generation = pyo.Var(model.country_index, model.time_index, model.generation_fuels, bounds=(0.0, None))\n",
    "    model.delta_load = pyo.Var(model.country_index, model.time_index, bounds=(0.0, None))\n",
    "    model.delta_transmission = pyo.Var(model.time_index, model.transmission_index, bounds=(0.0, None))\n",
    "\n",
    "    print('VARIABLES DECLARED')\n",
    "\n",
    "    def balance_rule(model, country, time):\n",
    "        return sum(model.delta_generation[country, time, generation] + generation_dic[country][generation][time] for generation in generation_index[country]) + sum(model.delta_transmission[time, link] + transmission_data[link][time] for link in imports_index[country]) ==  \\\n",
    "            model.delta_load[country, time] + load_dic[country][\"demand\"][time] + \\\n",
    "            sum(model.delta_transmission[time, link] + transmission_data[link][time] for link in exports_index[country])\n",
    "    model.balance_rule = pyo.Constraint(model.country_index, model.time_index, rule=balance_rule)\n",
    "\n",
    "    def ObjRule(model):\n",
    "        return sum(model.delta_generation[country, time, generation] ** 2 * float(sigma[country][generation].iloc[time]) for country in model.country_index for time in model.time_index for generation in generation_index[country]) \\\n",
    "            + sum(model.delta_transmission[time, link] ** 2 * float(sigma[\"transmission_data\"][link].iloc[time]) for time in model.time_index for link in model.transmission_index)\\\n",
    "            + sum(model.delta_load[country, time] ** 2 * float(sigma[country][\"demand\"].iloc[time]) for country in model.country_index for time in model.time_index)\n",
    "\n",
    "    model.obj = pyo.Objective(rule=ObjRule, sense=pyo.minimize)\n",
    "    opt = SolverFactory(\"gurobi\", solver_io=\"python\")\n",
    "    opt.solve(model)\n",
    "    print('OPTIMIZATION COMPLETED')\n",
    "\n",
    "    # In the following commands we copy the pyomo results into different intermediary variables.\n",
    "    # We create 'intermediary_var' dictionary and inside it create two other dictionaries as keys called 'generation' and 'load' and create a dataframe as another key called 'transmission'.\n",
    "    # We create 'unit_var' dictionary and inside it create two other dictionaries as keys called 'generation' and 'load' and create a dataframe as another key called 'transmission'.\n",
    "    # We fill the 'intermediary_var[\"generation\"]' from the 'delta_generation' values and 'intermediary_var[\"load\"]' with 'delta_load' values and 'intermediary_var[\"transmission\"]' from 'delta_transmission' values.\n",
    "    # We fill 'unit_var[\"generation\"]', 'unit_var[\"load\"]' and 'unit_var[\"transmission\"]' which have the same size as of 'intermediary_var[\"generation\"]', 'intermediary_var[\"load\"]' and ''intermediary_var[\"transmission\"]' with integer 1.\n",
    "\n",
    "    intermediary_var = {}\n",
    "    intermediary_var[\"generation\"] = {}\n",
    "    intermediary_var[\"load\"] = {}\n",
    "    unit_var = {}\n",
    "    unit_var[\"generation\"] = {}\n",
    "    unit_var[\"load\"] = {}\n",
    "\n",
    "    for country in country_index:\n",
    "        table_gen = []\n",
    "        row_load = []\n",
    "        for time in time_index:\n",
    "            row_gen = []\n",
    "            for generation in generation_index_copy[country]:\n",
    "                row_gen.append(model.delta_generation[country, time, generation].value)\n",
    "            table_gen.append(row_gen)\n",
    "            row_load.append(model.delta_load[country, time].value)\n",
    "\n",
    "        intermediary_var[\"generation\"][country] = pd.DataFrame.from_records(table_gen)\n",
    "        intermediary_var[\"load\"][country] = pd.DataFrame(row_load)\n",
    "        intermediary_var[\"generation\"][country].columns = generation_index_copy[country]\n",
    "        intermediary_var[\"load\"][country].columns = ['demand']\n",
    "        intermediary_var[\"generation\"][country].to_csv(\"../Data Sources/output/Internal/Debugging/\" + \"internal_sigma_gen_\" + country + \".csv\")\n",
    "        intermediary_var[\"load\"][country].to_csv(\"../Data Sources/output/Internal/Debugging/\" + \"internal_sigma_load_\" + country + \".csv\")\n",
    "        unit_var[\"generation\"][country] = pd.DataFrame(1, index=np.arange(df.shape[0]), columns=generation_index_copy[country])\n",
    "        unit_var[\"load\"][country] = pd.DataFrame(1, index=np.arange(df.shape[0]), columns=['demand'])\n",
    "\n",
    "    table_transmission = []\n",
    "\n",
    "    for time in time_index:\n",
    "        row = []\n",
    "        for link in transmission_index:\n",
    "            row.append(model.delta_transmission[time, link].value)\n",
    "        table_transmission.append(row)\n",
    "    intermediary_var[\"transmission\"] = pd.DataFrame.from_records(table_transmission)\n",
    "    intermediary_var[\"transmission\"].columns = transmission_index\n",
    "    intermediary_var[\"transmission\"].to_csv(\"../Data Sources/output/Internal/Debugging/internal_sigma_transmission.csv\")\n",
    "    unit_var[\"transmission\"] = pd.DataFrame(1, index=np.arange(transmission_data.shape[0]), columns=transmission_data.columns)\n",
    "\n",
    "    # We send the intermediary_var and unit_var to 'data_preperation.ipynb' to get the consolidated generation,load and transmission values.\n",
    "\n",
    "    consolidated_gen_data, consolidated_load_data, consolidated_transmission_data = dp.data_consolidation(copy.deepcopy(generation_dic), copy.deepcopy(load_dic), copy.deepcopy(transmission_data), intermediary_var, unit_var)\n",
    "\n",
    "    return(consolidated_gen_data, consolidated_load_data, consolidated_transmission_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Mismatch analysis in the consolidated ENTSO-E data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to 'omit_dst' function above, we use 'hourly_data' function to get the divider value based on dataframe length\n",
    "\n",
    "def hourly_data(df):\n",
    "    length = len(df.index)\n",
    "    if length == 35040:\n",
    "        divider = 4\n",
    "    elif length == 17520:\n",
    "        divider = 2\n",
    "    else:\n",
    "        divider = 1\n",
    "    return (df, divider)\n",
    "\n",
    "# In the following function we get the ENTSO-E statistical factsheet of 2018 to a dataframe called 'stat_data'\n",
    "# Then we selct only the country_name, generation,load,transmission_imports and transmission_exports columns and convert import,exports values to TWh which are stated in GWh\n",
    "# We create a list with the names of the three folders where the consolidated csvs are saved in each internal consolidation method.\n",
    "# We create another list with the names of the two folders where the consolidated csvs of 'Generation' and 'Load' files are saved.\n",
    "# Then we iterate through the rows of 'stat_data' dataframe.\n",
    "# We check if the country_abbreviation in 'stat_data''s country column is inside the 'selected_countries' list.\n",
    "# Then we get generation and load data of the raw unedited data from 'mismatch_data' dictionary and generation and load data of Nearest Neighbour Mean, Polynomial Linear Regression and Optimization based methods from their csv files in the respective folders.\n",
    "# We get the total sum value in each csv and get the annual energy value using the 'divider' variable and save the value in 'stat_data' dataframe.\n",
    "# Similarly we get the transmission imports and exports from the respective csvs by filtering the country_abbreviations in the column headers.\n",
    "\n",
    "def mismatch_analysis(mismatch_data,selected_countries):\n",
    "    stat_data = pd.read_excel('../Data Sources/ENTSO-E/entsoe_sfs2018_web.xlsx').iloc[1:28, ].reset_index(drop=True)\n",
    "    stat_data = stat_data[['Country', 'Total net generation','Consumption', 'Sum of imports', 'Sum of exports']]\n",
    "    stat_data.iloc[:, 3:5] = stat_data.iloc[:, 3:5]/1000\n",
    "\n",
    "    # methods = ['Nearest Neighbours Mean','Polynomial Linear Regression', 'Sigma']\n",
    "    types = ['Generation', 'Load']\n",
    "\n",
    "    for index, value in enumerate(stat_data.loc[:, 'Country']):\n",
    "\n",
    "        if value in selected_countries:\n",
    "\n",
    "            stat_data.loc[index, 'Raw_gen'] = mismatch_data[value][0]/1000000\n",
    "            stat_data.loc[index, 'Raw_load'] = mismatch_data[value][2]/1000000\n",
    "            stat_data.loc[index, 'Raw_imports'] = mismatch_data[value][1]/1000000\n",
    "            stat_data.loc[index, 'Raw_exports'] = mismatch_data[value][3]/1000000\n",
    "\n",
    "            for type in types:\n",
    "\n",
    "                df, divider = hourly_data(pd.read_csv(f'../Data Sources/output/Sigma/{type}/{value}.csv'))\n",
    "                stat_data.loc[index, f'{type}_Sigma'] = df.iloc[:, 1:].sum(axis=1).sum()/(divider*1000000)\n",
    "\n",
    "            df = pd.read_csv(f'../Data Sources/output/Sigma/Transmission/all_transmissions.csv').iloc[:, 1:]\n",
    "\n",
    "            imports = [x for x in df.columns.values if value in x[-2:]]\n",
    "            exports = [x for x in df.columns.values if value in x[:2]]\n",
    "\n",
    "            stat_data.loc[index, f'Imports_Sigma'] = df[imports].sum(axis=1).sum()/1000000\n",
    "            stat_data.loc[index, f'Exports_Sigma'] = df[exports].sum(axis=1).sum()/1000000\n",
    "        else:\n",
    "            stat_data = stat_data.drop(labels=index, axis=0)\n",
    "    stat_data = stat_data.reset_index(drop=True)\n",
    "\n",
    "    # We create new variable list xi where i belongs to [1,2]\n",
    "    # Each xi list contains 4 values\n",
    "    # After the iteration is completed x1 has the sum values of generation,load,imports and exports of raw unedited data of all the countries with the percentage difference of each of the 4 values compared to the respective generation,load,imports and exports sum values of ENTSO-E statistical data.\n",
    "    # Similarly x2 has the percentage dofference of the 4 values of generation,load,imports and exports of the optimization  based method.\n",
    "\n",
    "    for i in range(1, 3):\n",
    "        globals()[f'x{i}'] = []\n",
    "        for j in range(1, 5):\n",
    "            globals()[f'x{i}'].append(abs(stat_data.iloc[:, j].sum()-stat_data.iloc[:, i*4+j].sum())*100/stat_data.iloc[:, j].sum())\n",
    "\n",
    "    width = 0.18\n",
    "    labels = ['Actual generation per type', 'Actual load','Physical flow imports', 'Physical flow exports']\n",
    "    X = np.arange(4)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar([x-width/2 for x in X], x1, width, color='magenta', edgecolor='black', label='Raw data')\n",
    "    plt.bar([x+width/2 for x in X], x2, width, color='lightcoral', edgecolor='black', label='Internal consolidated data')\n",
    "    plt.xlabel('Data Items')\n",
    "    plt.ylabel('Percentage difference')\n",
    "    plt.title('Percentage difference with respect to ENTSO-E statistical data - 2018')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xticks(X, labels)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "acb9e0cfff3151089362f20c3a81c7531326aa2c5fdbbc2eccac94359104a8e2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

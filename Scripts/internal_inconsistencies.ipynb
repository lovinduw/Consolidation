{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Internal Inconsistencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import glob\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from IPython.display import display\r\n",
    "import import_ipynb\r\n",
    "import data_preperation as dp\r\n",
    "import copy\r\n",
    "import pyomo.environ as pyo\r\n",
    "from pyomo.opt import SolverFactory\r\n",
    "import openpyxl"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing Jupyter notebook from data_preperation.ipynb\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 2,
>>>>>>> ac7687d690a410fc6bb9065c8fe5e295d4acb087
   "source": [
    "countries={ 'Austria': 'AT', 'Belgium': 'BE',  'Bulgaria': 'BG', 'Switzerland': 'CH', 'Czech Republic': 'CZ', 'Germany': 'DE', 'Denmark': 'DK', 'Estonia': 'EE', 'Spain': 'ES', 'Finland': 'FI', 'France': 'FR',  'Greece': 'GR', 'Hungary': 'HU', 'Ireland': 'IE', 'Italy': 'IT', 'Lithuania': 'LT', 'Latvia': 'LV', 'Montenegro': 'ME','Netherlands': 'NL', 'Norway': 'NO', 'Poland': 'PL', 'Portugal': 'PT', 'Serbia': 'RS', 'Sweden': 'SE', 'Slovenia': 'SI', 'Slovakia': 'SK', 'United Kingdom': 'UK'}\r\n",
    "\r\n",
    "abbr_list=list(countries.values())\r\n",
    "\r\n",
    "# load_data = dp.load(countries)\r\n",
    "# generation_data = dp.generation(countries)\r\n",
    "# cross_border_data = dp.cross_border(abbr_list)[1]\r\n",
    "# import_export_using_crossborder_data = dp.import_export_using_crossborder(dp.cross_border(abbr_list)[0],abbr_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Internal inconsistencies based on unedited ENTSO-E data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Calculating missing values and mismatch analysis in data files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "source": [
=======
   "execution_count": 3,
   "source": [
    "def omit_dst(df):\r\n",
    "\r\n",
    "    # First we filter the columns in which the whole column is not 0 in the dataframe.\r\n",
    "    # Due to day light saving, all the datasets have null values on 25th March from 02:00 - 03:00. \r\n",
    "    # Considering the time intervals each country update the data, a total number of rows of 4,2 or 1 are dropped from the 31st March, 02:00 - 03:00 time interval.\r\n",
    "    # Also this returns a integer ('divider') based on the file length to get the energy values in a later step. In 15 min interval files this is 4, in 30 min interval files this is 2 and in 1 hour interval files this is 1.\r\n",
    "    \r\n",
    "    length = len(df)\r\n",
    "    df = df.replace(['n/e'], 0)\r\n",
    "    for column in df.columns.values:\r\n",
    "        if(df[column] == 0).all():\r\n",
    "            df = df.drop(column, axis=1)\r\n",
    "    if length == 35044:\r\n",
    "        df = df.drop(range(7976, 7980)).reset_index(drop=True)\r\n",
    "        divider = 4\r\n",
    "    elif length == 17522:\r\n",
    "        df = df.drop(range(3988, 3990)).reset_index(drop=True)\r\n",
    "        divider = 2\r\n",
    "    else:\r\n",
    "        df = df.drop(range(1994, 1995)).reset_index(drop=True)\r\n",
    "        divider = 1\r\n",
    "    \r\n",
    "    return(df,divider)\r\n",
    "\r\n",
>>>>>>> ac7687d690a410fc6bb9065c8fe5e295d4acb087
    "def mismatch_raw_data(countries):\r\n",
    "    columns = ['Country','demand']\r\n",
    "    for x in pd.read_csv('../Data Sources/ENTSO-E/2018/Generation/Croatia.csv').iloc[:, 2:].columns.values:\r\n",
    "        columns.append(x)\r\n",
    "    generation_load_missing_data = pd.DataFrame(columns=columns)\r\n",
    "    generation_load_missing_data['Country'] = list(countries.values())\r\n",
    "\r\n",
    "    transmission_data = pd.DataFrame()\r\n",
    "    rows = []\r\n",
    "\r\n",
    "    mismatch_data = {}\r\n",
    "    temp = pd.DataFrame()\r\n",
    "    csvs = glob.glob(\"../Data Sources/ENTSO-E/2018/Transmission/*.csv\")\r\n",
    "\r\n",
    "    # In the following command we read each csv file and drop the rows related to day light saving using 'omit_dst' function\r\n",
    "    # We create a new dataframe 'transmission_data_temp' with all the columns in all the csvs attached together.\r\n",
    "    # We apend the 'row' list with string edited column names of the 'temp' dataframe and number of N/A values in that column.\r\n",
<<<<<<< HEAD
    "    # Then we cretae two new column in the 'transmission_data' dataframe and name it as the two country codes the power transmission occurs.\r\n",
    "    # We use string editing to get the two country codes from the file path.\r\n",
    "    # Then we convert the 'rows' to a dataframe called 'transmission_missing_data' \r\n",
=======
    "    # Then one by one we copy each csv to 'temp' dataframe and make all the 'n/e' values of the 'temp' 0.\r\n",
    "    # Then we cretae two new column in the 'transmission_data' dataframe and name it as the two country codes the power transmission occurs.\r\n",
    "    # We use string editing to get the two country codes from the file path.\r\n",
    "    # Then we convert the 'rows' to a dataframe called 'transmission_missing_data' \r\n",
    "    # We read generation and load csvs of each country and remove the columns in generation data with all n/e values by converting them to 0s\r\n",
>>>>>>> ac7687d690a410fc6bb9065c8fe5e295d4acb087
    "    # Then we save the total number of null values in each generation type of each country in the respective column and row of 'generation_load_missing_data' dataframe\r\n",
    "    # Then we save the total number of null values in load data of each country in the 'demand' column of 'generation_load_missing_data'.\r\n",
    "    # Then we create a list of column names with all the columns of 'transmission_data_temp' if {country_abbreviations} is in the column name.\r\n",
    "    # Then we make a list of column heads of imports and exports associated with a given country_abbreviation\r\n",
    "    # We calculate the annual data mismatch of each country as (generation + imports - load - exports) of the country.\r\n",
    "    # Countries provide the data in 15 min, 30 min and 1 hour intervals.\r\n",
    "    # Therefore to ge the energy values from the power values, we divide the total power data by 4,2 or 1 using the 'divider' variable as required.\r\n",
    "\r\n",
    "    for csv in csvs:\r\n",
    "\r\n",
<<<<<<< HEAD
    "        temp, divider_transmission = dp.omit_dst(pd.read_csv(csv))\r\n",
=======
    "        temp, divider_transmission = omit_dst(pd.read_csv(csv))\r\n",
>>>>>>> ac7687d690a410fc6bb9065c8fe5e295d4acb087
    "        rows.append([f'{csv[42:44]} - > {csv[45:47]}',temp.iloc[:, 2].isnull().sum()])\r\n",
    "        rows.append([f'{csv[45:47]} - > {csv[42:44]}',temp.iloc[:, 1].isnull().sum()])\r\n",
    "\r\n",
    "        transmission_data[f'{csv[42:44]} - > {csv[45:47]}'] = pd.to_numeric(temp.iloc[:, 2],errors='coerce')/divider_transmission\r\n",
    "        transmission_data[f'{csv[45:47]} - > {csv[42:44]}'] = pd.to_numeric(temp.iloc[:, 1],errors='coerce')/divider_transmission\r\n",
    "        \r\n",
    "    transmission_missing_data = pd.DataFrame(rows,columns = ['Link','No of missing data'])\r\n",
    "\r\n",
    "    for country,abbr in countries.items():\r\n",
<<<<<<< HEAD
    "        load_data,divider_load = dp.omit_dst(pd.read_csv(f'../Data Sources/ENTSO-E/2018/Load/{country}.csv'))\r\n",
    "        generation_data, divider_gen = dp.omit_dst(pd.read_csv(f'../Data Sources/ENTSO-E/2018/Generation/{country}.csv', low_memory=False))\r\n",
    "             \r\n",
=======
    "        load_data,divider_load = omit_dst(pd.read_csv(f'../Data Sources/ENTSO-E/2018/Load/{country}.csv'))\r\n",
    "        generation_data, divider_gen = omit_dst(pd.read_csv(f'../Data Sources/ENTSO-E/2018/Generation/{country}.csv', low_memory=False))\r\n",
    "        \r\n",
    "        generation_data=generation_data.drop(['Hydro Pumped Storage  - Actual Consumption [MW]'],axis=1)\r\n",
    "        generation_data = generation_data.replace(['n/e'], 0)\r\n",
    "        for column in generation_data.columns.values:\r\n",
    "            if(generation_data[column] == 0).all():\r\n",
    "                generation_data = generation_data.drop(column, axis=1)\r\n",
    "        \r\n",
>>>>>>> ac7687d690a410fc6bb9065c8fe5e295d4acb087
    "        for index, row in enumerate(generation_load_missing_data.values):\r\n",
    "            if row[0] == abbr:\r\n",
    "                for column in generation_load_missing_data.iloc[:,2:]:\r\n",
    "                    if column in generation_data.columns.values:\r\n",
    "                        generation_load_missing_data.loc[index,column] = generation_data[column].isnull().sum()\r\n",
    "                    else:\r\n",
    "                        generation_load_missing_data.loc[index, column] = 'n/e'\r\n",
    "                generation_load_missing_data.loc[index,'demand'] = load_data.iloc[:,2].isnull().sum()\r\n",
    "\r\n",
    "        transmission_links = [x for x in transmission_data.columns.values if abbr in x]\r\n",
    "\r\n",
    "        imports=[x for x in transmission_data.columns.values if abbr in x[-2:]]\r\n",
    "        exports=[x for x in transmission_data.columns.values if abbr in x[:2]]\r\n",
    "\r\n",
    "        mismatch_data.update({f'{abbr}': [generation_data.iloc[:, 2:].sum(axis=1).sum()/divider_gen, transmission_data[imports].sum(\r\n",
<<<<<<< HEAD
    "            axis=1).sum(), load_data.iloc[:, 2].sum()/divider_load, transmission_data[exports].sum(axis=1).sum(), round(generation_data.iloc[:, 2:].sum(axis=1).sum()/divider_gen + transmission_data[imports].sum(\r\n",
    "                axis=1).sum() - load_data.iloc[:, 2].sum()/divider_load - transmission_data[exports].sum(axis=1).sum()/1000000, 2)]})\r\n",
=======
    "            axis=1).sum(), load_data.iloc[:, 2].sum()/divider_load, transmission_data[exports].sum(axis=1).sum(), round(abs(generation_data.iloc[:, 2:].sum(axis=1).sum()/divider_gen + transmission_data[imports].sum(\r\n",
    "                axis=1).sum() - load_data.iloc[:, 2].sum()/divider_load - transmission_data[exports].sum(axis=1).sum())/1000000, 2)]})\r\n",
>>>>>>> ac7687d690a410fc6bb9065c8fe5e295d4acb087
    "\r\n",
    "    for i in range(2, len(columns)):\r\n",
    "        columns[i] = columns[i][:-26]\r\n",
    "        generation_load_missing_data.columns = columns\r\n",
    "    generation_load_missing_data['Total missing generation data'] = generation_load_missing_data.iloc[:, 2:].apply(pd.to_numeric, errors='coerce').sum(numeric_only=True,axis=1)\r\n",
    "    display(generation_load_missing_data, transmission_missing_data)\r\n",
    "\r\n",
    "    mismatch_data_copy = mismatch_data.copy()\r\n",
    "    mismatch_data_copy = sorted(mismatch_data_copy.items(), key= lambda item: item[1][-1], reverse=True)\r\n",
    "\r\n",
    "    width = 0.35\r\n",
    "    labels = [x[0] for x in mismatch_data_copy]\r\n",
    "    X = np.arange(len(labels))\r\n",
    "    plt.figure(figsize=(20,10))\r\n",
    "    plt.bar([x for x in X], [x[1][-1] for x in mismatch_data_copy], width, color='aqua',edgecolor='black')\r\n",
    "    plt.xlabel('Countries')\r\n",
    "    plt.ylabel('Mismatch [TWh]')\r\n",
    "    plt.title('Mismatch analysis based on (generation + imports - load - exports) in raw ENTSO-E data')\r\n",
    "    plt.grid()\r\n",
<<<<<<< HEAD
    "    plt.axhline(y=0, color='black', linestyle='-')\r\n",
=======
>>>>>>> ac7687d690a410fc6bb9065c8fe5e295d4acb087
    "    plt.xticks(X,labels)\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "    return generation_load_missing_data, transmission_missing_data,mismatch_data\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Identifying inconsistencies in Generation data of ENTSO-E"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": null,
>>>>>>> ac7687d690a410fc6bb9065c8fe5e295d4acb087
   "source": [
    "def inconcistencies_generation_data():    \r\n",
    "    csvs = glob.glob(\"../Data Sources/ENTSO-E/2018/Generation/Generation per unit/*.csv\")\r\n",
    "    data = []\r\n",
    "\r\n",
    "    # The data obtained of the data item 'Actual Generation Output per Generation' in cvs format are saved in '../Data Sources/ENTSO-E/2018/Generation/Generation per unit' folder\r\n",
    "    # The data obtained of the data item 'Aggregated Generation per Type' in cvs format are saved in '../Data Sources/ENTSO-E/2018/Generation' folder\r\n",
    "    # We get a list of paths of csv files in the first folder using glob function.\r\n",
    "    # Using the path name, respective 'Aggregated Generation per Type' csv of the 'Actual Generation Output per Generation' file is opened with variable name 'df_type' and sent it to 'omit_dst' function.\r\n",
<<<<<<< HEAD
    "    # Then in every column is renamed by removing the ' Actual Aggregated [MW]' part of the column name.\r\n",
    "    # Then we open the relevant'Aggregated Generation per Type' csv with variable name 'df_gen'.\r\n",
    "    # In the 'df_gen' file, if 'Actual Consumption' columns are available, such columns are dropped only keeping the 'Actual Aggregated' columns.\r\n",
=======
    "    # Then the 'Hydro Pumped Storage  - Actual Consumption [MW]' column is dropped and 'n/e',null values are converted to 0.\r\n",
    "    # Then in every column is renamed by removing the ' Actual Aggregated [MW]' part of the column name.\r\n",
    "    # Then we open the relevant'Aggregated Generation per Type' csv with variable name 'df_gen'.\r\n",
    "    # In the 'df_gen' file, if 'Actual Consumption' columns are available such columns are dropped only keeping the 'Actual Aggregated' columns.\r\n",
>>>>>>> ac7687d690a410fc6bb9065c8fe5e295d4acb087
    "    # If 'df_gen' has 'Actual Consumption' columns, then we remove the first row and reset the index of the dataframe.\r\n",
    "    # Then we replace 'n/e' and null values with 0 and transpose the dataframe.\r\n",
    "    # In the transposed dataframe we get the set of generation type names from the 1st column to the 'generation_types' variable.\r\n",
    "    # Then we group the 'df_gen' using the 1st column.\r\n",
    "    # By iterating through each generation type we get a subgroup of the 'df_gen' filtered by the generation type.\r\n",
    "    # This subgroup contains only the data related to the power plants of that particular generation type.\r\n",
    "    # We get the sum of all the columns (Sum of all the power values of the power plants of that particular generation type)\r\n",
    "    # Then we append the 'data' list with country_name, generation_type, sum value of previous step and sum of the column of that particular generation type in 'df_type' dataframe.\r\n",
    "    # Finally we convert this list to a dataframe.\r\n",
    "\r\n",
    "    for csv in csvs:\r\n",
    "\r\n",
<<<<<<< HEAD
    "        df_type,length = dp.omit_dst(pd.read_csv(f'../Data Sources/ENTSO-E/2018/Generation/{csv[60:-4]}.csv', low_memory=False).iloc[:,2:])\r\n",
    "        df_type = df_type.replace('n/e', 0)\r\n",
=======
    "        df_type,length = omit_dst(pd.read_csv(f'../Data Sources/ENTSO-E/2018/Generation/{csv[60:-4]}.csv', low_memory=False).iloc[:,2:])\r\n",
    "        df_type = df_type.drop(['Hydro Pumped Storage  - Actual Consumption [MW]'], axis=1)\r\n",
    "        df_type = df_type.replace(['n/e', np.nan], 0)\r\n",
>>>>>>> ac7687d690a410fc6bb9065c8fe5e295d4acb087
    "        columns = [column[:-26] for column in df_type.columns.values]\r\n",
    "        df_type.columns = columns\r\n",
    "\r\n",
    "        df_gen = pd.read_csv(csv,low_memory=False).iloc[:, 1:]\r\n",
    "\r\n",
    "        inc = False\r\n",
    "        for column in df_gen.columns.values:\r\n",
    "            if df_gen.loc[1, column] == 'Actual Consumption':\r\n",
    "                df_gen = df_gen.drop(column, axis=1)\r\n",
    "                inc = True\r\n",
    "        if inc:\r\n",
<<<<<<< HEAD
    "            df_gen = df_gen.drop(index=1).reset_index(drop=True)\r\n",
=======
    "            df_gen = df_gen.drop(index=1)\r\n",
    "            df_gen = df_gen.reset_index(drop=True)\r\n",
>>>>>>> ac7687d690a410fc6bb9065c8fe5e295d4acb087
    "\r\n",
    "        df_gen = df_gen.replace(['n/e', np.nan], 0)\r\n",
    "        df_gen = df_gen.T\r\n",
    "\r\n",
    "        generation_types = list(set(df_gen.iloc[:, 0].values))\r\n",
    "        df_gen = df_gen.groupby(df_gen.iloc[:, 0])\r\n",
    "        for item in generation_types:\r\n",
    "            selected_data = df_gen.get_group(item).iloc[:, 1:]\r\n",
    "            tot = 0\r\n",
    "            for column in selected_data.columns.values:\r\n",
    "                tot += pd.to_numeric(selected_data.loc[:, column]).sum()\r\n",
    "            print(csv[60:-4], item, tot, df_type.loc[:, item].sum()/length)\r\n",
    "            data.append([csv[60:-4], item, tot, df_type.loc[:, item].sum()/length])\r\n",
    "\r\n",
    "    df = pd.DataFrame(data,columns=['Country','Generation type','Based on per generator','Based on per type'])\r\n",
    "\r\n",
    "    return df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Consolidation based on internally gap filled ENTSO-E data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Nearest Neighbours Mean value based internal approach"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Polynomial linear regression based internal approach"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 27,
>>>>>>> ac7687d690a410fc6bb9065c8fe5e295d4acb087
   "source": [
    "# # In the following section we take each generation,load and transmission csvs and send them to the 'function' function.\r\n",
    "# # This function send each dataframe to 'omit_dst' function and removes the empty observations on March 28th and get the 'divider' value which is associated with the length of the dataframe.\r\n",
    "# # Then we replace 'n/e' values with 0\r\n",
    "\r\n",
    "# def function(df,location):\r\n",
    "#     edited_df = pd.DataFrame()\r\n",
    "#     df, divider = omit_dst(df)\r\n",
    "\r\n",
    "#     # Then we check the dataframe column by column.\r\n",
    "#     # Then we add the index and value of each value in the column to a list called 'column_data'\r\n",
    "#     # Then we check value by value in the column untill a null value is found.\r\n",
    "#     # If the value is a null value, we get all the values in that column which are the values associated with the same time step of the null value and the absolute difference of the indexes of that value and the null value to the 'temp_list'.\r\n",
    "#     # For example if the null value occurus at 10:00 - 10:15 time step of some day, all the values associated with 10:00 - 10:15 time step throughout the year and and the absolute difference of the indexes of those value and the null value go to the 'temp_list'\r\n",
    "#     # Then the 'temp_list' is sorted and the first 60 values are taken to the 'nearest_neighbours' list\r\n",
    "#     # Here in the 'nearest_neighbours' list we get the values of the nearest 60 days of the same time step in which the null value has occured.\r\n",
    "#     # Then we replace any null values in the 'nearest_neighbours' list with 0\r\n",
    "#     # Then we get the mean value of the 'nearest_neighbours' list and assign that value to the original null value in the dataframe\r\n",
    "\r\n",
    "#     if location == 'Nearest Neighbours Mean':\r\n",
    "#         for column in df.columns.values:\r\n",
    "#             column_data = [[index, value] for index, value in enumerate(df.loc[:, column])]\r\n",
    "#             for index, value in column_data:\r\n",
    "#                 if pd.isnull(value):\r\n",
    "#                     temp_list = [[abs(index-item[0]), item[1]] for item in column_data if (index-item[0]) % (24*divider) == 0]\r\n",
    "#                     temp_list.sort()\r\n",
    "#                     nearest_neighbours = [x[1] for x in temp_list[1:61]]\r\n",
    "#                     nearest_neighbours = [0 if pd.isna(x) else x for x in nearest_neighbours]\r\n",
    "#                     df.loc[index, column] = np.mean(np.array(nearest_neighbours))\r\n",
    "#             edited_df = pd.concat([edited_df, df.loc[:, column]], axis=1)\r\n",
    "#         return edited_df\r\n",
    "\r\n",
    "#     # In the following section we take each generation,load and transmission csvs and send them to the 'neareset_neighbours_mean' function.\r\n",
    "#     # This function send each dataframe to 'omit_dst' function and removes the empty observations on March 28th and get the 'divider' value which is associated with the length of the dataframe.\r\n",
    "#     # Then we replace 'n/e' values with 0\r\n",
    "#     # Then we check the dataframe column by column.\r\n",
    "#     # First we filter the columns in which the whole column is not 0 in the dataframe.\r\n",
    "#     # Then we add the index and value of each value in the column to a list called 'column_data'\r\n",
    "#     # Then we check value by value in the column untill a null value is found.\r\n",
    "#     # If the value is a null value, we get all the values in that column which are the values associated with the same time step of the null value and the absolute difference of the indexes of that value and the null value to the 'temp_list'.\r\n",
    "#     # For example if the null value occurus at 10:00 - 10:15 time step of some day, all the values associated with 10:00 - 10:15 time step throughout the year and and the absolute difference of the indexes of those value and the null value go to the 'temp_list'\r\n",
    "#     # Then the 'temp_list' is sorted and we get all the indexes in the 'temp_list' except the index of the null value to a 2D numpy array 'X'.\r\n",
    "#     # We get all the values in the 'temp_list' except the value of the null value to a 1D numpy array 'y'\r\n",
    "#     # Then we divide 'X' and 'y' values in the ratio of 30% test and 70% train data.\r\n",
    "#     # We create an array of degree values from 1 to 10.\r\n",
    "#     # Then we iterate the 'degrees' one by one and create polynomial values of 'x_train' data called 'x_poly_train' based on the value of the degree\r\n",
    "#     # Then we fit the polynomial linear regression function using 'x_poly_train' data and 'y_train' data.\r\n",
    "#     # Then based on the polynomial function, using the 'x_poly_test' data we predict the values of the 'y_test' data\r\n",
    "#     # Then based on the predicted values and 'y_test\" data we calculate the Root Mean Square Error.\r\n",
    "#     # Applying the last 4 steps for each degree value, we select the degree value which gives the Lowest Root Mean Square Error.\r\n",
    "#     # Then we fit the polynomial linear regression function again using that degree which gives the Lowest Root Mean Square Error.\r\n",
    "#     # Based on the polynimial function we get the predicted value of the null value.\r\n",
    "    \r\n",
    "#     elif location == 'Polynomial Linear Regression':\r\n",
    "#         for column in df.columns.values:\r\n",
    "#             column_data = [[index, value] for index, value in enumerate(df.loc[:, column])]\r\n",
    "#             for index, value in column_data:\r\n",
    "#                 if pd.isnull(value):\r\n",
    "#                     temp_list = [item for item in column_data if (index-item[0]) % (24*divider) == 0]\r\n",
    "#                     temp_list.sort()\r\n",
    "#                     X = np.array([i[0] for i in temp_list[1:]]).reshape(len(temp_list)-1, 1)\r\n",
    "#                     y = [i[1] for i in temp_list[1:]]\r\n",
    "#                     y = [0 if pd.isna(x) else x for x in y]\r\n",
    "\r\n",
    "#                     x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\r\n",
    "\r\n",
    "#                     degrees = np.arange(1, 11)\r\n",
    "#                     min_rmse, min_deg = 1e10, 0\r\n",
    "\r\n",
    "#                     for degree in degrees:\r\n",
    "\r\n",
    "#                         # Preparing polynomial Train features based on x_train\r\n",
    "#                         poly_features = PolynomialFeatures(degree=degree, include_bias=False)\r\n",
    "#                         x_poly_train = poly_features.fit_transform(x_train)\r\n",
    "\r\n",
    "#                         # Polynomial linear regression based on train data\r\n",
    "#                         poly_reg = LinearRegression()\r\n",
    "#                         poly_reg.fit(x_poly_train, y_train)\r\n",
    "\r\n",
    "#                         # Predicting y values and getting root mean squared error based on predicted y values and y_test values\r\n",
    "#                         x_poly_test = poly_features.fit_transform(x_test)\r\n",
    "#                         poly_predict = poly_reg.predict(x_poly_test)\r\n",
    "#                         poly_mse = mean_squared_error(y_test, poly_predict)\r\n",
    "#                         poly_rmse = np.sqrt(poly_mse)\r\n",
    "\r\n",
    "#                         # Selecting the best degree of the polynimial function based on lowest root mean squared error\r\n",
    "#                         if min_rmse > poly_rmse:\r\n",
    "#                             min_rmse = poly_rmse\r\n",
    "#                             min_deg = degree\r\n",
    "\r\n",
    "#                     # Fitting the regression function again based on the selected best degree above\r\n",
    "#                     poly_features = PolynomialFeatures(degree=min_deg, include_bias=False)\r\n",
    "#                     x_poly_train = poly_features.fit_transform(x_train)\r\n",
    "#                     poly_reg = LinearRegression()\r\n",
    "#                     poly_reg.fit(x_poly_train, y_train)\r\n",
    "\r\n",
    "#                     df.loc[index,column] = poly_reg.predict(poly_features.fit_transform([[0]]))[0]\r\n",
    "#             edited_df = pd.concat([edited_df, df.loc[:, column]], axis=1) \r\n",
    "#         edited_df[edited_df<0]=0\r\n",
    "#         return edited_df\r\n",
    "    \r\n",
    "#     else:\r\n",
    "#         print('WRONG LOCATION SENT')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 5,
>>>>>>> ac7687d690a410fc6bb9065c8fe5e295d4acb087
   "source": [
    "# def polynomial(selected_values,selected_index):\r\n",
    "#     X = np.array([i[0] for i in selected_values]).reshape(len(selected_values), 1)\r\n",
    "#     y = [i[1] for i in selected_values]\r\n",
    "#     y = [0 if pd.isna(x) else x for x in y]\r\n",
    "\r\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=1/3)\r\n",
    "\r\n",
    "#     degrees = np.arange(1, 11)\r\n",
    "#     min_rmse, min_deg = 1e10, 0\r\n",
    "\r\n",
    "#     for degree in degrees:\r\n",
    "\r\n",
    "#         # Preparing polynomial Train features based on x_train\r\n",
    "#         poly_features = PolynomialFeatures(degree=degree, include_bias=False)\r\n",
    "#         x_poly_train = poly_features.fit_transform(x_train)\r\n",
    "\r\n",
    "#         # Polynomial linear regression based on train data\r\n",
    "#         poly_reg = LinearRegression()\r\n",
    "#         poly_reg.fit(x_poly_train, y_train)\r\n",
    "\r\n",
    "#         # Predicting y values and getting root mean squared error based on predicted y values and y_test values\r\n",
    "#         x_poly_test = poly_features.fit_transform(x_test)\r\n",
    "#         poly_predict = poly_reg.predict(x_poly_test)\r\n",
    "#         poly_mse = mean_squared_error(y_test, poly_predict)\r\n",
    "#         poly_rmse = np.sqrt(poly_mse)\r\n",
    "\r\n",
    "#         # Selecting the best degree of the polynimial function based on lowest root mean squared error\r\n",
    "#         if min_rmse > poly_rmse:\r\n",
    "#             min_rmse = poly_rmse\r\n",
    "#             min_deg = degree\r\n",
    "\r\n",
    "#     # Fitting the regression function again based on the selected best degree above\r\n",
    "#     poly_features = PolynomialFeatures(degree=min_deg, include_bias=False)\r\n",
    "#     x_poly_train = poly_features.fit_transform(x_train)\r\n",
    "#     poly_reg = LinearRegression()\r\n",
    "#     poly_reg.fit(x_poly_train, y_train)\r\n",
    "\r\n",
    "#     prediction = poly_reg.predict(poly_features.fit_transform([[selected_index]]))[0]\r\n",
    "#     if prediction<0:\r\n",
    "#         prediction = 0\r\n",
    "        \r\n",
    "#     return(prediction)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 11,
>>>>>>> ac7687d690a410fc6bb9065c8fe5e295d4acb087
   "source": [
    "# def fill_missing_data(df,length):\r\n",
    "\r\n",
    "#     counter =0  \r\n",
    "#     for column in df.columns.values:\r\n",
    "#         column_data = {}\r\n",
    "#         for index, value in enumerate(df.loc[:, column]):\r\n",
    "#             column_data[index] = value\r\n",
    "\r\n",
    "#         for selected_index, selected_value in column_data.items():\r\n",
    "\r\n",
    "#             if pd.isnull(column_data[selected_index]) and selected_index in range(3*length, len(df[column])-3*length):\r\n",
    "#                 selected_values = []\r\n",
    "#                 for i in [x for x in range(-3,4) if x!=0]:\r\n",
    "#                     selected_values.append([selected_index + i*length,column_data[selected_index + i*length]])\r\n",
    "#                 if pd.isnull(selected_values).sum() >=3 and selected_index in range(27*length, len(df[column])-27*length):\r\n",
    "#                     selected_values = []\r\n",
    "#                     for i in [x for x in range(-3, 4) if x != 0]:\r\n",
    "#                         for j in [-24,0,24]:\r\n",
    "#                             selected_values.append([selected_index + (i+j)*length,column_data[selected_index + (i+j)*length]])\r\n",
    "#                     if pd.isnull(selected_values).sum() >= 12 and selected_index in range(51*length, len(df[column])-51*length):\r\n",
    "#                         selected_values = []\r\n",
    "#                         for i in [x for x in range(-3, 4) if x != 0]:\r\n",
    "#                             for j in [-48,-24,0, 24,48]:\r\n",
    "#                                 selected_values.append([selected_index + (i+j)*length,column_data[selected_index + (i+j)*length]])\r\n",
    "#                         # print(selected_values)\r\n",
    "#                         if pd.isnull(selected_values).sum() >= 24 and pd.isnull(selected_values).sum() < len(selected_values):\r\n",
    "#                             prediction = np.mean(np.array([i[1] for i in selected_values]))\r\n",
    "#                             df.loc[selected_index,column] = prediction\r\n",
    "#                             counter +=1\r\n",
    "\r\n",
    "#                         elif pd.isnull(selected_values).sum() < 24:\r\n",
    "#                             prediction = polynomial(selected_values,selected_index)\r\n",
    "#                             df.loc[selected_index, column] = prediction\r\n",
    "#                             counter += 1\r\n",
    "\r\n",
    "#                         else:\r\n",
    "#                             df.loc[selected_index, column] = 0\r\n",
    "#                             counter += 1\r\n",
    "\r\n",
    "#                     elif pd.isnull(selected_values).sum() >= 12:\r\n",
    "#                         prediction = np.mean(np.array([i[1] for i in selected_values]))\r\n",
    "#                         df.loc[selected_index, column] = prediction\r\n",
    "#                         counter += 1\r\n",
    "\r\n",
    "#                     else:\r\n",
    "#                         prediction = polynomial(selected_values,selected_index)\r\n",
    "#                         df.loc[selected_index, column] = prediction\r\n",
    "#                         counter += 1\r\n",
    "\r\n",
    "#                 elif pd.isnull(selected_values).sum() >= 3:\r\n",
    "#                     prediction = np.mean(np.array([i[1] for i in selected_values]))\r\n",
    "#                     df.loc[selected_index, column] = prediction\r\n",
    "#                     counter += 1\r\n",
    "\r\n",
    "#                 else:\r\n",
    "#                     prediction = polynomial(selected_values, selected_index)\r\n",
    "#                     df.loc[selected_index, column] = prediction\r\n",
    "#                     counter += 1\r\n",
    "\r\n",
    "#             elif pd.isnull(column_data[selected_index]) and selected_index < 3*length: \r\n",
    "#                 prediction = np.mean(np.array([column_data[i] for i in range(3*length)]))\r\n",
    "#                 df.loc[selected_index, column] = prediction\r\n",
    "#                 counter += 1\r\n",
    "\r\n",
    "#             elif pd.isnull(column_data[selected_index]) and selected_index >= (len(df[column])-3*length): \r\n",
    "#                 prediction = np.mean(np.array([column_data[i] for i in range(len(df[column])-3*length,len(df[column]))]))\r\n",
    "#                 df.loc[selected_index, column] = prediction\r\n",
    "#                 counter += 1\r\n",
    "                \r\n",
    "#     return df,counter\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 12,
>>>>>>> ac7687d690a410fc6bb9065c8fe5e295d4acb087
   "source": [
    "# def consolidation(countries, location):\r\n",
    "#     for country, abbr in countries.items():\r\n",
    "#         df_load,length = omit_dst(pd.read_csv(f'../Data Sources/ENTSO-E/2018/Load/{country}.csv').iloc[:, 2:])\r\n",
    "#         df_load = df_load.rename(columns={df_load.columns[0]: 'demand'})\r\n",
    "#         load_consolidated,counter = fill_missing_data(df_load, length)\r\n",
    "#         print(f'{country} - Load: {counter} missing data filled')\r\n",
    "#         load_consolidated.to_csv(f'../Data Sources/output/{location}/Load/{abbr}.csv')\r\n",
    "\r\n",
    "#         df_gen,length = omit_dst(pd.read_csv(f'../Data Sources/ENTSO-E/2018/Generation/{country}.csv').iloc[:, 2:])\r\n",
    "#         df_gen = df_gen.drop(['Hydro Pumped Storage  - Actual Consumption [MW]'], axis=1)\r\n",
    "#         gen_consolidated,counter = fill_missing_data(df_gen, length)\r\n",
    "#         print(f'{country} - Generation: {counter} missing data filled')\r\n",
    "#         gen_consolidated.to_csv(f'../Data Sources/output/{location}/Generation/{abbr}.csv')\r\n",
    "#     print('LOAD & GENERATION DATA CONSOLIDATED')\r\n",
    "\r\n",
    "#     transmission_consolidated = pd.DataFrame()\r\n",
    "#     csvs = glob.glob(\"../Data Sources/ENTSO-E/2018/Transmission/*.csv\")\r\n",
    "#     for csv in csvs:\r\n",
    "#         df,length = omit_dst(pd.read_csv(csv).iloc[:, 1:])\r\n",
    "#         df = df.rename(columns={df.columns[0]: f'{csv[45:47]} - > {csv[42:44]}', df.columns[1]: f'{csv[42:44]} - > {csv[45:47]}'})\r\n",
    "#         transmission_consolidated = pd.concat([transmission_consolidated, fill_missing_data(df,length)[0]], axis=1)\r\n",
    "#     transmission_consolidated.to_csv(f'../Data Sources/output/{location}/Transmission/all_transmissions.csv')\r\n",
    "#     print('TRANSMISSION DATA CONSOLIDATED')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 13,
>>>>>>> ac7687d690a410fc6bb9065c8fe5e295d4acb087
   "source": [
    "# consolidation(countries, 'Polynomial Linear Regression')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Austria - Load: 0 missing data filled\n",
      "Austria - Generation: 0 missing data filled\n",
      "Belgium - Load: 0 missing data filled\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Optimization based internal approach"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 3,
>>>>>>> ac7687d690a410fc6bb9065c8fe5e295d4acb087
   "source": [
    "def sigma(load_dic, generation_dic, transmission_data, countries):\r\n",
    "\r\n",
    "    abbr_list = list(countries.values())\r\n",
    "    load_dic_copy = copy.deepcopy(load_dic)\r\n",
    "    generation_dic_copy = copy.deepcopy(generation_dic)\r\n",
    "\r\n",
    "    # We filter the transmission links between given two countries in which if both countries associated with the power transmission are included in our country_abbreviation list.\r\n",
    "    # For example power import(export) occurs from(to) a country other than the countries in the abbreviation_list (for ex: 'Cyprus','Turkey' etc.) are omitted.\r\n",
    "\r\n",
    "    transmission_lines = list([x for x in transmission_data.columns.values if x[:2] in abbr_list and x[-2:] in abbr_list])\r\n",
    "    transmission_data = transmission_data[transmission_lines]\r\n",
    "    transmission_data_copy = copy.deepcopy(transmission_data)\r\n",
    "\r\n",
    "    # In the following command, we calculate the sigma value by sending it to 'data_preperation.ipynb'\r\n",
    "\r\n",
    "    sigma = dp.calculate_sigma(load_dic_copy, generation_dic_copy, transmission_data_copy, abbr_list)\r\n",
    "    print('SIGMA CALCULATED')\r\n",
    "\r\n",
    "    # In the follwoing commands we create a 'country_index' with the names abbreviations of the 27 countries we consider.\r\n",
    "    # Then we create a 'time_index' which is a list of integers from 0 to 8760 which indicates the timesteps \r\n",
    "    # Then we create a 'generation_index' which is a dictionary with country_abbreviations as keys and generation_sources os each country as the values of each key.\r\n",
    "    # Then we create a 'generation_fuels' which is a list of all the generation_sources we consider.\r\n",
    "    # Then we create 2 dictionaries 'imports_index' and 'exports_index' with country_abbreviations as the keys.\r\n",
    "    # In the column heads of 'transmission_data' dataframe, if the name of the key is in the first two characters of the column head, that column head goes as the value in 'export_index' dictionary\r\n",
    "    # In the column heads of 'transmission_data' dataframe, if the name of the key is in the last two characters of the column head, that column head goes as the value in 'import_index' dictionary\r\n",
    "    # Then we create a 'transmission_index' with all the column heads of 'transmission_data' dataframe.\r\n",
    "\r\n",
    "    country_index = list(countries.values())\r\n",
    "    time_index = np.arange(8760, dtype=int)\r\n",
    "\r\n",
    "    generation_index = {}\r\n",
    "\r\n",
    "    for abbr, df in generation_dic.items():\r\n",
    "        generation_index[abbr] = [x for x in df.columns.values]\r\n",
    "\r\n",
    "    generation_fuels = np.array(list(set([item for sublist in generation_index.values() for item in sublist])))\r\n",
    "    generation_fuels.sort()\r\n",
    "\r\n",
    "    imports_index = {}\r\n",
    "    exports_index = {}\r\n",
    "\r\n",
    "    for abbr in abbr_list:\r\n",
    "        imports_index[abbr] = [x for x in transmission_data.columns.values if abbr in x[-2:]]\r\n",
    "        exports_index[abbr] = [x for x in transmission_data.columns.values if abbr in x[:2]]\r\n",
    "\r\n",
    "    transmission_index = np.array(transmission_data.columns.values)\r\n",
    "    transmission_index.sort()\r\n",
    "\r\n",
    "    # In the following commands, we initiate the pyomo optimization with 'Gurobi' solver\r\n",
    "    # We declare the three variables 'delta_generation', 'delta_load' and 'delta_transmission'\r\n",
    "    # 'delta_generation' consists of 2,207,520 values which vary with country, generation_source and timestep.\r\n",
    "    # 'delta_load' consists of 236,520 values which vary with country and timestep.\r\n",
    "    # 'delta_transmission' consists of 858,480 values which vary with timestep and with country indirectly.\r\n",
    "    # We declare the model constraint as the sum of (delta_generation + generation + delta_transmission(imports) + imports) is equal to the sum of (delta_load + load + delta_transmission(exports) + exports) in all time steps.\r\n",
    "    # We declare objective function as to MINIMIZE the sum of (delta_generation^2 * sigma(generaion)) + sum of (delta_load^2 * sigma(load)) + sum of (delta_transmission^2 * sigma(transmission)) in all timesteps.\r\n",
    "    # Then solve the model.\r\n",
    "\r\n",
    "    model = pyo.ConcreteModel()\r\n",
    "\r\n",
    "    model.country_index = pyo.Set(initialize=country_index)\r\n",
    "    model.time_index = pyo.Set(initialize=time_index)\r\n",
    "    model.generation_fuels = pyo.Set(initialize=generation_fuels)\r\n",
    "    model.transmission_index = pyo.Set(initialize=transmission_index)\r\n",
    "\r\n",
    "    model.delta_generation = pyo.Var(model.country_index, model.time_index, model.generation_fuels, bounds=(0.0, None))\r\n",
    "    model.delta_load = pyo.Var(model.country_index, model.time_index, bounds=(0.0, None))\r\n",
    "    model.delta_transmission = pyo.Var(model.time_index, model.transmission_index, bounds=(0.0, None))\r\n",
    "\r\n",
    "    print('VARIABLES DECLARED')\r\n",
    "\r\n",
    "    def balance_rule(model, country, time):\r\n",
    "        return sum(model.delta_generation[country, time, generation] + generation_dic[country][generation][time] for generation in generation_index[country]) + sum(model.delta_transmission[time, link] + transmission_data[link][time] for link in imports_index[country]) ==  \\\r\n",
    "            model.delta_load[country, time] + load_dic[country][\"demand\"][time] + \\\r\n",
    "            sum(model.delta_transmission[time, link] + transmission_data[link][time] for link in exports_index[country])\r\n",
    "    model.balance_rule = pyo.Constraint(model.country_index, model.time_index, rule=balance_rule)\r\n",
    "\r\n",
    "    def ObjRule(model):\r\n",
    "        return sum(model.delta_generation[country, time, generation] ** 2 * float(sigma[country][generation].iloc[time]) for country in model.country_index for time in model.time_index for generation in generation_index[country]) \\\r\n",
    "            + sum(model.delta_transmission[time, link] ** 2 * float(sigma[\"transmission_data\"][link].iloc[time]) for time in model.time_index for link in model.transmission_index)\\\r\n",
    "            + sum(model.delta_load[country, time] ** 2 * float(sigma[country][\"demand\"].iloc[time]) for country in model.country_index for time in model.time_index)\r\n",
    "\r\n",
    "    model.obj = pyo.Objective(rule=ObjRule, sense=pyo.minimize)\r\n",
    "    opt = SolverFactory(\"gurobi\", solver_io=\"python\")\r\n",
    "    opt.solve(model)\r\n",
    "    print('OPTIMIZATION COMPLETED')\r\n",
    "\r\n",
    "    # In the following commands we copy the pyomo results into different intermediary variables.\r\n",
    "    # We create 'intermediary_var' dictionary and inside it create two other dictionaries as keys called 'generation' and 'load' and create a dataframe as another key called 'transmission'.\r\n",
    "    # We create 'unit_var' dictionary and inside it create two other dictionaries as keys called 'generation' and 'load' and create a dataframe as another key called 'transmission'.\r\n",
    "    # We fill the 'intermediary_var[\"generation\"]' from the 'delta_generation' values and 'intermediary_var[\"load\"]' with 'delta_load' values and 'intermediary_var[\"transmission\"]' from 'delta_transmission' values.\r\n",
    "    # We fill 'unit_var[\"generation\"]', 'unit_var[\"load\"]' and 'unit_var[\"transmission\"]' which have the same size as of 'intermediary_var[\"generation\"]', 'intermediary_var[\"load\"]' and ''intermediary_var[\"transmission\"]' with integer 1.\r\n",
    "\r\n",
    "    generation_index_copy = copy.deepcopy(generation_index)\r\n",
    "\r\n",
    "    intermediary_var = {}\r\n",
    "    intermediary_var[\"generation\"] = {}\r\n",
    "    intermediary_var[\"load\"] = {}\r\n",
    "    unit_var = {}\r\n",
    "    unit_var[\"generation\"] = {}\r\n",
    "    unit_var[\"load\"] = {}\r\n",
    "\r\n",
    "    for country in country_index:\r\n",
    "        table_gen = []\r\n",
    "        row_load = []\r\n",
    "        for time in time_index:\r\n",
    "            row_gen = []\r\n",
    "            for generation in generation_index_copy[country]:\r\n",
    "                row_gen.append(model.delta_generation[country, time, generation].value)\r\n",
    "            table_gen.append(row_gen)\r\n",
    "            row_load.append(model.delta_load[country, time].value)\r\n",
    "\r\n",
    "        intermediary_var[\"generation\"][country] = pd.DataFrame.from_records(table_gen)\r\n",
    "        intermediary_var[\"load\"][country] = pd.DataFrame(row_load)\r\n",
    "        intermediary_var[\"generation\"][country].columns = generation_index_copy[country]\r\n",
    "        intermediary_var[\"load\"][country].columns = ['demand']\r\n",
    "        intermediary_var[\"generation\"][country].to_csv(\"../Data Sources/output/Sigma/Debugging/\" + \"internal_sigma_gen_\" + country + \".csv\")\r\n",
    "        intermediary_var[\"load\"][country].to_csv(\"../Data Sources/output/Sigma/Debugging/\" + \"internal_sigma_load_\" + country + \".csv\")\r\n",
    "        unit_var[\"generation\"][country] = pd.DataFrame(1, index=np.arange(df.shape[0]), columns=generation_index_copy[country])\r\n",
    "        unit_var[\"load\"][country] = pd.DataFrame(1, index=np.arange(df.shape[0]), columns=['demand'])\r\n",
    "\r\n",
    "    table_transmission = []\r\n",
    "\r\n",
    "    for time in time_index:\r\n",
    "        row = []\r\n",
    "        for link in transmission_index:\r\n",
    "            row.append(model.delta_transmission[time, link].value)\r\n",
    "        table_transmission.append(row)\r\n",
    "    intermediary_var[\"transmission\"] = pd.DataFrame.from_records(table_transmission)\r\n",
    "    intermediary_var[\"transmission\"].columns = transmission_index\r\n",
    "    intermediary_var[\"transmission\"].to_csv(\"../Data Sources/output/Sigma/Debugging/internal_sigma_transmission.csv\")\r\n",
    "    unit_var[\"transmission\"] = pd.DataFrame(1, index=np.arange(transmission_data.shape[0]), columns=transmission_data.columns)\r\n",
    "\r\n",
    "    # We send the intermediary_var and unit_var to 'data_preperation.ipynb' to get the consolidated generation,load and transmission values.\r\n",
    "\r\n",
    "    consolidated_gen_data, consolidated_load_data, consolidated_transmission_data = dp.data_consolidation(generation_dic, load_dic, transmission_data, intermediary_var, unit_var)\r\n",
    "\r\n",
    "    return(consolidated_gen_data, consolidated_load_data, consolidated_transmission_data)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 Mismatch analysis in the consolidated ENTSO-E data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 7,
>>>>>>> ac7687d690a410fc6bb9065c8fe5e295d4acb087
   "source": [
    "# Similar to 'omit_dst' function above, we use 'hourly_data' function to get the divider value based on dataframe length\r\n",
    "\r\n",
    "def hourly_data(df):\r\n",
    "    length = len(df.index)\r\n",
    "    if length == 35040:\r\n",
    "        divider = 4\r\n",
    "    elif length == 17520:\r\n",
    "        divider = 2\r\n",
    "    else:\r\n",
    "        divider = 1\r\n",
    "    return (df, divider)\r\n",
    "\r\n",
    "# In the following function we get the ENTSO-E statistical factsheet of 2018 to a dataframe called 'stat_data'\r\n",
    "# Then we selct only the country_name, generation,load,transmission_imports and transmission_exports columns and convert import,exports values to TWh which are stated in GWh\r\n",
    "# We create a list with the names of the three folders where the consolidated csvs are saved in each internal consolidation method.\r\n",
    "# We create another list with the names of the two folders where the consolidated csvs of 'Generation' and 'Load' files are saved.\r\n",
    "# Then we iterate through the rows of 'stat_data' dataframe.\r\n",
    "# We check if the country_abbreviation in 'stat_data''s country column is inside the 'selected_countries' list.\r\n",
    "# Then we get generation and load data of the raw unedited data from 'mismatch_data' dictionary and generation and load data of Nearest Neighbour Mean, Polynomial Linear Regression and Optimization based methods from their csv files in the respective folders.\r\n",
    "# We get the total sum value in each csv and get the annual energy value using the 'divider' variable and save the value in 'stat_data' dataframe.\r\n",
    "# Similarly we get the transmission imports and exports from the respective csvs by filtering the country_abbreviations in the column headers.\r\n",
    "\r\n",
    "def mismatch_analysis(mismatch_data,selected_countries):\r\n",
    "    stat_data = pd.read_excel('../Data Sources/ENTSO-E/entsoe_sfs2018_web.xlsx').iloc[1:28, ].reset_index(drop=True)\r\n",
    "    stat_data = stat_data[['Country', 'Total net generation','Consumption', 'Sum of imports', 'Sum of exports']]\r\n",
    "    stat_data.iloc[:, 3:5] = stat_data.iloc[:, 3:5]/1000\r\n",
    "\r\n",
    "    methods = ['Nearest Neighbours Mean','Polynomial Linear Regression', 'Sigma']\r\n",
    "    types = ['Generation', 'Load']\r\n",
    "\r\n",
    "    for index, value in enumerate(stat_data.loc[:, 'Country']):\r\n",
    "\r\n",
    "        if value in selected_countries:\r\n",
    "\r\n",
    "            stat_data.loc[index, 'Raw_gen'] = mismatch_data[value][0]/1000000\r\n",
    "            stat_data.loc[index, 'Raw_load'] = mismatch_data[value][2]/1000000\r\n",
    "            stat_data.loc[index, 'Raw_imports'] = mismatch_data[value][1]/1000000\r\n",
    "            stat_data.loc[index, 'Raw_exports'] = mismatch_data[value][3]/1000000\r\n",
    "\r\n",
    "            for method in methods:\r\n",
    "                for type in types:\r\n",
    "\r\n",
    "                    df, divider = hourly_data(pd.read_csv(f'../Data Sources/output/{method}/{type}/{value}.csv'))\r\n",
    "                    stat_data.loc[index, f'{type}_{method}'] = df.iloc[:, 1:].sum(axis=1).sum()/(divider*1000000)\r\n",
    "\r\n",
    "                df = pd.read_csv(f'../Data Sources/output/{method}/Transmission/all_transmissions.csv').iloc[:, 1:]\r\n",
    "\r\n",
    "                imports = [x for x in df.columns.values if value in x[-2:]]\r\n",
    "                exports = [x for x in df.columns.values if value in x[:2]]\r\n",
    "\r\n",
    "                stat_data.loc[index, f'Imports_{method}'] = df[imports].sum(axis=1).sum()/1000000\r\n",
    "                stat_data.loc[index, f'Exports_{method}'] = df[exports].sum(axis=1).sum()/1000000\r\n",
    "        else:\r\n",
    "            stat_data = stat_data.drop(labels=index, axis=0)\r\n",
    "    stat_data = stat_data.reset_index(drop=True)\r\n",
    "\r\n",
    "    # We create new variable list xi where i belongs to [1,2,3,4]\r\n",
    "    # Each xi list contains 4 values\r\n",
    "    # After the iteration is completed x1 has the sum values of generation,load,imports and exports of raw unedited data of all the countries with the percentage difference of each of the 4 values compared to the respective generation,load,imports and exports sum values of ENTSO-E statistical data.\r\n",
    "    # Similarly x2 has the percentage dofference of the 4 values of generation,load,imports and exports of Nearest Neighbour Mean method.\r\n",
    "    # In the same way x3 is the Polynmial Linear regression method and x4 is the optimization  based method. \r\n",
    "\r\n",
    "    for i in range(1, 5):\r\n",
    "        globals()[f'x{i}'] = []\r\n",
    "        for j in range(1, 5):\r\n",
    "            globals()[f'x{i}'].append(abs(stat_data.iloc[:, j].sum()-stat_data.iloc[:, i*4+j].sum())*100/stat_data.iloc[:, j].sum())\r\n",
    "\r\n",
    "    width = 0.18\r\n",
    "    labels = ['Actual generation per type', 'Actual load','Physical flow imports', 'Physical flow exports']\r\n",
    "    X = np.arange(4)\r\n",
    "    plt.figure(figsize=(10, 5))\r\n",
    "    plt.bar([x-3*width/2 for x in X], x1, width, color='magenta', edgecolor='black', label='Raw data')\r\n",
    "    plt.bar([x-width/2 for x in X], x2, width, color='aqua', edgecolor='black', label='Nearest Neighbours Mean consolidated data')\r\n",
    "    plt.bar([x+width/2 for x in X], x3, width, color='gold', edgecolor='black', label='Polynomial Linear Regression consolidated data')\r\n",
    "    plt.bar([x+3*width/2 for x in X], x4, width, color='lightcoral', edgecolor='black', label='Sigma consolidated data')\r\n",
    "    plt.xlabel('Data Items')\r\n",
    "    plt.ylabel('Percentage difference')\r\n",
    "    plt.title('Percentage difference with respect to ENTSO-E statistical data - 2018')\r\n",
    "    plt.legend()\r\n",
    "    plt.grid()\r\n",
    "    plt.xticks(X, labels)\r\n",
    "    plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "acb9e0cfff3151089362f20c3a81c7531326aa2c5fdbbc2eccac94359104a8e2"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
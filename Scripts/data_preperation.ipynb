{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preperation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import copy\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "countries={ 'Austria': 'AT', 'Belgium': 'BE',  'Bulgaria': 'BG', 'Switzerland': 'CH', 'Czech Republic': 'CZ', 'Germany': 'DE', 'Denmark': 'DK', 'Estonia': 'EE', 'Spain': 'ES', 'Finland': 'FI', 'France': 'FR',  'Greece': 'GR', 'Hungary': 'HU', 'Ireland': 'IE', 'Italy': 'IT', 'Lithuania': 'LT', 'Latvia': 'LV', 'Montenegro': 'ME','Netherlands': 'NL', 'Norway': 'NO', 'Poland': 'PL', 'Portugal': 'PT', 'Serbia': 'RS', 'Sweden': 'SE', 'Slovenia': 'SI', 'Slovakia': 'SK', 'United Kingdom': 'UK'}\n",
    "\n",
    "abbr_list=list(countries.values())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Making the first,second,third and fourth columns of the dataframe as date,month,year and time"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "data=pd.DataFrame()\n",
    "temp=pd.read_csv('../Data Sources/ENTSO-E/2018/Load/Croatia.csv')\n",
    "data['Day']=temp['Time (CET)'].str[:2]\n",
    "data['Month']=temp['Time (CET)'].str[3:5]\n",
    "data['Year']=temp['Time (CET)'].str[6:10]\n",
    "data['Time']=temp['Time (CET)'].str[11:16]+' - '+temp['Time (CET)'].str[29:35]\n",
    "data = data.drop(range(1994, 1995)).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "# In the country speciic Load data set, Austria,Belgium,Germany,Hungary,Netherlands report data every 15 minutes. \n",
    "# Therefore, these countries have 35044 data points per year. \n",
    "# UK and Ireland report data every 30 minutes henece these countries have 17522 datapoints per year. \n",
    "# All the others report every 1 hour hence have 8761 datapoints per year. \n",
    "# In Genearion dataset, situation is same as abobe except Belgium reports hourly data hence have 8761 datapoints. \n",
    "# In Transmission dataset, all countries report data hourly except Germany which reports every 15 minutes. \n",
    "# Therefroe,it is easy if all the data are converted to hourly data. \n",
    "# To do that in the countries with 35044 datapoints, mean is calculated in every successive 4 datapoints. \n",
    "# In the countries with 17522 datapoints, mean is calculated in every successive 2 datapoints.`\n",
    "\n",
    "# Due to day light saving, all the datasets have null values on 25th March from 02:00 - 03:00.\n",
    "# Considering the time intervals each country update the data, a total number of rows of 4,2 or 1 are dropped from the 31st March, 02:00 - 03:00 time interval.\n",
    "# Also this returns a integer ('divider') based on the file length to get the energy values in a later step. In 15 min interval files this is 4, in 30 min interval files this is 2 and in 1 hour interval files this is 1.\n",
    "\n",
    "def hourly_data(df):\n",
    "    length=len(df.index)\n",
    "    if length==35044:\n",
    "        df = df.drop(range(7976, 7980)).reset_index(drop=True)\n",
    "        divider=4\n",
    "    elif length==17522:\n",
    "        df = df.drop(range(3988, 3990)).reset_index(drop=True)\n",
    "        divider=2\n",
    "    else:\n",
    "        df = df.drop(range(1994, 1995)).reset_index(drop=True)\n",
    "        divider=1\n",
    "\n",
    "    # Following command creates a numpy array with a length similar to the length of the dataframe. \n",
    "    # Values of the array are obtained by getting the floor division of the length value. \n",
    "    # For example, when divider=4, this array will be [0,0,0,0,1,1,1,1,2,2,2,2,....]. \n",
    "    # Then the rows of the dataframe will be grouped according to the order of the numpy array with the mean value of those 4 rows. \n",
    "    # For example, in the numpy array first 4 values are similar. Accordingly first 4 rows of the dataframe will be grouped and get the mean value of those rows \n",
    "    \n",
    "    df=df.groupby(np.arange(len(df))//divider).mean()\n",
    "    return(df)    \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preparing Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "def load(countries):\n",
    "\n",
    "    load_dic = {}\n",
    "    load_data=pd.DataFrame()\n",
    "    load_data[['Day','Month','Year','Time']]=data[['Day','Month','Year','Time']]\n",
    "\n",
    "    # In the following command we open the csv file of each country and save the data in the 'temp' dataframe. \n",
    "    # Then we call the 'hourly_data' function to make all the time steps to hourly data. \n",
    "    # Then the column 'Actual Total Load [MW] - {country_name} ({country_code})' is saved in the new dataframe 'load_data' under the column name '{country_code}'. \n",
    "    # For example, the column 'Actual Total Load [MW] - Germany (DE)' in the 'temp' dataframe will be saved in the 'load_data' dataframe under the column name 'DE'.\n",
    "    \n",
    "    for country,abbr in countries.items():\n",
    "        temp=pd.read_csv(f'../Data Sources/ENTSO-E/2018/Load/{country}.csv')\n",
    "        # temp = temp.replace(['n/e',np.nan] ,0)\n",
    "        temp = temp.replace(['n/e'], 0)\n",
    "        temp = hourly_data(temp)\n",
    "        load_data[f'{abbr}']=temp[f'Actual Total Load [MW] - {country} ({abbr})']\n",
    "        temp['demand'] = temp[f'Actual Total Load [MW] - {country} ({abbr})']\n",
    "        load_dic[abbr] = temp[['demand']]\n",
    "\n",
    "    return(load_data,load_dic)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Preparing Generation Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "def generation(countries):\n",
    "    \n",
    "    generation_dic = {}\n",
    "    generation_data=pd.DataFrame()\n",
    "    generation_data[['Day','Month','Year','Time']]=data[['Day','Month','Year','Time']]\n",
    "\n",
    "    # In the following command we open the csv file of each country and save the data in the 'temp' dataframe. \n",
    "    # Then we make all the 'n/e' values of the 'temp' 0.\n",
    "    # Then we copy the hydro pumped storage consumption data to load_data of the respective country\n",
    "    # then we remove the 'Hydro Pumped Storage  - Actual Consumption [MW]' column\n",
    "    # Then we call the 'hourly_data' function to make all the time steps to hourly data. \n",
    "    # Then we remove the columns in which a single data is not recorded\n",
    "    # Then we get the column names of the 'temp' dataframe into a numpy array called 'fuels' and get the column name without the '- Actual Aggregated [MW]' part.\n",
    "    # Then we change the column names of the dataframe with the edited names in the previous step.\n",
    "    # Then we update the 'generation_data' dataframe and 'generation_dic' dictionary using the 'temp' dataframe.\n",
    "        \n",
    "    for country,abbr in countries.items():\n",
    "        temp=pd.read_csv(f'../Data Sources/ENTSO-E/2018/Generation/{country}.csv',low_memory=False)\n",
    "        # temp = temp.replace(['n/e',np.nan] ,0)\n",
    "        temp = temp.replace(['n/e'], 0)\n",
    "        temp = temp.drop(['Hydro Pumped Storage  - Actual Consumption [MW]'],axis=1)\n",
    "\n",
    "        temp=hourly_data(temp)\n",
    "        \n",
    "        for column in temp.columns.values:\n",
    "            if(temp[column]==0).all():\n",
    "                temp=temp.drop(column,axis=1)\n",
    "\n",
    "        fuels = [x[:-26] for x in temp.columns.values]\n",
    "        temp.columns = fuels\n",
    "\n",
    "        for fuel in fuels:\n",
    "            generation_data[f'{abbr} - {fuel}'] = temp[fuel]\n",
    "\n",
    "        generation_dic[abbr] = temp\n",
    "\n",
    "    return(generation_data,generation_dic)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Preparing Cross-border Transmission Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "def cross_border(abbr_list):\n",
    "\n",
    "    transmission_data = pd.DataFrame()\n",
    "    cross_border_data = pd.DataFrame()\n",
    "    cross_border_data[['Day','Month','Year','Time']]=data[['Day','Month','Year','Time']]\n",
    "\n",
    "# In the following command we get the list of the paths of all files in the directory. \n",
    "# Then one by one we copy each csv to 'temp' dataframe and make all the 'n/e' values of the 'temp' 0. \n",
    "# Then we cretae two new column in the 'transmission_data' dataframe and name it as the two country codes the power transmission occurs. \n",
    "# We use string editing to get the two country codes from the file path. \n",
    "# For example, in the power transmission occur between Germany and Austria, we name the column as 'DE -> AT' and if the power transmission occur between Austria and Germany, we name the column as 'AT -> DE '.\n",
    "# We use 'pd.to_numeric' function to convert the string values to numerical values if any numeric values have been recorded as string in the datasets. \n",
    "# Then we send the numeric converted column to 'hourly_data' function because cross border trasnmissions occur between Germany and a some countries have 35044 data points but We need to convert them to hourly values.\n",
    "# Then we make a list of column heads of imports and exports associated with a given country_abbreviation \n",
    "# After this step we assume imports power transmission as a negative value and exports transmission as a positive value of a given country. \n",
    "# Therefore we multiply the 'imports' columns of the 'transmission_data' dataframe by -1 and add the 'exports' columns of the 'transmission_data' to get the net inbound/outbound in that country in that particular time step and save that in the 'cross_border_data' dataframe.\n",
    "# Then we filter the transmission links between given two countries in which if both countries associated with the power transmission are included in our country_abbreviation list.\n",
    "# For example power import(export) occurs from(to) a country other than the countries in the abbreviation_list (for ex: 'Cyprus','Turkey' etc.) are omitted. \n",
    "# Then we select only those filtered columns in the 'transmission_data' dataframe.\n",
    "\n",
    "    csvs = glob.glob(\"../Data Sources/ENTSO-E/2018/Transmission/*.csv\")\n",
    "\n",
    "    for csv in csvs:\n",
    "        temp = pd.read_csv(csv)\n",
    "        # temp = temp.replace(['n/e', np.nan], 0)\n",
    "        temp = temp.replace(['n/e'], 0)\n",
    "\n",
    "        transmission_data[f'{csv[42:44]} - > {csv[45:47]}'] = hourly_data(pd.to_numeric(temp.iloc[:, 2]))\n",
    "        transmission_data[f'{csv[45:47]} - > {csv[42:44]}'] = hourly_data(pd.to_numeric(temp.iloc[:, 1]))\n",
    "\n",
    "    for abbr in abbr_list:\n",
    "        imports = [x for x in transmission_data.columns.values if abbr in x[-2:]]\n",
    "        exports = [x for x in transmission_data.columns.values if abbr in x[:2]]\n",
    "\n",
    "        cross_border_data[f'{abbr}'] = transmission_data[exports].sum(axis=1) + (transmission_data[imports].sum(axis=1))*-1\n",
    "\n",
    "    transmission_lines = list([x for x in transmission_data.columns.values if x[:2] in abbr_list and x[-2:] in abbr_list])\n",
    "    transmission_data = transmission_data[transmission_lines]\n",
    "\n",
    "    return cross_border_data, transmission_data\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Calculating net imports/exports based on generation and load data "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "def import_export_using_load_gen(load_data, generation_data, abbr_list):\n",
    "\n",
    "    import_export_data = pd.DataFrame()\n",
    "    import_export_data[['Day', 'Month', 'Year', 'Time']] = load_data[['Day', 'Month', 'Year', 'Time']]\n",
    "\n",
    "    # In the following command we calculate net import/export in each time step of each country by subtracting '{country_code}' column of 'load_data' dataframe from '{country_code} - Total' of 'generation_data' dataframe and save the result in '{country_code} - [gen - load]' column of 'import_export_data' dataframe.\n",
    "    # for example, import_export_data['DE - [gen - load]']=generation_data['DE - Total'] - load_data['DE'].\n",
    "    # Then we create a new column in the 'import_export_data' dataframe called '{country_code} - import/export' and make that column 'Net Export' if '{country_code} - [gen - load]' is greater than 0 and make the '{country_code} - import/export' column 'Net Import' if '{country_code} - [gen - load]' is equal or lower than 0\n",
    "\n",
    "    for abbr in abbr_list:\n",
    "        import_export_data[f'{abbr}'] = generation_data.filter(like=abbr).sum(axis=1) - load_data[f'{abbr}']\n",
    "\n",
    "    return(import_export_data)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Calculating net imports/exports based on cross-border transmission data "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "def import_export_using_crossborder(crossborder_data, abbr_list):\r\n",
    "\r\n",
    "    import_export_data = pd.DataFrame()\r\n",
    "    import_export_data[['Day', 'Month', 'Year', 'Time']] = crossborder_data[['Day', 'Month', 'Year', 'Time']]\r\n",
    "\r\n",
    "# In the following command we copy the '{country_code}' column of 'crossborder_data' dataframe to '{country_code} - [exp - imp]' column of 'import_export_data' dataframe.\r\n",
    "# for example, import_export_data['DE - [exp - imp]'].\r\n",
    "# Then we create a new column in the 'import_export_data' dataframe called '{country_code} - import/export' and make that column 'Net Export' if '{country_code} - [exp - imp]' is greater than 0 and make the '{country_code} - import/export' column 'Net Import' if '{country_code} - [exp - imp]' is equal or lower than 0\r\n",
    "\r\n",
    "    for abbr in abbr_list:\r\n",
    "        import_export_data[f'{abbr}'] = crossborder_data[f'{abbr}']\r\n",
    "\r\n",
    "    return(import_export_data)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Internal sigma calculation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 Alpha calculation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "# def calculate_alpha(load_data,generation_data,transmission_data,abbr_list):\r\n",
    "\r\n",
    "#     eph = 0.1\r\n",
    "#     A=100\r\n",
    "#     load_gen_data = {}\r\n",
    "#     alpha = {}\r\n",
    "\r\n",
    "#     # First we create a new dictionary called 'load_gen_data' and in that dictionary keys are country_abbreviations and as value of each key we add the combined demand column of each country and generation columns from all the sources in that country.\r\n",
    "\r\n",
    "#     for abbr, df in generation_data.items():\r\n",
    "#         load_gen_data[abbr] = pd.concat([df, load_data[abbr]],axis=1)\r\n",
    "\r\n",
    "#     # In the following section we take each generation,load and transmission csvs and send them to the 'neareset_neighbours_mean' function.\r\n",
    "#     # This function send each dataframe to 'omit_dst' function and removes the empty observations on March 28th and get the 'divider' value which is associated with the length of the dataframe.\r\n",
    "#     # Then we replace 'n/e' values with 0\r\n",
    "#     # Then we check the dataframe column by column.\r\n",
    "#     # First we filter the columns in which the whole column is not 0 in the dataframe.\r\n",
    "#     # Then we add the index and value of each value in the column to a list called 'column_data'\r\n",
    "#     # Then we check value by value in the column untill a null value is found.\r\n",
    "#     # If the value is a null value, we get all the values in that column which are the values associated with the same time step of the null value and the absolute difference of the indexes of that value and the null value to the 'temp_list'.\r\n",
    "#     # For example if the null value occurus at 10:00 - 10:15 time step of some day, all the values associated with 10:00 - 10:15 time step throughout the year and and the absolute difference of the indexes of those value and the null value go to the 'temp_list'\r\n",
    "#     # Then the 'temp_list' is sorted and we get all the indexes in the 'temp_list' except the index of the null value to a 2D numpy array 'X'.\r\n",
    "#     # We get all the values in the 'temp_list' except the value of the null value to a 1D numpy array 'y'\r\n",
    "#     # Then we divide 'X' and 'y' values in the ratio of 30% test and 70% train data.\r\n",
    "#     # We create an array of degree values from 1 to 10.\r\n",
    "#     # Then we iterate the 'degrees' one by one and create polynomial values of 'x_train' data called 'x_poly_train' based on the value of the degree\r\n",
    "#     # Then we fit the polynomial linear regression function using 'x_poly_train' data and 'y_train' data.\r\n",
    "#     # Then based on the polynomial function, using the 'x_poly_test' data we predict the values of the 'y_test' data\r\n",
    "#     # Then based on the predicted values and 'y_test\" data we calculate the Root Mean Square Error.\r\n",
    "#     # Applying the last 4 steps for each degree value, we select the degree value which gives the Lowest Root Mean Square Error.\r\n",
    "#     # Then we fit the polynomial linear regression function again using that degree which gives the Lowest Root Mean Square Error.\r\n",
    "#     # Based on the polynimial function we get the predicted value of the null value.\r\n",
    "\r\n",
    "#     for abbr, df in load_gen_data.items():\r\n",
    "#         for column in df.columns.values:\r\n",
    "#             column_data = [[index, value] for index, value in enumerate(df.loc[:, column])]\r\n",
    "#             for index, value in column_data:\r\n",
    "#                 if pd.isnull(value):\r\n",
    "#                     temp_list = [[abs(index-item[0]), item[1]] for item in column_data if (index-item[0]) % 24 == 0]\r\n",
    "#                     temp_list.sort()\r\n",
    "#                     X = np.array([i[0] for i in temp_list[1:]]).reshape(len(temp_list)-1, 1)\r\n",
    "#                     y = [i[1] for i in temp_list[1:]]\r\n",
    "#                     y = [0 if pd.isna(x) else x for x in y]\r\n",
    "\r\n",
    "#                     x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\r\n",
    "\r\n",
    "#                     degrees = np.arange(1, 11)\r\n",
    "#                     min_rmse, min_deg = 1e10, 0\r\n",
    "\r\n",
    "#                     for degree in degrees:\r\n",
    "\r\n",
    "#                         # Preparing polynomial Train features based on x_train\r\n",
    "#                         poly_features = PolynomialFeatures(degree=degree, include_bias=False)\r\n",
    "#                         x_poly_train = poly_features.fit_transform(x_train)\r\n",
    "\r\n",
    "#                         # Polynomial linear regression based on train data\r\n",
    "#                         poly_reg = LinearRegression()\r\n",
    "#                         poly_reg.fit(x_poly_train, y_train)\r\n",
    "\r\n",
    "#                         # Predicting y values and getting root mean squared error based on predicted y values and y_test values\r\n",
    "#                         x_poly_test = poly_features.fit_transform(x_test)\r\n",
    "#                         poly_predict = poly_reg.predict(x_poly_test)\r\n",
    "#                         poly_mse = mean_squared_error(y_test, poly_predict)\r\n",
    "#                         poly_rmse = np.sqrt(poly_mse)\r\n",
    "\r\n",
    "#                         # Selecting the best degree of the polynimial function based on lowest root mean squared error\r\n",
    "#                         if min_rmse > poly_rmse:\r\n",
    "#                             min_rmse = poly_rmse\r\n",
    "#                             min_deg = degree\r\n",
    "\r\n",
    "#                     # Fitting the regression function again based on the selected best degree above\r\n",
    "#                     poly_features = PolynomialFeatures(degree=min_deg, include_bias=False)\r\n",
    "#                     x_poly_train = poly_features.fit_transform(x_train)\r\n",
    "#                     poly_reg = LinearRegression()\r\n",
    "#                     poly_reg.fit(x_poly_train, y_train)\r\n",
    "\r\n",
    "#                     df.loc[index, column] = poly_reg.predict(poly_features.fit_transform([[0]]))[0]\r\n",
    "#             df[column] = df[column].apply(lambda x: eph if x < eph else x)\r\n",
    "#             df[column] = df[column].apply(lambda x:  A/x)\r\n",
    "#         alpha[abbr] = df\r\n",
    "\r\n",
    "#     for column in transmission_data.columns.values:\r\n",
    "#         column_data = [[index, value] for index, value in enumerate(transmission_data.loc[:, column])]\r\n",
    "#         for index,value in column_data:\r\n",
    "#             if pd.isnull(value):\r\n",
    "#                 temp_list = [[abs(index-item[0]), item[1]] for item in column_data if (index-item[0]) % 24 == 0]\r\n",
    "#                 temp_list.sort()\r\n",
    "#                 X = np.array([i[0] for i in temp_list[1:]]).reshape(len(temp_list)-1, 1)\r\n",
    "#                 y = [i[1] for i in temp_list[1:]]\r\n",
    "#                 y = [0 if pd.isna(x) else x for x in y]\r\n",
    "\r\n",
    "#                 x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\r\n",
    "\r\n",
    "#                 degrees = np.arange(1, 11)\r\n",
    "#                 min_rmse, min_deg = 1e10, 0\r\n",
    "\r\n",
    "#                 for degree in degrees:\r\n",
    "\r\n",
    "#                     # Preparing polynomial Train features based on x_train\r\n",
    "#                     poly_features = PolynomialFeatures(degree=degree, include_bias=False)\r\n",
    "#                     x_poly_train = poly_features.fit_transform(x_train)\r\n",
    "\r\n",
    "#                     # Polynomial linear regression based on train data\r\n",
    "#                     poly_reg = LinearRegression()\r\n",
    "#                     poly_reg.fit(x_poly_train, y_train)\r\n",
    "\r\n",
    "#                     # Predicting y values and getting root mean squared error based on predicted y values and y_test values\r\n",
    "#                     x_poly_test = poly_features.fit_transform(x_test)\r\n",
    "#                     poly_predict = poly_reg.predict(x_poly_test)\r\n",
    "#                     poly_mse = mean_squared_error(y_test, poly_predict)\r\n",
    "#                     poly_rmse = np.sqrt(poly_mse)\r\n",
    "\r\n",
    "#                     # Selecting the best degree of the polynimial function based on lowest root mean squared error\r\n",
    "#                 if min_rmse > poly_rmse:\r\n",
    "#                     min_rmse = poly_rmse\r\n",
    "#                     min_deg = degree\r\n",
    "\r\n",
    "#                 # Fitting the regression function again based on the selected best degree above\r\n",
    "#                 poly_features = PolynomialFeatures(degree=min_deg, include_bias=False)\r\n",
    "#                 x_poly_train = poly_features.fit_transform(x_train)\r\n",
    "#                 poly_reg = LinearRegression()\r\n",
    "#                 poly_reg.fit(x_poly_train, y_train)\r\n",
    "\r\n",
    "#                 transmission_data.loc[index,column]= poly_reg.predict(poly_features.fit_transform([[0]]))[0]\r\n",
    "#         transmission_data[column] = transmission_data[column].apply(lambda x: eph if x < eph else x)\r\n",
    "#         transmission_data[column] = transmission_data[column].apply(lambda x:  A/x)\r\n",
    "#     alpha[\"transmission_data\"] = transmission_data\r\n",
    "\r\n",
    "#     return(alpha)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 Sigma calculation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "def calculate_sigma(load_data,generation_data,transmission_data,abbr_list):\r\n",
    "\r\n",
    "    eph = 0.1\r\n",
    "    A=100\r\n",
    "    load_gen_data = {}\r\n",
    "    sigma = {}\r\n",
    "\r\n",
    "    # First we create a new dictionary called 'load_gen_data' and in that dictionary keys are country_abbreviations and as value of each key we add the combined demand column of each country and generation columns from all the sources in that country.\r\n",
    "\r\n",
    "    for abbr, df in generation_data.items():\r\n",
    "        load_gen_data[abbr] = pd.concat([df, load_data[abbr]],axis=1)\r\n",
    "\r\n",
    "    # Then we check the dataframe column by column.\r\n",
    "    # Then we add the index and value of each value in the column to a list called 'column_data'\r\n",
    "    # Then we check value by value in the column untill a null value is found.\r\n",
    "    # If the value is a null value, we get all the values in that column which are the values associated with the same time step of the null value and the absolute difference of the indexes of that value and the null value to the 'temp_list'.\r\n",
    "    # For example if the null value occurus at 10:00 - 10:15 time step of some day, all the values associated with 10:00 - 10:15 time step throughout the year and and the absolute difference of the indexes of those value and the null value go to the 'temp_list'\r\n",
    "    # Then the 'temp_list' is sorted and the first 60 values are taken to the 'moving_average' list\r\n",
    "    # Here in the 'moving_average' list we get the values of the nearest 60 days of the same time step in which the null value has occured.\r\n",
    "    # Then we replace any null values in the 'moving_average' list with 0\r\n",
    "    # Then we get the mean value of the 'moving_average' list and assign that value to the original null value in the dataframe\r\n",
    "\r\n",
    "    for abbr, df in load_gen_data.items():\r\n",
    "        for column in df.columns.values:\r\n",
    "            column_data = [[index, value] for index, value in enumerate(df.loc[:, column])]\r\n",
    "            for index, value in column_data:\r\n",
    "                if pd.isnull(value):\r\n",
    "                    temp_list = [[abs(index-item[0]), item[1]] for item in column_data if (index-item[0]) % 24 == 0]\r\n",
    "                    temp_list.sort()\r\n",
    "                    moving_average = [x[1] for x in temp_list[1:11]]\r\n",
    "                    moving_average = [0 if pd.isna(x) else x for x in moving_average]\r\n",
    "                    df.loc[index, column] = np.mean(np.array(moving_average))\r\n",
    "            df[column] = df[column].apply(lambda x: eph if x < eph else x)\r\n",
    "            df[column] = df[column].apply(lambda x:  A/x)\r\n",
    "        sigma[abbr] = df\r\n",
    "\r\n",
    "    for column in transmission_data.columns.values:\r\n",
    "        column_data = [[index, value] for index, value in enumerate(transmission_data.loc[:, column])]\r\n",
    "        for index,value in column_data:\r\n",
    "            if pd.isnull(value):\r\n",
    "                temp_list = [[abs(index-item[0]), item[1]] for item in column_data if (index-item[0]) % 24 == 0]\r\n",
    "                temp_list.sort()\r\n",
    "                moving_average = [x[1] for x in temp_list[1:11]]\r\n",
    "                moving_average = [0 if pd.isna(x) else x for x in moving_average]\r\n",
    "                transmission_data.loc[index,column]= np.mean(np.array(moving_average))\r\n",
    "        transmission_data[column] = transmission_data[column].apply(lambda x: eph if x < eph else x)\r\n",
    "        transmission_data[column] = transmission_data[column].apply(lambda x:  A/x)\r\n",
    "    sigma[\"transmission_data\"] = transmission_data\r\n",
    "\r\n",
    "    return(sigma)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 Internal data consolidation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "def data_consolidation(method,generation_dic_copy, load_dic_copy, transmission_data_copy, intermediary_var, unit_var):\n",
    "    consolidated_gen_data = {}\n",
    "    consolidated_load_data = {}\n",
    "    consolidated_transmission_data = {}\n",
    "\n",
    "    # In the following command, we fill each generation by source value in each timestep in each country with ('intermediary_var[\"generation\"]' + 'original_value' * 'unit_var['generation]') value of that value\n",
    "    # Similarly we fill each load value in each timestep in each country with ('intermediary_var['load'] + 'original_value' * 'unit_var['load']') value of that value\n",
    "    # Then we save the consolidated generation and load values in seperate csv files.\n",
    "    # We follow similar steps to obtain consolidated transmission values and save the result in a seperate csv file. \n",
    "\n",
    "    for abbr, df in generation_dic_copy.items():\n",
    "        for column in df.columns:\n",
    "            df[column] = intermediary_var[\"generation\"][abbr][column] + \\\n",
    "                df[column] * unit_var[\"generation\"][abbr][column]\n",
    "        consolidated_gen_data[abbr] = df\n",
    "        consolidated_load_data[abbr] = intermediary_var[\"load\"][abbr]['demand'] + \\\n",
    "            load_dic_copy[abbr]['demand'] * unit_var[\"load\"][abbr]['demand']\n",
    "        consolidated_gen_data[abbr].to_csv(f\"../Data Sources/output/{method}/Generation/{abbr}.csv\")\n",
    "        consolidated_load_data[abbr].to_csv(f\"../Data Sources/output/{method}/Load/{abbr}.csv\")\n",
    "\n",
    "    for column in transmission_data_copy.columns:\n",
    "        transmission_data_copy[column] = intermediary_var[\"transmission\"][column] + \\\n",
    "            transmission_data_copy[column] * unit_var[\"transmission\"][column]\n",
    "    consolidated_transmission_data = transmission_data_copy\n",
    "    consolidated_transmission_data.to_csv(f'../Data Sources/output/{method}/Transmission/all_transmissions.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "# sigma(load(countries)[1], generation(countries)[1], cross_border(abbr_list)[1],abbr_list)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
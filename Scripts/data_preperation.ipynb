{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries={ 'Austria': 'AT', 'Belgium': 'BE',  'Bulgaria': 'BG', 'Switzerland': 'CH', 'Czech Republic': 'CZ', 'Germany': 'DE', 'Denmark': 'DK', 'Estonia': 'EE', 'Spain': 'ES', 'Finland': 'FI', 'France': 'FR',  'Greece': 'GR', 'Hungary': 'HU', 'Ireland': 'IE', 'Italy': 'IT', 'Lithuania': 'LT', 'Latvia': 'LV', 'Montenegro': 'ME','Netherlands': 'NL', 'Norway': 'NO', 'Poland': 'PL', 'Portugal': 'PT', 'Serbia': 'RS', 'Sweden': 'SE', 'Slovenia': 'SI', 'Slovakia': 'SK', 'United Kingdom': 'UK'}\n",
    "\n",
    "abbr_list=list(countries.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the first,second,third and fourth columns of the dataframe as date,month,year and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame()\n",
    "temp=pd.read_csv('../Data Sources/ENTSO-E/2018/Load/Croatia.csv')\n",
    "data['Day']=temp['Time (CET)'].str[:2]\n",
    "data['Month']=temp['Time (CET)'].str[3:5]\n",
    "data['Year']=temp['Time (CET)'].str[6:10]\n",
    "data['Time']=temp['Time (CET)'].str[11:16]+' - '+temp['Time (CET)'].str[29:35]\n",
    "data = data.drop(range(1994, 1995)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the country speciic Load data set, Austria,Belgium,Germany,Hungary,Netherlands report data every 15 minutes. \n",
    "# Therefore, these countries have 35044 data points per year. \n",
    "# UK and Ireland report data every 30 minutes henece these countries have 17522 datapoints per year. \n",
    "# All the others report every 1 hour hence have 8761 datapoints per year. \n",
    "# In Genearion dataset, situation is same as abobe except Belgium reports hourly data hence have 8761 datapoints. \n",
    "# In Transmission dataset, all countries report data hourly except Germany which reports every 15 minutes. \n",
    "# Therefroe,it is easy if all the data are converted to hourly data. \n",
    "# To do that in the countries with 35044 datapoints, mean is calculated in every successive 4 datapoints. \n",
    "# In the countries with 17522 datapoints, mean is calculated in every successive 2 datapoints.`\n",
    "\n",
    "# Due to day light saving, all the datasets have null values on 25th March from 02:00 - 03:00.\n",
    "# Considering the time intervals each country update the data, a total number of rows of 4,2 or 1 are dropped from the 31st March, 02:00 - 03:00 time interval.\n",
    "# Also this returns a integer ('divider') based on the file length to get the energy values in a later step. In 15 min interval files this is 4, in 30 min interval files this is 2 and in 1 hour interval files this is 1.\n",
    "\n",
    "def hourly_data(df):\n",
    "    length=len(df.index)\n",
    "    if length==35044:\n",
    "        df = df.drop(range(7976, 7980)).reset_index(drop=True)\n",
    "        divider=4\n",
    "    elif length==17522:\n",
    "        df = df.drop(range(3988, 3990)).reset_index(drop=True)\n",
    "        divider=2\n",
    "    else:\n",
    "        df = df.drop(range(1994, 1995)).reset_index(drop=True)\n",
    "        divider=1\n",
    "\n",
    "    # Following command creates a numpy array with a length similar to the length of the dataframe. \n",
    "    # Values of the array are obtained by getting the floor division of the length value. \n",
    "    # For example, when divider=4, this array will be [0,0,0,0,1,1,1,1,2,2,2,2,....]. \n",
    "    # Then the rows of the dataframe will be grouped according to the order of the numpy array with the mean value of those 4 rows. \n",
    "    # For example, in the numpy array first 4 values are similar. Accordingly first 4 rows of the dataframe will be grouped and get the mean value of those rows \n",
    "    \n",
    "    df=df.groupby(np.arange(len(df))//divider).mean()\n",
    "    return(df)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(countries):\n",
    "\n",
    "    load_dic = {}\n",
    "    load_data=pd.DataFrame()\n",
    "    load_data[['Day','Month','Year','Time']]=data[['Day','Month','Year','Time']]\n",
    "\n",
    "    # In the following command we open the csv file of each country and save the data in the 'temp' dataframe. \n",
    "    # Then we call the 'hourly_data' function to make all the time steps to hourly data. \n",
    "    # Then the column 'Actual Total Load [MW] - {country_name} ({country_code})' is saved in the new dataframe 'load_data' under the column name '{country_code}'. \n",
    "    # For example, the column 'Actual Total Load [MW] - Germany (DE)' in the 'temp' dataframe will be saved in the 'load_data' dataframe under the column name 'DE'.\n",
    "    \n",
    "    for country,abbr in countries.items():\n",
    "        temp=pd.read_csv(f'../Data Sources/ENTSO-E/2018/Load/{country}.csv')\n",
    "        temp = temp.replace(['n/e',np.nan] ,0)\n",
    "        temp = hourly_data(temp)\n",
    "        load_data[f'{abbr}']=temp[f'Actual Total Load [MW] - {country} ({abbr})']\n",
    "        temp['demand'] = temp[f'Actual Total Load [MW] - {country} ({abbr})']\n",
    "        load_dic[abbr] = temp[['demand']]\n",
    "\n",
    "    return(load_data,load_dic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparing Generation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(countries):\n",
    "    \n",
    "    generation_dic = {}\n",
    "    generation_data=pd.DataFrame()\n",
    "    generation_data[['Day','Month','Year','Time']]=data[['Day','Month','Year','Time']]\n",
    "\n",
    "    # In the following command we open the csv file of each country and save the data in the 'temp' dataframe. \n",
    "    # Then we make all the 'n/e' values of the 'temp' 0.\n",
    "    # Then we copy the hydro pumped storage consumption data to load_data of the respective country\n",
    "    # then we remove the 'Hydro Pumped Storage  - Actual Consumption [MW]' column\n",
    "    # Then we call the 'hourly_data' function to make all the time steps to hourly data. \n",
    "    # Then we remove the columns in which a single data is not recorded\n",
    "    # Then we get the column names of the 'temp' dataframe into a numpy array called 'fuels' and get the column name without the '- Actual Aggregated [MW]' part.\n",
    "    # Then we change the column names of the dataframe with the edited names in the previous step.\n",
    "    # Then we update the 'generation_data' dataframe and 'generation_dic' dictionary using the 'temp' dataframe.\n",
    "        \n",
    "    for country,abbr in countries.items():\n",
    "        temp=pd.read_csv(f'../Data Sources/ENTSO-E/2018/Generation/{country}.csv',low_memory=False)\n",
    "        temp = temp.replace(['n/e',np.nan] ,0)\n",
    "        temp = temp.drop(['Hydro Pumped Storage  - Actual Consumption [MW]'],axis=1)\n",
    "\n",
    "        temp=hourly_data(temp)\n",
    "        \n",
    "        for column in temp.columns.values:\n",
    "            if(temp[column]==0).all():\n",
    "                temp=temp.drop(column,axis=1)\n",
    "\n",
    "        fuels = [x[:-26] for x in temp.columns.values]\n",
    "        temp.columns = fuels\n",
    "\n",
    "        for fuel in fuels:\n",
    "            generation_data[f'{abbr} - {fuel}'] = temp[fuel]\n",
    "\n",
    "        generation_dic[abbr] = temp\n",
    "\n",
    "    return(generation_data,generation_dic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing Cross-border Transmission Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_border(abbr_list):\n",
    "\n",
    "    transmission_data = pd.DataFrame()\n",
    "    cross_border_data = pd.DataFrame()\n",
    "    cross_border_data[['Day','Month','Year','Time']]=data[['Day','Month','Year','Time']]\n",
    "\n",
    "# In the following command we get the list of the paths of all files in the directory. \n",
    "# Then one by one we copy each csv to 'temp' dataframe and make all the 'n/e' values of the 'temp' 0. \n",
    "# Then we cretae two new column in the 'transmission_data' dataframe and name it as the two country codes the power transmission occurs. \n",
    "# We use string editing to get the two country codes from the file path. \n",
    "# For example, in the power transmission occur between Germany and Austria, we name the column as 'DE -> AT' and if the power transmission occur between Austria and Germany, we name the column as 'AT -> DE '.\n",
    "# We use 'pd.to_numeric' function to convert the string values to numerical values if any numeric values have been recorded as string in the datasets. \n",
    "# Then we send the numeric converted column to 'hourly_data' function because cross border trasnmissions occur between Germany and a some countries have 35044 data points but We need to convert them to hourly values.\n",
    "# Then we make a list of column heads of imports and exports associated with a given country_abbreviation \n",
    "# After this step we assume imports power transmission as a negative value and exports transmission as a positive value of a given country. \n",
    "# Therefore we multiply the 'imports' columns of the 'transmission_data' dataframe by -1 and add the 'exports' columns of the 'transmission_data' to get the net inbound/outbound in that country in that particular time step and save that in the 'cross_border_data' dataframe.\n",
    "# Then we filter the transmission links between given two countries in which if both countries associated with the power transmission are included in our country_abbreviation list.\n",
    "# For example power import(export) occurs from(to) a country other than the countries in the abbreviation_list (for ex: 'Cyprus','Turkey' etc.) are omitted. \n",
    "# Then we select only those filtered columns in the 'transmission_data' dataframe.\n",
    "\n",
    "    csvs = glob.glob(\"../Data Sources/ENTSO-E/2018/Transmission/*.csv\")\n",
    "\n",
    "    for csv in csvs:\n",
    "        temp = pd.read_csv(csv)\n",
    "        temp = temp.replace(['n/e', np.nan], 0)\n",
    "\n",
    "        transmission_data[f'{csv[42:44]} - > {csv[45:47]}'] = hourly_data(pd.to_numeric(temp.iloc[:, 2]))\n",
    "        transmission_data[f'{csv[45:47]} - > {csv[42:44]}'] = hourly_data(pd.to_numeric(temp.iloc[:, 1]))\n",
    "\n",
    "    for abbr in abbr_list:\n",
    "        imports = [x for x in transmission_data.columns.values if abbr in x[-2:]]\n",
    "        exports = [x for x in transmission_data.columns.values if abbr in x[:2]]\n",
    "\n",
    "        cross_border_data[f'{abbr}'] = transmission_data[exports].sum(axis=1) + (transmission_data[imports].sum(axis=1))*-1\n",
    "\n",
    "    transmission_lines = list([x for x in transmission_data.columns.values if x[:2] in abbr_list and x[-2:] in abbr_list])\n",
    "    transmission_data = transmission_data[transmission_lines]\n",
    "\n",
    "    return cross_border_data, transmission_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Calculating net imports/exports based on generation and load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_export_using_load_gen(load_data, generation_data, abbr_list):\n",
    "\n",
    "    import_export_data = pd.DataFrame()\n",
    "    import_export_data[['Day', 'Month', 'Year', 'Time']] = load_data[['Day', 'Month', 'Year', 'Time']]\n",
    "\n",
    "    # In the following command we calculate net import/export in each time step of each country by subtracting '{country_code}' column of 'load_data' dataframe from '{country_code} - Total' of 'generation_data' dataframe and save the result in '{country_code} - [gen - load]' column of 'import_export_data' dataframe.\n",
    "    # for example, import_export_data['DE - [gen - load]']=generation_data['DE - Total'] - load_data['DE'].\n",
    "    # Then we create a new column in the 'import_export_data' dataframe called '{country_code} - import/export' and make that column 'Net Export' if '{country_code} - [gen - load]' is greater than 0 and make the '{country_code} - import/export' column 'Net Import' if '{country_code} - [gen - load]' is equal or lower than 0\n",
    "\n",
    "    for abbr in abbr_list:\n",
    "        import_export_data[f'{abbr}'] = generation_data.filter(like=abbr).sum(axis=1) - load_data[f'{abbr}']\n",
    "\n",
    "    return(import_export_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Calculating net imports/exports based on cross-border transmission data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_export_using_crossborder(crossborder_data, abbr_list):\n",
    "\n",
    "    import_export_data = pd.DataFrame()\n",
    "    import_export_data[['Day', 'Month', 'Year', 'Time']] = crossborder_data[['Day', 'Month', 'Year', 'Time']]\n",
    "\n",
    "# In the following command we copy the '{country_code}' column of 'crossborder_data' dataframe to '{country_code} - [exp - imp]' column of 'import_export_data' dataframe.\n",
    "# for example, import_export_data['DE - [exp - imp]'].\n",
    "# Then we create a new column in the 'import_export_data' dataframe called '{country_code} - import/export' and make that column 'Net Export' if '{country_code} - [exp - imp]' is greater than 0 and make the '{country_code} - import/export' column 'Net Import' if '{country_code} - [exp - imp]' is equal or lower than 0\n",
    "\n",
    "    for abbr in abbr_list:\n",
    "        import_export_data[f'{abbr}'] = crossborder_data[f'{abbr}']\n",
    "\n",
    "    return(import_export_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Internal sigma calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Sigma calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(load_data,generation_data,transmission_data,abbr_list):\n",
    "\n",
    "    eph = 0.1\n",
    "    A=100\n",
    "    load_gen_data = {}\n",
    "    sigma = {}\n",
    "\n",
    "    # First we create a new dictionary called 'load_gen_data' and in that dictionary keys are country_abbreviations and as value of each key we add the combined demand column of each country and generation columns from all the sources in that country.\n",
    "    # Then in each value of the dictionary (which is a dataframe), we add new columns with the name '{Original_column_name} moving_average' and make it 0.\n",
    "    # Then we get the moving average of each timestep using the values in that same time period in the last 10 days.\n",
    "    # This creates null values in the first 10 days of the year. We fill them with the values from the original column.\n",
    "    # Then in these 'moving_average' columns we replace the value with 0.1, if the current value is less than 0.1. \n",
    "    # Then in these 'moving_average' columns we change the value as 100/current_value.\n",
    "    # Then we filter only the column names which have 'moving_avareg' as column name and remove the 'moving_average' part from the column name.\n",
    "    # We save the resultant dataframe as a value in a dictionary called 'sigma' with the key as country_abbreviation.\n",
    "    # We do the same procedure for transmission_data and save the resultant dataframe as the value of 'transmission_data' key of 'sigma' dictionary.\n",
    "\n",
    "    for abbr, df in generation_data.items():\n",
    "        load_gen_data[abbr] = pd.concat([df, load_data[abbr]],axis=1)\n",
    "\n",
    "    for abbr, df in load_gen_data.items():\n",
    "        for column in df.columns.values:\n",
    "            df[column + \"_moving_average\"] = 0\n",
    "            for i in range(10):\n",
    "                df[column + \"_moving_average\"] += df[column].shift(24 * i) / 10\n",
    "            df[column + \"_moving_average\"].fillna(df[column], inplace=True)\n",
    "            df[column + \"_moving_average\"] = df[column + \"_moving_average\"].apply(lambda x: eph if x < eph else x)\n",
    "            df[column + \"_moving_average\"] = df[column + \"_moving_average\"].apply(lambda x:  A/x)\n",
    "        df = df[[x for x in df.columns.values if \"_moving_average\" in x]]\n",
    "        df.columns = [[x[:-15] for x in df.columns.values]]\n",
    "        sigma[abbr] = df\n",
    "\n",
    "    for transmission in transmission_data.columns.values:\n",
    "        transmission_data[transmission + \"_moving_average\"] = 0\n",
    "        for i in range(10):\n",
    "            transmission_data[transmission + \"_moving_average\"] += transmission_data[transmission].shift(24 * i) /10\n",
    "        transmission_data[transmission + \"_moving_average\"].fillna(transmission_data[transmission], inplace=True)\n",
    "        transmission_data[transmission + \"_moving_average\"] = transmission_data[transmission + \"_moving_average\"].apply(lambda x: eph if x < eph else x)\n",
    "        transmission_data[transmission + \"_moving_average\"] = transmission_data[transmission + \"_moving_average\"].apply(lambda x:  A/x)\n",
    "    transmission_data = transmission_data[[x for x in transmission_data.columns.values if \"_moving_average\" in x]]\n",
    "    transmission_data.columns = [[x[:-15] for x in transmission_data.columns.values]]\n",
    "    sigma[\"transmission_data\"] = transmission_data\n",
    "\n",
    "    return(sigma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Internal data consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_consolidation(generation_dic_copy, load_dic, transmission_data_copy, intermediary_var, unit_var):\n",
    "    consolidated_gen_data = {}\n",
    "    consolidated_load_data = {}\n",
    "    consolidated_transmission_data = {}\n",
    "\n",
    "    # In the following command, we fill each generation by source value in each timestep in each country with ('intermediary_var[\"generation\"]' + 'original_value' * 'unit_var['generation]') value of that value\n",
    "    # Similarly we fill each load value in each timestep in each country with ('intermediary_var['load'] + 'original_value' * 'unit_var['load']') value of that value\n",
    "    # Then we save the consolidated generation and load values in seperate csv files.\n",
    "    # We follow similar steps to obtain consolidated transmission values and save the result in a seperate csv file. \n",
    "\n",
    "    for abbr, df in generation_dic_copy.items():\n",
    "        for column in df.columns:\n",
    "            df[column] = intermediary_var[\"generation\"][abbr][column] + \\\n",
    "                df[column] * unit_var[\"generation\"][abbr][column]\n",
    "        consolidated_gen_data[abbr] = df\n",
    "        consolidated_load_data[abbr] = intermediary_var[\"load\"][abbr]['demand'] + \\\n",
    "            load_dic[abbr]['demand'] * unit_var[\"load\"][abbr]['demand']\n",
    "        consolidated_gen_data[abbr].to_csv(f\"../Data Sources/output/Sigma/Generation/{abbr}.csv\")\n",
    "        consolidated_load_data[abbr].to_csv(f\"../Data Sources/output/Sigma/Load/{abbr}.csv\")\n",
    "\n",
    "    for column in transmission_data_copy.columns:\n",
    "        transmission_data_copy[column] = intermediary_var[\"transmission\"][column] + \\\n",
    "            transmission_data_copy[column] * unit_var[\"transmission\"][column]\n",
    "    consolidated_transmission_data = transmission_data_copy\n",
    "    consolidated_transmission_data.to_csv('../Data Sources/output/Sigma/Transmission/all_transmissions.csv')\n",
    "\n",
    "    return(consolidated_gen_data, consolidated_load_data, consolidated_transmission_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigma(load(countries)[1], generation(countries)[1], cross_border(abbr_list)[1],abbr_list)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

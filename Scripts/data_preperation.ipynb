{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preperation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import glob\r\n",
    "import copy\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "from sklearn.preprocessing import PolynomialFeatures\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "from sklearn.model_selection import train_test_split\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "countries={ 'Austria': 'AT', 'Belgium': 'BE',  'Bulgaria': 'BG', 'Switzerland': 'CH', 'Czech Republic': 'CZ', 'Germany': 'DE', 'Denmark': 'DK', 'Estonia': 'EE', 'Spain': 'ES', 'Finland': 'FI', 'France': 'FR',  'Greece': 'GR', 'Hungary': 'HU', 'Ireland': 'IE', 'Italy': 'IT', 'Lithuania': 'LT', 'Latvia': 'LV', 'Montenegro': 'ME','Netherlands': 'NL', 'Norway': 'NO', 'Poland': 'PL', 'Portugal': 'PT', 'Serbia': 'RS', 'Sweden': 'SE', 'Slovenia': 'SI', 'Slovakia': 'SK', 'United Kingdom': 'UK'}\r\n",
    "\r\n",
    "abbr_list=list(countries.values())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Making the first,second,third and fourth columns of the dataframe as date,month,year and time"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# data=pd.DataFrame()\r\n",
    "# temp=pd.read_csv('../Data Sources/ENTSO-E/2018/Load/Croatia.csv')\r\n",
    "# data['Day']=temp['Time (CET)'].str[:2]\r\n",
    "# data['Month']=temp['Time (CET)'].str[3:5]\r\n",
    "# data['Year']=temp['Time (CET)'].str[6:10]\r\n",
    "# data['Time']=temp['Time (CET)'].str[11:16]+' - '+temp['Time (CET)'].str[29:35]\r\n",
    "# data = data.drop(range(1994, 1995)).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def polynomial(selected_values,selected_index):\r\n",
    "\r\n",
    "    # We get all the indexes in the 'selected_values' to a 2D numpy array 'X'.\r\n",
    "    # We get all the values in the 'selected_values'to a 1D numpy array 'y'\r\n",
    "    # Then we fill the null values in array 'y' with the mean value of the array.\r\n",
    "    # Then we divide 'X' and 'y' values in the ratio of 30% test and 70% train data.\r\n",
    "    # We create an array of degree values from 1 to 10.\r\n",
    "    # Then we iterate the 'degrees' one by one and create polynomial values of 'x_train' data called 'x_poly_train' based on the value of the degree\r\n",
    "    # Then we fit the polynomial linear regression function using 'x_poly_train' data and 'y_train' data.\r\n",
    "    # Then based on the polynomial function, using the 'x_poly_test' data we predict the values of the 'y_test' data\r\n",
    "    # Then based on the predicted values and 'y_test\" data we calculate the Root Mean Square Error.\r\n",
    "    # Applying the last 4 steps for each degree value, we select the degree value which gives the Lowest Root Mean Square Error.\r\n",
    "    # Then we fit the polynomial linear regression function again using that degree which gives the Lowest Root Mean Square Error.\r\n",
    "    # Based on the polynimial function we get the predicted value of the null value.\r\n",
    "\r\n",
    "\r\n",
    "    X = np.array([i[0] for i in selected_values]).reshape(len(selected_values), 1)\r\n",
    "    y = [i[1] for i in selected_values]\r\n",
    "    mean = np.nanmean(y)\r\n",
    "    y = [mean if pd.isna(x) else x for x in y]\r\n",
    "\r\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=1/3)\r\n",
    "\r\n",
    "    degrees = np.arange(1, 11)\r\n",
    "    min_rmse, min_deg = 1e10, 0\r\n",
    "\r\n",
    "    for degree in degrees:\r\n",
    "\r\n",
    "        # Preparing polynomial Train features based on x_train\r\n",
    "        poly_features = PolynomialFeatures(degree=degree, include_bias=False)\r\n",
    "        x_poly_train = poly_features.fit_transform(x_train)\r\n",
    "\r\n",
    "        # Polynomial linear regression based on train data\r\n",
    "        poly_reg = LinearRegression()\r\n",
    "        poly_reg.fit(x_poly_train, y_train)\r\n",
    "\r\n",
    "        # Predicting y values and getting root mean squared error based on predicted y values and y_test values\r\n",
    "        x_poly_test = poly_features.fit_transform(x_test)\r\n",
    "        poly_predict = poly_reg.predict(x_poly_test)\r\n",
    "        poly_mse = mean_squared_error(y_test, poly_predict)\r\n",
    "        poly_rmse = np.sqrt(poly_mse)\r\n",
    "\r\n",
    "        # Selecting the best degree of the polynimial function based on lowest root mean squared error\r\n",
    "        if min_rmse > poly_rmse:\r\n",
    "            min_rmse = poly_rmse\r\n",
    "            min_deg = degree\r\n",
    "\r\n",
    "    # Fitting the regression function again based on the selected best degree above\r\n",
    "    poly_features = PolynomialFeatures(degree=min_deg, include_bias=False)\r\n",
    "    x_poly_train = poly_features.fit_transform(x_train)\r\n",
    "    poly_reg = LinearRegression()\r\n",
    "    poly_reg.fit(x_poly_train, y_train)\r\n",
    "\r\n",
    "    prediction = poly_reg.predict(poly_features.fit_transform([[selected_index]]))[0]\r\n",
    "    if prediction<0:\r\n",
    "        prediction = 0\r\n",
    "        \r\n",
    "    return(prediction)\r\n",
    "\r\n",
    "def mean(selected_values):\r\n",
    "    if pd.isnull(selected_values).sum() != len(selected_values):\r\n",
    "        mean_value = np.nanmean(selected_values)\r\n",
    "        selected_values = [mean_value if pd.isna(x) else x for x in selected_values]\r\n",
    "        prediction = np.mean(np.array(selected_values))\r\n",
    "    else:\r\n",
    "        prediction = 0\r\n",
    "\r\n",
    "    return prediction\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def fill_missing_data(df,length):\r\n",
    "\r\n",
    "    # 1. In the following section, we get the indexes and values of a column of a dataframe to a dictionary called 'column_data'.\r\n",
    "    # 2. Then we iterate the \"column_data\" dictionary line by line untill a null value is found (We called this index as 'selected_index').\r\n",
    "    # 3. If the index of the value is within the range of after first 3 hours and before the last 3 hours of the column we create a empty list called 'selected_values'.\r\n",
    "    # 4. Else if the index is null but it is not in the above range, we get the mean value of first 3 hours or mean value of last 3 hours according to the position of the index of the null value as the missing null value.\r\n",
    "    # 5. In 3, We append the 'seleced_values' list with the indexes of the 3 hours before the index of the null value and 3 hours after the index of the null value and respective values of those indexes.\r\n",
    "    # 6. In the 6 values of this list if more than 3 are null values and the 'selected_index' is within the range of after first 27 hours and before the last 27 hours of the column we create another empty list called 'selected_values'.\r\n",
    "    # 7. Else if more than 3 are null values but not in the range mentioned above, get the mean value of the remaining values in the 'seleced_values' list as the missing null value.\r\n",
    "    # 8. If both the two conditions in 6 and 7 are not met, get the missing null value by using the polynomial function.\r\n",
    "    # 9. In 6, We append the 'seleced_values' list with the indexes of the (1,2,3,21,22,23,25,26,27) hours before and after the index of the null value and respective values of those indexes.\r\n",
    "    # 10. In the 18 values of this llist, if more than 12 are null values and the 'selected_index' is within the range of after first 51 hours and before the last 51 hours of the column we create another empty list called 'selected_values'.\r\n",
    "    # 11. Else if more than 12 are null values but not in the range mentioned above, get the mean value of the remaining values in the 'seleced_values' list as the missing null value.\r\n",
    "    # 12. If both the two conditions in 10 and 11 are not met, get the missing null value by using the polynomial function.\r\n",
    "    # 13. In 10, We append the 'seleced_values' list with the indexes of the (1,2,3,21,22,23,25,26,27,45,46,47,49,50,51) hours before and after the index of the null value and respective values of those indexes.\r\n",
    "    # 14. In the 18 values of this llist, if more than 24 are null values and but not all 30 are null values, get the mean value of the remaining values in the 'seleced_values' list as the missing null value.\r\n",
    "    # 15. If all 30 are null values, make the missing null value 0.\r\n",
    "    # 16. Then we replace 'n/e' and any remining null values with 0.\r\n",
    "\r\n",
    "    counter =0  \r\n",
    "    for column in df.columns.values:\r\n",
    "        column_data = {}\r\n",
    "        for index, value in enumerate(df.loc[:, column]):\r\n",
    "            column_data[index] = value\r\n",
    "\r\n",
    "        for selected_index, selected_value in column_data.items():\r\n",
    "\r\n",
    "            if pd.isnull(column_data[selected_index]) and selected_index in range(3*length, len(df[column])-3*length):\r\n",
    "                selected_values = []\r\n",
    "                for i in [x for x in range(-3,4) if x!=0]:\r\n",
    "                    selected_values.append([selected_index + i*length,column_data[selected_index + i*length]])\r\n",
    "                if pd.isnull(selected_values).sum() >=3 and selected_index in range(27*length, len(df[column])-27*length):\r\n",
    "                    selected_values = []\r\n",
    "                    for i in [x for x in range(-3, 4) if x != 0]:\r\n",
    "                        for j in [-24,0,24]:\r\n",
    "                            selected_values.append([selected_index + (i+j)*length,column_data[selected_index + (i+j)*length]])\r\n",
    "                    if pd.isnull(selected_values).sum() >= 12 and selected_index in range(51*length, len(df[column])-51*length):\r\n",
    "                        selected_values = []\r\n",
    "                        for i in [x for x in range(-3, 4) if x != 0]:\r\n",
    "                            for j in [-48,-24,0, 24,48]:\r\n",
    "                                selected_values.append([selected_index + (i+j)*length,column_data[selected_index + (i+j)*length]])\r\n",
    "                        # print(selected_values)\r\n",
    "                        if pd.isnull(selected_values).sum() >= 24 and pd.isnull(selected_values).sum() < len(selected_values):\r\n",
    "                            prediction = mean([i[1] for i in selected_values])\r\n",
    "                            df.loc[selected_index,column] = prediction\r\n",
    "                            counter +=1\r\n",
    "\r\n",
    "                        elif pd.isnull(selected_values).sum() < 24:\r\n",
    "                            prediction = polynomial(selected_values,selected_index)\r\n",
    "                            df.loc[selected_index, column] = prediction\r\n",
    "                            counter += 1\r\n",
    "\r\n",
    "                        else:\r\n",
    "                            df.loc[selected_index, column] = 0\r\n",
    "                            counter += 1\r\n",
    "\r\n",
    "                    elif pd.isnull(selected_values).sum() >= 12:\r\n",
    "                        prediction = mean([i[1] for i in selected_values])\r\n",
    "                        df.loc[selected_index, column] = prediction\r\n",
    "                        counter += 1\r\n",
    "\r\n",
    "                    else:\r\n",
    "                        prediction = polynomial(selected_values,selected_index)\r\n",
    "                        df.loc[selected_index, column] = prediction\r\n",
    "                        counter += 1\r\n",
    "\r\n",
    "                elif pd.isnull(selected_values).sum() >= 3:\r\n",
    "                    prediction = mean([i[1] for i in selected_values])\r\n",
    "                    df.loc[selected_index, column] = prediction\r\n",
    "                    counter += 1\r\n",
    "\r\n",
    "                else:\r\n",
    "                    prediction = polynomial(selected_values, selected_index)\r\n",
    "                    df.loc[selected_index, column] = prediction\r\n",
    "                    counter += 1\r\n",
    "\r\n",
    "            elif pd.isnull(column_data[selected_index]) and selected_index < 3*length:\r\n",
    "                selected_values =  [column_data[i] for i in range(3*length)]\r\n",
    "                prediction = mean(selected_values)\r\n",
    "                df.loc[selected_index, column] = prediction\r\n",
    "                counter += 1\r\n",
    "\r\n",
    "            elif pd.isnull(column_data[selected_index]) and selected_index >= (len(df[column])-3*length): \r\n",
    "                selected_values = [column_data[i] for i in range(len(df[column])-3*length,len(df[column]))]\r\n",
    "                prediction = mean(selected_values)\r\n",
    "                df.loc[selected_index, column] = prediction\r\n",
    "                counter += 1\r\n",
    "\r\n",
    "    # df = df.replace(['n/e', np.nan], 0)\r\n",
    "  \r\n",
    "    # Following command creates a numpy array with a length similar to the length of the dataframe.\r\n",
    "    # Values of the array are obtained by getting the floor division of the length value.\r\n",
    "    # For example, when divider=4, this array will be [0,0,0,0,1,1,1,1,2,2,2,2,....].\r\n",
    "    # Then the rows of the dataframe will be grouped according to the order of the numpy array with the mean value of those 4 rows.\r\n",
    "    # For example, in the numpy array first 4 values are similar. Accordingly first 4 rows of the dataframe will be grouped and get the mean value of those rows\r\n",
    "\r\n",
    "    df = df.groupby(np.arange(len(df))//length).mean()\r\n",
    "    return df,counter\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# In the country speciic Load data set, Austria,Belgium,Germany,Hungary,Netherlands report data every 15 minutes. \r\n",
    "# Therefore, these countries have 35044 data points per year. \r\n",
    "# UK and Ireland report data every 30 minutes henece these countries have 17522 datapoints per year. \r\n",
    "# All the others report every 1 hour hence have 8761 datapoints per year. \r\n",
    "# In Genearion dataset, situation is same as abobe except Belgium reports hourly data hence have 8761 datapoints. \r\n",
    "# In Transmission dataset, all countries report data hourly except Germany which reports every 15 minutes. \r\n",
    "# Therefroe,it is easy if all the data are converted to hourly data. \r\n",
    "# To do that in the countries with 35044 datapoints, mean is calculated in every successive 4 datapoints. \r\n",
    "# In the countries with 17522 datapoints, mean is calculated in every successive 2 datapoints.`\r\n",
    "\r\n",
    "# Due to day light saving, all the datasets have null values on 25th March from 02:00 - 03:00.\r\n",
    "# Considering the time intervals each country update the data, a total number of rows of 4,2 or 1 are dropped from the 31st March, 02:00 - 03:00 time interval.\r\n",
    "# Also this returns a integer ('divider') based on the file length to get the energy values in a later step. In 15 min interval files this is 4, in 30 min interval files this is 2 and in 1 hour interval files this is 1.\r\n",
    "\r\n",
    "def omit_dst(df):\r\n",
    "    length=len(df.index)\r\n",
    "    if length==35044:\r\n",
    "        df = df.drop(range(7976, 7980)).reset_index(drop=True)\r\n",
    "        divider=4\r\n",
    "    elif length==17522:\r\n",
    "        df = df.drop(range(3988, 3990)).reset_index(drop=True)\r\n",
    "        divider=2\r\n",
    "    else:\r\n",
    "        df = df.drop(range(1994, 1995)).reset_index(drop=True)\r\n",
    "        divider=1\r\n",
    "\r\n",
    "    return(df,divider)    \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preparing Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "def load(countries):\r\n",
    "\r\n",
    "    load_dic = {}\r\n",
    "\r\n",
    "    # In the following command we open the csv file of each country and save the data in the 'temp' dataframe. \r\n",
    "    # Then we call the 'omit_dst' function to remove the null data in DST changing date. \r\n",
    "    # Then we call the 'fill_missing_data' function to fill the missing values\r\n",
    "    # Then the change the column name to 'demand. \r\n",
    "    # Then we save the gap filled csv and update the 'load_dic' dictionary using the 'temp' dataframe.\r\n",
    "    \r\n",
    "    for country,abbr in countries.items():\r\n",
    "        temp,length=omit_dst(pd.read_csv(f'../Data Sources/ENTSO-E/2018/Load/{country}.csv').iloc[:, 2:])\r\n",
    "        temp, counter = fill_missing_data(temp, length)\r\n",
    "        display(f'{country} - load: {counter} missing data filled')\r\n",
    "        temp.columns = ['demand']\r\n",
    "        temp.to_csv(f'../Data Sources/output/Polynomial Linear Regression/Load/{abbr}.csv')\r\n",
    "        load_dic[abbr] = temp\r\n",
    "\r\n",
    "    return(load_dic)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Preparing Generation Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def generation(countries):\r\n",
    "    \r\n",
    "    generation_dic = {}\r\n",
    "\r\n",
    "    # In the following command we open the csv file of each country and save the data in the 'temp' dataframe. \r\n",
    "    # then we remove the 'Hydro Pumped Storage  - Actual Consumption [MW]' column\r\n",
    "    # Then we call the 'omit_dst' function to remove the null data in DST changing date.\r\n",
    "    # Then we call the 'fill_missing_data' function to fill the missing values\r\n",
    "    # Then we remove the columns in which a single data is not recorded\r\n",
    "    # Then we get the column names of the 'temp' dataframe into a numpy array called 'fuels' and get the column name without the '- Actual Aggregated [MW]' part.\r\n",
    "    # Then we change the column names of the dataframe with the edited names in the previous step.\r\n",
    "    # Then we save the gap filled csv and update the 'generation_dic' dictionary using the 'temp' dataframe.\r\n",
    "        \r\n",
    "    for country,abbr in countries.items():\r\n",
    "        temp = pd.read_csv(f'../Data Sources/ENTSO-E/2018/Generation/{country}.csv',low_memory=False).iloc[:, 2:]\r\n",
    "        temp = temp.drop(['Hydro Pumped Storage  - Actual Consumption [MW]'],axis=1)\r\n",
    "        temp, length = omit_dst(temp)\r\n",
    "        temp, counter = fill_missing_data(temp, length)\r\n",
    "        display(f'{country} - Generation: {counter} missing data filled')\r\n",
    "        \r\n",
    "        for column in temp.columns.values:\r\n",
    "            if(temp[column]==0).all():\r\n",
    "                temp=temp.drop(column,axis=1)\r\n",
    "\r\n",
    "        fuels = [x[:-26] for x in temp.columns.values]\r\n",
    "        temp.columns = fuels\r\n",
    "        temp.to_csv(f'../Data Sources/output/Polynomial Linear Regression/Generation/{abbr}.csv')\r\n",
    "        generation_dic[abbr] = temp\r\n",
    "\r\n",
    "    return(generation_dic)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Preparing Cross-border Transmission Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def cross_border():\r\n",
    "\r\n",
    "    transmission_data = pd.DataFrame()\r\n",
    "\r\n",
    "# In the following command we get the list of the paths of all files in the directory. \r\n",
    "# Then we call the 'omit_dst' function to remove the null data in DST changing date.\r\n",
    "# Then we rename the two new column in the 'temp' dataframe as the two country codes the power transmission occurs. \r\n",
    "# We use string editing to get the two country codes from the file path. \r\n",
    "# For example, in the power transmission occur between Germany and Austria, we name the column as 'DE -> AT' and if the power transmission occur between Austria and Germany, we name the column as 'AT -> DE '.\r\n",
    "# Then we call the 'fill_missing_data' function to fill the missing values\r\n",
    "# Then we get all the gap filled transmission data to a single dataframe called 'transmission_data' and save the gap filled csv\r\n",
    "\r\n",
    "    csvs = glob.glob(\"../Data Sources/ENTSO-E/2018/Transmission/*.csv\")\r\n",
    "\r\n",
    "    for csv in csvs:\r\n",
    "        temp, length = omit_dst(pd.read_csv(csv).iloc[:, 1:])\r\n",
    "        temp = temp.rename(columns={temp.columns[0]: f'{csv[45:47]} - > {csv[42:44]}', temp.columns[1]: f'{csv[42:44]} - > {csv[45:47]}'})\r\n",
    "        temp, counter = fill_missing_data(temp, length)\r\n",
    "        transmission_data = pd.concat([transmission_data, temp], axis=1)\r\n",
    "        display(f'{csv[42:44]} - > {csv[45:47]} - transmission: {counter} missing data filled')\r\n",
    "    transmission_data.to_csv(f'../Data Sources/output//Polynomial Linear Regression/Transmission/all_transmissions.csv')\r\n",
    "\r\n",
    "    return transmission_data\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Internal sigma calculation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 Sigma calculation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "def calculate_sigma(load_data, generation_data, transmission_data, abbr_list):\r\n",
    "\r\n",
    "    eph = 0.1\r\n",
    "    A=100\r\n",
    "    load_gen_data = {}\r\n",
    "    sigma = {}\r\n",
    "\r\n",
    "    # First we create a new dictionary called 'load_gen_data' and in that dictionary keys are country_abbreviations and as value of each key we add the combined demand column of each country and generation columns from all the sources in that country.\r\n",
    "    # Then in each column we replace the value with 0.1, if the current value is less than 0.1. \r\n",
    "    # Then in each column we change the value as 100/current_value.\r\n",
    "    # We save the resultant dataframe as a value in a dictionary called 'sigma' with the key as country_abbreviation.\r\n",
    "    # We do the same procedure for transmission_data and save the resultant dataframe as the value of 'transmission_data' key of 'sigma' dictionary.\r\n",
    "\r\n",
    "    for abbr, df in generation_data.items():\r\n",
    "        load_gen_data[abbr] = pd.concat([df, load_data[abbr]],axis=1)\r\n",
    "\r\n",
    "    for abbr, df in load_gen_data.items():\r\n",
    "\r\n",
    "        for column in df.columns.values:\r\n",
    "            df[column] = df[column].apply(lambda x: eph if x < eph else x)\r\n",
    "            df[column] = df[column].apply(lambda x:  A/x)\r\n",
    "        sigma[abbr] = df\r\n",
    "\r\n",
    "    for transmission in transmission_data.columns.values:\r\n",
    "        transmission_data[transmission] = transmission_data[transmission].apply(lambda x: eph if x < eph else x)\r\n",
    "        transmission_data[transmission] = transmission_data[transmission].apply(lambda x:  A/x)\r\n",
    "    sigma[\"transmission_data\"] = transmission_data\r\n",
    "\r\n",
    "    return(sigma)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 Internal data consolidation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "def data_consolidation(generation_dic, load_dic, transmission_data, intermediary_var, unit_var):\r\n",
    "    consolidated_gen_data = {}\r\n",
    "    consolidated_load_data = {}\r\n",
    "    consolidated_transmission_data = {}\r\n",
    "\r\n",
    "    # In the following command, we fill each generation by source value in each timestep in each country with ('intermediary_var[\"generation\"]' + 'original_value' * 'unit_var['generation]') value of that value\r\n",
    "    # Similarly we fill each load value in each timestep in each country with ('intermediary_var['load'] + 'original_value' * 'unit_var['load']') value of that value\r\n",
    "    # Then we save the consolidated generation and load values in seperate csv files.\r\n",
    "    # We follow similar steps to obtain consolidated transmission values and save the result in a seperate csv file. \r\n",
    "\r\n",
    "    for abbr, df in generation_dic.items():\r\n",
    "        for column in df.columns:\r\n",
    "            df[column] = intermediary_var[\"generation\"][abbr][column] + \\\r\n",
    "                df[column] * unit_var[\"generation\"][abbr][column]\r\n",
    "        consolidated_gen_data[abbr] = df\r\n",
    "        consolidated_load_data[abbr] = intermediary_var[\"load\"][abbr]['demand'] + \\\r\n",
    "            load_dic[abbr]['demand'] * unit_var[\"load\"][abbr]['demand']\r\n",
    "        consolidated_gen_data[abbr].to_csv(f\"../Data Sources/output/Sigma/Generation/{abbr}.csv\")\r\n",
    "        consolidated_load_data[abbr].to_csv(f\"../Data Sources/output/Sigma/Load/{abbr}.csv\")\r\n",
    "\r\n",
    "    for column in transmission_data.columns:\r\n",
    "        transmission_data[column] = intermediary_var[\"transmission\"][column] + \\\r\n",
    "            transmission_data[column] * unit_var[\"transmission\"][column]\r\n",
    "    consolidated_transmission_data = transmission_data\r\n",
    "    consolidated_transmission_data.to_csv('../Data Sources/output/Sigma/Transmission/all_transmissions.csv')\r\n",
    "\r\n",
    "    return(consolidated_gen_data, consolidated_load_data, consolidated_transmission_data)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# sigma(load(countries)[1], generation(countries)[1], cross_border(abbr_list)[1],abbr_list)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "acb9e0cfff3151089362f20c3a81c7531326aa2c5fdbbc2eccac94359104a8e2"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
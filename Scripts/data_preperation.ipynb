{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preperation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import glob\r\n",
    "import copy\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "from sklearn.preprocessing import PolynomialFeatures\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "from sklearn.model_selection import train_test_split\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "countries={ 'Austria': 'AT', 'Belgium': 'BE',  'Bulgaria': 'BG', 'Switzerland': 'CH', 'Czech Republic': 'CZ', 'Germany': 'DE', 'Denmark': 'DK', 'Estonia': 'EE', 'Spain': 'ES', 'Finland': 'FI', 'France': 'FR',  'Greece': 'GR', 'Hungary': 'HU', 'Ireland': 'IE', 'Italy': 'IT', 'Lithuania': 'LT', 'Latvia': 'LV', 'Montenegro': 'ME','Netherlands': 'NL', 'Norway': 'NO', 'Poland': 'PL', 'Portugal': 'PT', 'Serbia': 'RS', 'Sweden': 'SE', 'Slovenia': 'SI', 'Slovakia': 'SK', 'United Kingdom': 'UK'}\r\n",
    "\r\n",
    "abbr_list=list(countries.values())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Making the first,second,third and fourth columns of the dataframe as date,month,year and time"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# data=pd.DataFrame()\r\n",
    "# temp=pd.read_csv('../Data Sources/ENTSO-E/2018/Load/Croatia.csv')\r\n",
    "# data['Day']=temp['Time (CET)'].str[:2]\r\n",
    "# data['Month']=temp['Time (CET)'].str[3:5]\r\n",
    "# data['Year']=temp['Time (CET)'].str[6:10]\r\n",
    "# data['Time']=temp['Time (CET)'].str[11:16]+' - '+temp['Time (CET)'].str[29:35]\r\n",
    "# data = data.drop(range(1994, 1995)).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# In the country speciic Load data set, Austria,Belgium,Germany,Hungary,Netherlands report data every 15 minutes. \r\n",
    "# Therefore, these countries have 35044 data points per year. \r\n",
    "# UK and Ireland report data every 30 minutes henece these countries have 17522 datapoints per year. \r\n",
    "# All the others report every 1 hour hence have 8761 datapoints per year. \r\n",
    "# In Genearion dataset, situation is same as abobe except Belgium reports hourly data hence have 8761 datapoints. \r\n",
    "# In Transmission dataset, all countries report data hourly except Germany which reports every 15 minutes. \r\n",
    "# Due to day light saving, all the datasets have null values on 25th March from 02:00 - 03:00.\r\n",
    "# Considering the time intervals each country update the data, a total number of rows of 4,2 or 1 are dropped from the 31st March, 02:00 - 03:00 time interval.\r\n",
    "# Also this returns a integer ('divider') based on the file length to get the energy values in a later step. In 15 min interval files this is 4, in 30 min interval files this is 2 and in 1 hour interval files this is 1.\r\n",
    "\r\n",
    "def omit_dst(df):\r\n",
    "\r\n",
    "    length=len(df.index)\r\n",
    "    \r\n",
    "    if length==35044:\r\n",
    "        df = df.drop(range(7976, 7980)).reset_index(drop=True)\r\n",
    "        divider=4\r\n",
    "    elif length==17522:\r\n",
    "        df = df.drop(range(3988, 3990)).reset_index(drop=True)\r\n",
    "        divider=2\r\n",
    "    else:\r\n",
    "        df = df.drop(range(1994, 1995)).reset_index(drop=True)\r\n",
    "        divider=1\r\n",
    "\r\n",
    "    return(df,divider)    \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preparing Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Preparing Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def load(countries):\r\n",
    "\r\n",
    "    load_dic = {}\r\n",
    "\r\n",
    "    # In the following command we open the csv file of each country and save the data in the 'temp' dataframe. \r\n",
    "    # Then we call the 'omit_dst' function to remove the null data in DST changing date. \r\n",
    "    # Then we call the 'fill_missing_data' function to fill the missing values\r\n",
    "    # Then the change the column name to 'demand. \r\n",
    "\r\n",
    "    # 'groupby' command creates a numpy array with a length similar to the length of the dataframe.\r\n",
    "    # Values of the array are obtained by getting the floor division of the length value.\r\n",
    "    # For example, when divider=4, this array will be [0,0,0,0,1,1,1,1,2,2,2,2,....].\r\n",
    "    # Then the rows of the dataframe will be grouped according to the order of the numpy array with the mean value of those 4 rows.\r\n",
    "    # For example, in the numpy array first 4 values are similar. Accordingly first 4 rows of the dataframe will be grouped and get the mean value of those rows\r\n",
    "\r\n",
    "    # Then we save the gap filled csv and update the 'load_dic' dictionary using the 'temp' dataframe.\r\n",
    "    \r\n",
    "    for country,abbr in countries.items():\r\n",
    "        temp,length=omit_dst(pd.read_csv(f'../Data Sources/ENTSO-E/2018/Load/{country}.csv').iloc[:, 2:])\r\n",
    "        temp, counter = fill_missing_data(temp,length)\r\n",
    "        display(f'{country}: {counter} missing data filled')\r\n",
    "        temp.columns = ['demand']\r\n",
    "        temp = temp.groupby(np.arange(len(temp))//length).mean()\r\n",
    "        # temp.to_csv(f'../Data Sources/output/Polynomial Linear Regression/Load/{abbr}.csv')\r\n",
    "        load_dic[abbr] = temp\r\n",
    "\r\n",
    "    return(load_dic)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Preparing Generation Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def generation(countries):\r\n",
    "    \r\n",
    "    generation_dic = {}\r\n",
    "\r\n",
    "    # In the following command we open the csv file of each country and save the data in the 'temp' dataframe. \r\n",
    "    # Then we call the 'omit_dst' function to remove the null data in DST changing date.\r\n",
    "    # We remove 'Hydro Pumped Storage  - Actual Consumption [MW]' column which is only available in Generation based datasets.\r\n",
    "    # We read remove the columns with all n/e values by converting them to 0s\r\n",
    "    # Then we call the 'fill_missing_data' function to fill the missing values\r\n",
    "    # In the dataset if a whole column consists only with 0 s, such columns are dropped.\r\n",
    "    # Then we get the column names of the 'temp' dataframe into a numpy array called 'fuels' and get the column name without the '- Actual Aggregated [MW]' part.\r\n",
    "    # Then we change the column names of the dataframe with the edited names in the previous step.\r\n",
    "    # Then we save the gap filled csv and update the 'generation_dic' dictionary using the 'temp' dataframe.\r\n",
    "        \r\n",
    "    for country,abbr in countries.items():\r\n",
    "        temp = pd.read_csv(f'../Data Sources/ENTSO-E/2018/Generation/{country}.csv',low_memory=False).iloc[:, 2:]\r\n",
    "        temp, length = omit_dst(temp)\r\n",
    "\r\n",
    "        temp = temp.drop(['Hydro Pumped Storage  - Actual Consumption [MW]'], axis=1)\r\n",
    "        temp = temp.replace('n/e', 0)\r\n",
    "        for column in temp.columns.values:\r\n",
    "            if(temp[column] == 0).all():\r\n",
    "                temp = temp.drop(column, axis=1)\r\n",
    "\r\n",
    "        temp, counter = fill_missing_data(temp, length)\r\n",
    "        display(f'{country}: {counter} missing data filled')\r\n",
    "        \r\n",
    "        for column in temp.columns.values:\r\n",
    "            if(temp[column]==0).all():\r\n",
    "                temp=temp.drop(column,axis=1)\r\n",
    "\r\n",
    "        fuels = [x[:-26] for x in temp.columns.values]\r\n",
    "        temp.columns = fuels\r\n",
    "        temp = temp.groupby(np.arange(len(temp))//length).mean()\r\n",
    "        # temp.to_csv(f'../Data Sources/output/Polynomial Linear Regression/Generation/{abbr}.csv')\r\n",
    "        generation_dic[abbr] = temp\r\n",
    "\r\n",
    "    return(generation_dic)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 Preparing Cross-border Transmission Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def cross_border():\r\n",
    "\r\n",
    "    transmission_data = pd.DataFrame()\r\n",
    "\r\n",
    "# In the following command we get the list of the paths of all files in the directory. \r\n",
    "# Then we call the 'omit_dst' function to remove the null data in DST changing date.\r\n",
    "# Then we rename the two new column in the 'temp' dataframe as the two country codes the power transmission occurs. \r\n",
    "# We use string editing to get the two country codes from the file path. \r\n",
    "# For example, in the power transmission occur between Germany and Austria, we name the column as 'DE -> AT' and if the power transmission occur between Austria and Germany, we name the column as 'AT -> DE '.\r\n",
    "# Then we call the 'fill_missing_data' function to fill the missing values\r\n",
    "# Then we get all the gap filled transmission data to a single dataframe called 'transmission_data' and save the gap filled csv\r\n",
    "\r\n",
    "    csvs = glob.glob(\"../Data Sources/ENTSO-E/2018/Transmission/*.csv\")\r\n",
    "\r\n",
    "    for csv in csvs:\r\n",
    "        temp, length = omit_dst(pd.read_csv(csv).iloc[:, 1:])\r\n",
    "        temp = temp.rename(columns={temp.columns[0]: f'{csv[45:47]} - > {csv[42:44]}', temp.columns[1]: f'{csv[42:44]} - > {csv[45:47]}'})\r\n",
    "        temp, counter = fill_missing_data(temp, length)\r\n",
    "        temp = temp.groupby(np.arange(len(temp))//length).mean()\r\n",
    "        transmission_data = pd.concat([transmission_data, temp], axis=1)\r\n",
    "        display(f'{csv[42:44]} - > {csv[45:47]}: {counter} missing data filled')\r\n",
    "    # transmission_data.to_csv(f'../Data Sources/output//Polynomial Linear Regression/Transmission/all_transmissions.csv')\r\n",
    "\r\n",
    "    return transmission_data\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Filling the missing observations in the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def polynomial(selected_values,selected_index):\r\n",
    "\r\n",
    "    # We get all the indexes in the 'selected_values' to a 2D numpy array 'X'.\r\n",
    "    # We get all the values in the 'selected_values'to a 1D numpy array 'y'\r\n",
    "    # Then we fill the null values in array 'y' with the mean value of the array.\r\n",
    "    # Then we divide 'X' and 'y' values in the ratio of 30% test and 70% train data.\r\n",
    "    # We create an array of degree values from 1 to 10.\r\n",
    "    # Then we iterate the 'degrees' one by one and create polynomial values of 'x_train' data called 'x_poly_train' based on the value of the degree\r\n",
    "    # Then we fit the polynomial linear regression function using 'x_poly_train' data and 'y_train' data.\r\n",
    "    # Then based on the polynomial function, using the 'x_poly_test' data we predict the values of the 'y_test' data\r\n",
    "    # Then based on the predicted values and 'y_test\" data we calculate the Root Mean Square Error.\r\n",
    "    # Applying the last 4 steps for each degree value, we select the degree value which gives the Lowest Root Mean Square Error.\r\n",
    "    # Then we fit the polynomial linear regression function again using that degree which gives the Lowest Root Mean Square Error.\r\n",
    "    # Based on the polynimial function we get the predicted value of the null value.\r\n",
    "\r\n",
    "    # print(selected_values, selected_index)\r\n",
    "\r\n",
    "    X = np.array([i[0] for i in selected_values]).reshape(len(selected_values), 1)\r\n",
    "    y = [i[1] for i in selected_values]\r\n",
    "    if pd.isnull(y).all():\r\n",
    "        mean_value = 0\r\n",
    "    else:\r\n",
    "        mean_value = np.nanmean(y)\r\n",
    "    y = [mean_value if pd.isna(x) else x for x in y]\r\n",
    "\r\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=1/3)\r\n",
    "\r\n",
    "    degrees = np.arange(1, 11)\r\n",
    "    min_rmse, min_deg = 1e10, 0\r\n",
    "\r\n",
    "    for degree in degrees:\r\n",
    "\r\n",
    "        # Preparing polynomial Train features based on x_train\r\n",
    "        poly_features = PolynomialFeatures(degree=degree, include_bias=False)\r\n",
    "        x_poly_train = poly_features.fit_transform(x_train)\r\n",
    "\r\n",
    "        # Polynomial linear regression based on train data\r\n",
    "        poly_reg = LinearRegression()\r\n",
    "        poly_reg.fit(x_poly_train, y_train)\r\n",
    "\r\n",
    "        # Predicting y values and getting root mean squared error based on predicted y values and y_test values\r\n",
    "        x_poly_test = poly_features.fit_transform(x_test)\r\n",
    "        poly_predict = poly_reg.predict(x_poly_test)\r\n",
    "        poly_mse = mean_squared_error(y_test, poly_predict)\r\n",
    "        poly_rmse = np.sqrt(poly_mse)\r\n",
    "\r\n",
    "        # Selecting the best degree of the polynimial function based on lowest root mean squared error\r\n",
    "        if min_rmse > poly_rmse:\r\n",
    "            min_rmse = poly_rmse\r\n",
    "            min_deg = degree\r\n",
    "\r\n",
    "    # Fitting the regression function again based on the selected best degree above\r\n",
    "    poly_features = PolynomialFeatures(degree=min_deg, include_bias=False)\r\n",
    "    x_poly_train = poly_features.fit_transform(x_train)\r\n",
    "    poly_reg = LinearRegression()\r\n",
    "    poly_reg.fit(x_poly_train, y_train)\r\n",
    "\r\n",
    "    prediction = poly_reg.predict(poly_features.fit_transform([[selected_index]]))[0]\r\n",
    "    if prediction<0:\r\n",
    "        prediction = 0\r\n",
    "        \r\n",
    "    return(prediction)\r\n",
    "\r\n",
    "def mean(selected_values):\r\n",
    "    if pd.isnull(selected_values).all():\r\n",
    "        prediction = 0\r\n",
    "\r\n",
    "    else:\r\n",
    "        mean_value = np.nanmean(selected_values)\r\n",
    "        selected_values = [mean_value if pd.isna(x) else x for x in selected_values]\r\n",
    "        prediction = np.mean(np.array(selected_values))\r\n",
    "    return prediction\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def fill_missing_data(df,length):\r\n",
    "\r\n",
    "    # 1. In the following section, we get the indexes and values of a column of a dataframe to a dictionary called 'column_data'.\r\n",
    "    # 2. Then we iterate the \"column_data\" dictionary line by line untill a null value is found (We called this index as 'selected_index').\r\n",
    "    # 3. If the index of the value is within the range of after first 3 hours and before the last 3 hours of the column we create a empty list called 'selected_values'.\r\n",
    "    # 4. Else if the index is null but it is not in the above range, we get the mean value of first 3 hours or mean value of last 3 hours according to the position of the index of the null value as the missing null value.\r\n",
    "    # 5. In 3, We append the 'seleced_values' list with the indexes of the 3 hours before the index of the null value and 3 hours after the index of the null value and respective values of those indexes.\r\n",
    "    # 6. In the 6 values of this list if more than 3 are null values and the 'selected_index' is within the range of after first 27 hours and before the last 27 hours of the column we create another empty list called 'selected_values'.\r\n",
    "    # 7. Else if more than 3 are null values but not in the range mentioned above, get the mean value of the remaining values in the 'seleced_values' list as the missing null value.\r\n",
    "    # 8. If both the two conditions in 6 and 7 are not met, get the missing null value by using the polynomial function.\r\n",
    "    # 9. In 6, We append the 'seleced_values' list with the indexes of the (1,2,3,21,22,23,25,26,27) hours before and after the index of the null value and respective values of those indexes.\r\n",
    "    # 10. In the 18 values of this llist, if more than 12 are null values and the 'selected_index' is within the range of after first 51 hours and before the last 51 hours of the column we create another empty list called 'selected_values'.\r\n",
    "    # 11. Else if more than 12 are null values but not in the range mentioned above, get the mean value of the remaining values in the 'seleced_values' list as the missing null value.\r\n",
    "    # 12. If both the two conditions in 10 and 11 are not met, get the missing null value by using the polynomial function.\r\n",
    "    # 13. In 10, We append the 'seleced_values' list with the indexes of the (1,2,3,21,22,23,25,26,27,45,46,47,49,50,51) hours before and after the index of the null value and respective values of those indexes.\r\n",
    "    # 14. In the 18 values of this llist, if more than 24 are null values and but not all 30 are null values, get the mean value of the remaining values in the 'seleced_values' list as the missing null value.\r\n",
    "    # 15. If all 30 are null values, make the missing null value 0.\r\n",
    "    # 16. Then we replace 'n/e' and any remining null values with 0.\r\n",
    "\r\n",
    "    counter =0  \r\n",
    "    for column in df.columns.values:\r\n",
    "        column_data = {}\r\n",
    "        for index, value in enumerate(df.loc[:, column]):\r\n",
    "            column_data[index] = value\r\n",
    "\r\n",
    "        for selected_index, selected_value in column_data.items():\r\n",
    "\r\n",
    "            if pd.isnull(column_data[selected_index]) and selected_index in range(3*length, len(df[column])-3*length):\r\n",
    "                selected_values = []\r\n",
    "                for i in [x for x in range(-3,4) if x!=0]:\r\n",
    "                    selected_values.append([selected_index + i*length,column_data[selected_index + i*length]])\r\n",
    "                if pd.isnull(selected_values).sum() >=3 and selected_index in range(27*length, len(df[column])-27*length):\r\n",
    "                    selected_values = []\r\n",
    "                    for i in [x for x in range(-3, 4) if x != 0]:\r\n",
    "                        for j in [-24,0,24]:\r\n",
    "                            selected_values.append([selected_index + (i+j)*length,column_data[selected_index + (i+j)*length]])\r\n",
    "                    if pd.isnull(selected_values).sum() >= 14 and selected_index in range(51*length, len(df[column])-51*length):\r\n",
    "                        selected_values = []\r\n",
    "                        for i in [x for x in range(-3, 4) if x != 0]:\r\n",
    "                            for j in [-48,-24,0, 24,48]:\r\n",
    "                                selected_values.append([selected_index + (i+j)*length,column_data[selected_index + (i+j)*length]])\r\n",
    "\r\n",
    "                        if pd.isnull(selected_values).sum() >= 26 and pd.isnull(selected_values).sum() < len(selected_values):\r\n",
    "                            prediction = mean([i[1] for i in selected_values])\r\n",
    "                            df.loc[selected_index,column] = prediction\r\n",
    "                            counter +=1\r\n",
    "\r\n",
    "                        elif pd.isnull(selected_values).sum() < 26:\r\n",
    "                            prediction = polynomial(selected_values,selected_index)\r\n",
    "                            df.loc[selected_index, column] = prediction\r\n",
    "                            counter += 1\r\n",
    "\r\n",
    "                        else:\r\n",
    "                            df.loc[selected_index, column] = 0\r\n",
    "                            counter += 1\r\n",
    "\r\n",
    "                    elif pd.isnull(selected_values).sum() >= 14:\r\n",
    "                        prediction = mean([i[1] for i in selected_values])\r\n",
    "                        df.loc[selected_index, column] = prediction\r\n",
    "                        counter += 1\r\n",
    "\r\n",
    "                    else:\r\n",
    "                        prediction = polynomial(selected_values,selected_index)\r\n",
    "                        df.loc[selected_index, column] = prediction\r\n",
    "                        counter += 1                       \r\n",
    "\r\n",
    "                elif pd.isnull(selected_values).sum() >= 3:\r\n",
    "                    prediction = mean([i[1] for i in selected_values])\r\n",
    "                    df.loc[selected_index, column] = prediction\r\n",
    "                    counter += 1\r\n",
    "\r\n",
    "                else:\r\n",
    "                    prediction = polynomial(selected_values, selected_index)\r\n",
    "                    df.loc[selected_index, column] = prediction\r\n",
    "                    counter += 1\r\n",
    "\r\n",
    "            elif pd.isnull(column_data[selected_index]) and selected_index < 3*length:\r\n",
    "                selected_values =  [[i,column_data[i]] for i in range(6*length)]\r\n",
    "                prediction = polynomial(selected_values, selected_index)\r\n",
    "                df.loc[selected_index, column] = prediction\r\n",
    "                counter += 1\r\n",
    "\r\n",
    "            elif pd.isnull(column_data[selected_index]) and selected_index >= (len(df[column])-3*length): \r\n",
    "                selected_values = [[i,column_data[i]] for i in range(len(df[column])-6*length,len(df[column]))]\r\n",
    "                prediction = polynomial(selected_values, selected_index)\r\n",
    "                df.loc[selected_index, column] = prediction\r\n",
    "                counter += 1\r\n",
    "\r\n",
    "    # df = df.replace(['n/e', np.nan], 0)\r\n",
    "    return df,counter\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Validating the missing observations filling model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def validation(selected_countries):\r\n",
    "\r\n",
    "    # For the validation only 4 countries are consisdered as they are the countries with the lowest missing observations.\r\n",
    "    # First we consider Generation and Load data\r\n",
    "    # We read the csv file and send it to 'omit_dst\" function and save it as 'df_orig'.\r\n",
    "    # We remove 'Hydro Pumped Storage  - Actual Consumption [MW]' column which is only available in Generation based datasets.\r\n",
    "    # We remove the columns with all n/e values by converting them to 0s\r\n",
    "    # Then we iterate the 'df_orig' column by column and deep copy the column data to to a new variable 'df_column'.\r\n",
    "    # Then we create a numpy arrange of  random values  with a number of 10% of the length of the 'df_orig' and save the random numbers in 'values'\r\n",
    "    # In the 'df_column' variable, we make the column values null which have the similar indexes as in 'values' variable.\r\n",
    "    # Then we send the 'df_column' to the function 'fill_missing_data' to predict the values of the null values we made in the previous step.\r\n",
    "    # We fill the 'filled_values' array with the values predicted in the previous step.\r\n",
    "    # We fill any null value in 'df_orig' with 0\r\n",
    "    # Then we calcultae the sum of the predicted values and sum of the original values of the predicted values before there were made null.\r\n",
    "    # Using these two calculations, percentage error in the predicted values of the country are calculated.\r\n",
    "\r\n",
    "    countries = list(selected_countries.keys())\r\n",
    "    gen_load = {}\r\n",
    "    for country in countries:\r\n",
    "        gen_load[country] = []\r\n",
    "\r\n",
    "    for item in ['Generation','Load']:\r\n",
    "        for country in countries:\r\n",
    "            sum_orig = 0\r\n",
    "            sum_fill = 0\r\n",
    "            df_orig, length = omit_dst(pd.read_csv(f'../Data Sources/ENTSO-E/2018/{item}/{country}.csv', low_memory=False).iloc[:, 2:])\r\n",
    "            \r\n",
    "            if 'Hydro Pumped Storage  - Actual Consumption [MW]' in df_orig.columns.values:\r\n",
    "                df_orig = df_orig.drop(['Hydro Pumped Storage  - Actual Consumption [MW]'], axis=1)\r\n",
    "            df_orig = df_orig.replace('n/e', 0)\r\n",
    "            for column in df_orig.columns.values:\r\n",
    "                if(df_orig[column] == 0).all():\r\n",
    "                    df_orig = df_orig.drop(column, axis=1)\r\n",
    "\r\n",
    "            for column in df_orig.columns.values:\r\n",
    "\r\n",
    "                df_column = copy.deepcopy(df_orig[[column]])\r\n",
    "                values = np.random.randint(low=0, high=8760*length, size=876*length)\r\n",
    "\r\n",
    "                for value in values:\r\n",
    "                    df_column.loc[value,column] = np.nan\r\n",
    "\r\n",
    "                fill_missing_data(df_column, length)\r\n",
    "\r\n",
    "                filled_values=[df_column.loc[value,column] for value in values]\r\n",
    "                filled_values = [0 if pd.isna(x) else x for x in filled_values]\r\n",
    "\r\n",
    "                df_orig = df_orig.replace(np.nan,0)\r\n",
    "                print(country, column)\r\n",
    "\r\n",
    "                sum_orig += sum([df_orig.loc[i, column] for i in values])\r\n",
    "                sum_fill += sum(filled_values)\r\n",
    "\r\n",
    "            percentage = round(abs((sum_fill - sum_orig)*100/sum_orig), 3)\r\n",
    "            gen_load[country].append(percentage)\r\n",
    "\r\n",
    "    # In the transmission category first we filter the transmission csvs which are relevant only for the 4 countries we consider here.\r\n",
    "    # Then for each transmission csv, we carry out the prediction process by randomly making a set values null same as earlier.\r\n",
    "    # Then we add the sum of the predicted values and sum of the original values of the predicted values to the two countries where the transmission occurs, if any of the two countries is one of the 4 countries we consider here.\r\n",
    "\r\n",
    "    csvs = glob.glob(\"../Data Sources/ENTSO-E/2018/Transmission/*.csv\")\r\n",
    "    countries = list(selected_countries.values())\r\n",
    "    selected_csvs= list(set([item for item in csvs for country in countries if country in item[42:47]]))\r\n",
    "    tx = {}\r\n",
    "\r\n",
    "    for country in countries:\r\n",
    "        tx[country] = [0,0]\r\n",
    "\r\n",
    "    for csv in selected_csvs:\r\n",
    "        df_orig,length = omit_dst(pd.read_csv(csv, low_memory=False).iloc[:, 1:])\r\n",
    "\r\n",
    "        for column in df_orig.columns.values:\r\n",
    "\r\n",
    "            df_column = copy.deepcopy(df_orig[[column]])\r\n",
    "            values = np.random.randint(low=0, high=8760*length, size=876*length)\r\n",
    "\r\n",
    "            for value in values:\r\n",
    "                df_column.loc[value, column] = np.nan\r\n",
    "\r\n",
    "            fill_missing_data(df_column, length)\r\n",
    "\r\n",
    "            filled_values = [df_column.loc[value, column] for value in values]\r\n",
    "            filled_values = [0 if pd.isna(x) else x for x in filled_values]\r\n",
    "            \r\n",
    "            df_orig = df_orig.replace(np.nan, 0)\r\n",
    "            print(csv[42:44],'-',csv[45:47], column)\r\n",
    "\r\n",
    "            if csv[42:44] in tx.keys():\r\n",
    "                tx[csv[42:44]][0] += sum([df_orig.loc[i, column] for i in values])\r\n",
    "                tx[csv[42:44]][1] += sum(filled_values)\r\n",
    "            if csv[45:47] in tx.keys():\r\n",
    "                tx[csv[45:47]][0] += sum([df_orig.loc[i, column] for i in values])\r\n",
    "                tx[csv[45:47]][1] += sum(filled_values)\r\n",
    "\r\n",
    "    # We get the generation percentage error values of each country to 'generation' variable and load percentage error values of each country to 'load' variable\r\n",
    "    # Using the sum of the predicted values and sum of the original values of the predicted values, we calculate percentage error in transmission data of the 4 countries.\r\n",
    "\r\n",
    "    generation = [x[0] for x in gen_load.values()]\r\n",
    "    load = [x[1] for x in gen_load.values()]\r\n",
    "    tranasmiision = [round(abs((value[1]-value[0])*100/value[0]), 2) for value in tx.values()]\r\n",
    "\r\n",
    "    #Then we plot the data in a stacked bar graph.\r\n",
    "\r\n",
    "    x = tx.keys()\r\n",
    "    bottom_of_tranasmiision = list(np.add(generation, load))\r\n",
    "    plt.figure(figsize=(20, 10))\r\n",
    "    plt.bar(x, load, 0.4, label='Actual Total Load')\r\n",
    "    plt.bar(x,generation,0.4,bottom=load,label='Aggregated Generation per Type')\r\n",
    "    plt.bar(x, tranasmiision, 0.4, bottom=bottom_of_tranasmiision, label='Physical Flow')\r\n",
    "    plt.xlabel('Countries')\r\n",
    "    # plt.yticks(np.arange(0,2,0.1))\r\n",
    "    plt.ylabel('Percentage Error [%]')\r\n",
    "    plt.title('Validation results of missing observations filling model')\r\n",
    "    plt.grid()\r\n",
    "    plt.legend()\r\n",
    "    plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Internal sigma calculation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 Sigma calculation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def calculate_sigma(load_data, generation_data, transmission_data, abbr_list):\r\n",
    "\r\n",
    "    eph = 0.1\r\n",
    "    lamda=100\r\n",
    "    load_gen_data = {}\r\n",
    "    sigma = {}\r\n",
    "\r\n",
    "    # First we create a new dictionary called 'load_gen_data' and in that dictionary keys are country_abbreviations and as value of each key we add the combined demand column of each country and generation columns from all the sources in that country.\r\n",
    "    # Then in each column we replace the value with 0.1, if the current value is less than 0.1. \r\n",
    "    # Then in each column we change the value as 100/current_value.\r\n",
    "    # We save the resultant dataframe as a value in a dictionary called 'sigma' with the key as country_abbreviation.\r\n",
    "    # We do the same procedure for transmission_data and save the resultant dataframe as the value of 'transmission_data' key of 'sigma' dictionary.\r\n",
    "\r\n",
    "    for abbr, df in generation_data.items():\r\n",
    "        load_gen_data[abbr] = pd.concat([df, load_data[abbr]],axis=1)\r\n",
    "\r\n",
    "    for abbr, df in load_gen_data.items():\r\n",
    "\r\n",
    "        for column in df.columns.values:\r\n",
    "            df[column] = df[column].apply(lambda x: eph if x < eph else x)\r\n",
    "            df[column] = df[column].apply(lambda x:  lamda/x)\r\n",
    "        sigma[abbr] = df\r\n",
    "\r\n",
    "    for transmission in transmission_data.columns.values:\r\n",
    "        transmission_data[transmission] = transmission_data[transmission].apply(lambda x: eph if x < eph else x)\r\n",
    "        transmission_data[transmission] = transmission_data[transmission].apply(lambda x:  lamda/x)\r\n",
    "    sigma[\"transmission_data\"] = transmission_data\r\n",
    "\r\n",
    "    return(sigma)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 Internal data consolidation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def data_consolidation(generation_dic, load_dic, transmission_data, intermediary_var, unit_var):\r\n",
    "    consolidated_gen_data = {}\r\n",
    "    consolidated_load_data = {}\r\n",
    "    consolidated_transmission_data = {}\r\n",
    "\r\n",
    "    # In the following command, we fill each generation by source value in each timestep in each country with ('intermediary_var[\"generation\"]' + 'original_value' * 'unit_var['generation]') value of that value\r\n",
    "    # Similarly we fill each load value in each timestep in each country with ('intermediary_var['load'] + 'original_value' * 'unit_var['load']') value of that value\r\n",
    "    # Then we save the consolidated generation and load values in seperate csv files.\r\n",
    "    # We follow similar steps to obtain consolidated transmission values and save the result in a seperate csv file. \r\n",
    "\r\n",
    "    for abbr, df in generation_dic.items():\r\n",
    "        for column in df.columns:\r\n",
    "            df[column] = intermediary_var[\"generation\"][abbr][column] + \\\r\n",
    "                df[column] * unit_var[\"generation\"][abbr][column]\r\n",
    "        consolidated_gen_data[abbr] = df\r\n",
    "        consolidated_load_data[abbr] = intermediary_var[\"load\"][abbr]['demand'] + \\\r\n",
    "            load_dic[abbr]['demand'] * unit_var[\"load\"][abbr]['demand']\r\n",
    "        consolidated_gen_data[abbr].to_csv(f\"../Data Sources/output/Sigma/Generation/{abbr}.csv\")\r\n",
    "        consolidated_load_data[abbr].to_csv(f\"../Data Sources/output/Sigma/Load/{abbr}.csv\")\r\n",
    "\r\n",
    "    for column in transmission_data.columns:\r\n",
    "        transmission_data[column] = intermediary_var[\"transmission\"][column] + \\\r\n",
    "            transmission_data[column] * unit_var[\"transmission\"][column]\r\n",
    "    consolidated_transmission_data = transmission_data\r\n",
    "    consolidated_transmission_data.to_csv('../Data Sources/output/Sigma/Transmission/all_transmissions.csv')\r\n",
    "\r\n",
    "    return(consolidated_gen_data, consolidated_load_data, consolidated_transmission_data)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# sigma(load(countries)[1], generation(countries)[1], cross_border(abbr_list)[1],abbr_list)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "acb9e0cfff3151089362f20c3a81c7531326aa2c5fdbbc2eccac94359104a8e2"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}